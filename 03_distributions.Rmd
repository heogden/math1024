# Random Variables and Their Probability Distributions

## Chapter mission

Last chapter's combinatorial probabilities are difficult to find and very problem-specific. Instead,
in this chapter we shall find easier ways to calculate probability in structured cases. The outcomes
of random experiments will be represented as values of a variable which will be random since the
outcomes are random (or un-predictable with certainty). In so doing, we will make our life a lot
easier in calculating probabilities in many stylised situations which represent reality. 

## Definition of a random variable

In this section we will learn about the probability distribution of a random variable defined by
its probability function. The probability function will be called the probability mass function for
discrete random variables and the probability density function for continuous random variables.

### Introduction

A random variable defines a one-to-one mapping of the sample space consisting of all possible
outcomes of a random experiment to the set of real numbers. For example, I toss a coin. Assuming
the coin is fair, there are two possible equally likely outcomes: head or tail. These two outcomes
must be mapped to real numbers. For convenience, I may define the mapping which assigns the
value 1 if head turns up and 0 otherwise. Hence, we have the mapping
\[\text{Head} \rightarrow 1, \text{Tail} \rightarrow 0.\]

We can conveniently denote the random variable by $X$ which is the number of heads obtained by
tossing a single coin. The possible values of $X$ are $0$ and $1$.

You will say that this is a trivial example. Indeed it is. But it is very easy to generalise the
concept of random variables. Simply define a mapping of the outcomes of a random experiment
to the real number space. For example, I toss the coin $n$ times and count the number of heads
and denote that to be $X$. $X$ can take any real positive integer value between $0$ and
$n$. Among other examples, suppose I select a University of Southampton student at random and
measure their height. The outcome in metres will probably be a number between one metre 
and two metres. 
But I can't exactly tell which value it will be since I do not know which student will be
selected in the first place. However, when a student has been selected I can measure their height
and get a value such as $1.432$ metres.

We now introduce two notations: $X$ (or in general the capital letters $Y, Z$ etc.) to denote the
random variable, e.g. height of a randomly selected student, and the corresponding lower case letter
$x$ (or $y$, $z$) to denote a particular value, e.g. 1.432 metres. We will follow this convention throughout.
For a random variable, say $X$, we will also adopt the notation $P (X \in A)$, read
probability that $X$
belongs to $A$, instead of the previous $P \{A\}$ for any event $A$.


### Discrete or continuous random variable

If a random variable has a finite or countably infinite set of values it is called discrete. For example,
the number of Apple computer users among 20 randomly selected students, or the number of credit
cards a randomly selected person has in their wallet.
When the random variable can take any value on the real line it is called a continuous random
variable. For example, the height of a randomly selected student. A random variable can also take
a mixture of discrete and continuous values, e.g. volume of precipitation collected in a day; some
days it could be zero, on other days it could be a continuous measurement, e.g. 1.234 mm.

### Probability distribution of a random variable


Recall the first axiom of probability ($P \{S\} = 1$), which means total probability equals 1. 
Since a random variable is merely a mapping from the outcome space to the real line, the combined
probability of all possible values of the random variable must be equal to 1.
A probability distribution distributes the total probability 1 among the
possible values of the random variable.

:::{.example}
Returning to the coin-tossing experiment, if the probability of getting a head with
a coin is $p$ (and therefore the probability of getting a tail is $1 - p$), 
then the probability that $Y = 0$
is $1 - p$ and the probability that $Y = 1$ is $p$. 
This gives us the probability distribution of $Y$, and we
say that $Y$ has the probability function
\[P(Y = y) = \begin{cases} 1-p & \text{for $y = 0$} \\
p & \text{for $y = 1$.} \end{cases}\]
This is an example of the **Bernoulli distribution** with parameter $p$, the simplest discrete
distribution.
:::

:::{.example #two-coins}
Suppose we consider tossing the coin twice and again defining the random variable
$X$ to be the number of heads obtained. The values that $X$ can take are $0$, $1$ and $2$ 
with probabilities
$(1 - p)^2$ , $2p(1 - p)$ and $p^2$, respectively. Here the probability function is
\[P(X = x) = \begin{cases}
  (1 - p)^2 & \text{for $x = 0$} \\
  2p(1 - p) & \text{for $x = 1$} \\
  p^2 & \text{for $x = 2$.}
  \end{cases}\]
This is a particular case of the Binomial distribution. We will learn about it soon.
:::

In general, for a discrete random variable we define a function $f(x)$ to denote $P (X = x)$
(or $f (y)$ to denote $P (Y = y)$) and call the function $f (x)$
the probability function (pf) or probability
mass function (pmf) of the random variable $X$. Arbitrary functions cannot be a pmf since the
total probability must be 1 and all probabilities are non-negative. Hence, for $f (x)$ to be the pmf
of a random variable $X$, we require:

1. $f (x) \geq 0$ for all possible values of $x$.
2. $\sum_{\text{all $x$}} f (x) = 1$

In Example \@ref(exm:two-coins), we may rewrite the probability function in the general form
\[f(x) = \binom{2}{x} p^x (1 - p)^{2-x}, \text{for $x = 0, 1, 2$},\]
where $f (x) = 0$ for any other value of $x$.

### Continuous random variable

In many situations (both theoretical and practical) we often encounter random variables that are
inherently continuous because they are measured on a continuum (such as time, length, weight)
or can be conveniently well-approximated by considering them as continuous (such as the annual
income of adults in a population, closing share prices).

For a continuous random variable, $P (X = x)$ is defined to be zero since we assume that the
measurements are continuous and there is zero probability of observing a particular value, e.g. $1.2$.
The argument goes that a finer measuring instrument will give us an even more precise 
measurement than $1.2$ and so on. Thus for a continuous random variable we adopt the
convention that
$P (X = x) = 0$ for any particular value $x$ on the real line. 
But we define probabilities for positive
length intervals, e.g. $P (1.2 < X < 1.9)$.

For a continuous random variable $X$ we define its probability by using a continuous function
$f(x)$ which we call its probability density function, abbreviated as its pdf. With the pdf we define
probabilities as integrals, e.g.
\[P (a < X < b) = \int_a^b f (u) du,\]
which is naturally interpreted as the area under the curve $f (x)$ inside the interval $(a, b)$. 
This is demonstrated in Figure \@ref(fig:interval-probs-from-pdf).
Recall
that we do not use $f (x) = P (X = x)$ for any $x$ as by convention we set $P(X = x) = 0$.

```{r interval-probs-from-pdf, echo = FALSE, fig.cap = "The shaded area is $P (a < X < b)$ if the pdf of $X$ is the drawn curve."}
library(ggplot2)
f <- function(x) {
    dgamma(x, shape = 3, rate = 0.5)
}

a <- 6
b <- 8

plot_data <- tibble(x = seq(a, b, length.out = 100),
                    ymax = f(x),
                    ymin = 0)

plot_data %>%
    ggplot(mapping = aes(x = x, ymin = ymin, ymax = ymax)) + 
    geom_function(fun = f) +
    geom_ribbon(alpha = 0.3) +
    xlab("x") +
    ylab("f(x)") +
    scale_x_continuous(breaks = c(0, 5, a, b, 10, 15, 20),
                       labels = c(0, 5, "a", "b", 10, 15, 20),
                       limits = c(0, 20)) +
    geom_hline(aes(yintercept = 0)) +
    theme(panel.grid.minor = element_blank(),
          panel.grid = element_blank())
```

Since we are dealing with probabilities which are always between $0$ and $1$, just any arbitrary
function f (x) cannot be a pdf of some random variable. For $f (x)$ to be a pdf, as in the discrete
case, we must have

1. $f (x) \geq 0$ for all possible values of $x$, i.e. $-\infty < x < \infty$,
2. $\int_{-\infty}^{\infty} f (u)du = 1$.

### Cumulative distribution function (cdf)

Along with the pdf we also frequently make use of another function which is called the cumulative
distribution function, abbreviated as the cdf. The cdf simply calculates the probability of the
random variable up to its argument.

For a discrete random variable $X$, the cdf is the cumulative sum
of the pmf $f(u)$ up to (and including) $u = x$. That is,
\[P (X \leq x) ≡ F (x) = \sum_{u \leq x} f (u).\]

:::{.example}
Let $X$ be the number of heads in the experiment of tossing two fair coins. Then
the probability function is
\[P (X = 0) = 1/4, \; P (X = 1) = 1/2, \; P (X = 2) = 1/4.\]

From the definition, the CDF is given by
\[F(x)= \begin{cases}0 & \text { if } x<0 \\ 1 / 4 & \text { if } 0 \leq x<1 \\ 3 / 4 & \text { if } 1 \leq x<2 \\ 1 & \text { if } x \geq 2\end{cases}\]
:::

<!-- TODO: check use of r.v. as abbreviation -->
The cdf for a discrete random variable is a step function. The jump-points are the
possible values of the random variable, and the height of a jump gives the probability of
the random variable taking that value. It is clear that the probability mass function is uniquely
determined by the cdf.

For a continuous random variable $X$, the cdf is defined as
\[P (X \leq x) ≡ F (x) = \int_{-\infty}^x f (u)du.\]

The fundamental theorem of calculus then tells us that
\[f (x) = \frac{dF (x)}{dx},\]
so for a continuous random variable the pdf is the derivative of the cdf. Also for any random
variable $X$, $P (c < X < d) = F (d) - F (c)$. Let us consider an example.

:::{.example name="Uniform distribution"}
Suppose
\[f(x)= \begin{cases}\frac{1}{b-a} & \text { if } a<x<b \\ 0 & \text { otherwise.}\end{cases}\]
We now have the cdf 
\[F(x)=\int_{a}^{x} \frac{1}{b-a} du =\frac{x-a}{b-a}, \; \; a<x<b.\]
A quick check confirms that $F^{\prime}(x)=f(x)$. If $a=0, b=1$ 
then 
\[P(0.5<X<0.75)=F(0.75)-F(0.5)=0.25.\] We shall see many more examples later.
:::

## Expectation and variance of a random variable

### Mean or expectation

In Chapter \@ref(intro-stats), we defined the mean and variance of sample data $x_1 , \ldots , x_n$. 
The random variables
with either a pmf $f (x)$ or a pdf $f (x)$ also have their own mean (which can be called
the expectation of $X$) and variance. The mean is called an expectation since it is a value we
can 'expect'! The expectation is defined as
\[E(X)= \begin{cases}\sum_{2 \ln x} x f(x) & \text { if } X \text { is discrete } \\ \int_{-\infty}^{\infty} x f(x) d x & \text { if } X \text { is continuous }\end{cases}\]
Thus, roughly spenking:
the expected value is either sum or integral of value times probability.
We use the $E(\cdot)$ notation to denote expectation. The argument is in upper case since it is the expected value of the random variable which is denoted by an upper case letter. We often use the Greek letter $\mu$ to denote $E(X)$.

:::{.example text="Discrete"}
Consider the fair-die tossing experiment, with each of the six sides having a probability of $1 / 6$ of landing face up. Let $X$ be the number on the up-face of the die. Then
\[E(X)=\sum_{x=1}^{6} x P(X=x)=\sum_{x=1}^{6} x / 6=3.5.\]
:::

:::{.example text="Continuous"}
Consider the uniform distribution which has the pdf $f(x)=\frac{1}{b-a}, \, a<x<b.$
\begin{align*}
E(X) &=\int_{-\infty}^{\infty} x f(x) d x \\
&=\int_{a}^{b} \frac{x}{b-a} d x \\
&=\frac{b^{2}-a^{2}}{2(b-a)}=\frac{b+a}{2},
\end{align*}
the mid-point of the interval $(a, b)$.
:::

If $Y=g(X)$ for any function $g(\cdot)$, then $Y$ is a random variable as well. To find $E(Y)$ we simply use the value times probability rule, i.e. the expected value of $Y$ is either sum or integral of its value, $g(x)$, times probability $f(x)$:
\[E(Y)=E(g(X))=\begin{cases}
\sum_{\text{all $x$}} g(x) f(x) & \text { if $X$  is discrete, } \\
\int_{-\infty}^{\infty} g(x) f(x) d x & \text { if $X$ is continuous.}
 \end{cases}\]
 
For example, if $X$ is continuous, then $E(X^2) = \int_{-\infty}{\infty} x^2 f(x) dx$.
We prove one important property
of expectation, namely expectation is a linear operator.

<!-- TODO: put in theorem environment -->
Suppose $Y = g(X) = aX +b$; then $E(Y ) = aE(X)+b$.

The proof of this is simple and
given below for the continuous case. In the discrete case replace
integral ($\int$) by summation ($\sum$).
\begin{align*}
E(Y ) &= \int_{-\infty}^\infty (ax + b) f(x)dx \\
&= a \int_{-\infty}^\infty x f (x)dx + b \int_{-\infty}^\infty f (x)dx \\
&= aE(X) + b,
\end{align*}
using the total probability is 1 property
($-\infty f (x)dx = 1$) in the last integral.
 This is very convenient, e.g. suppose $E(X) = 5$ and
$Y = -2X + 549$ then $E(Y) = 539$.

### Variance

The variance measures the variability of a random variable and is defined by
\[\operatorname{Var}(X)=E(X-\mu)^{2}= \begin{cases}
\sum_{\text {all } x}(x-\mu)^{2} f(x) & \text { if $X$ is discrete} \\
\int_{-\infty}^{\infty}(x-\mu)^{2} f(x) dx & \text { if $X$ is continuous,}
\end{cases}\]
where $\mu=E(X)$, and when the sum or integral exists. 
They can't always be assumed to exist! 
<!-- TODO: check for similar note about expectation sometimes not existing? -->
When the variance exists, 
it is the expectation of $(X-\mu)^{2}$ where $\mu$ is the mean of $X$.
We now derive an easy formula to calculate the variance:
<!-- TODO: put into theorem environment -->
\[\operatorname{Var}(X)=E(X-\mu)^{2}=E\left(X^{2}\right)-\mu^{2}.\]
**Proof**:
\begin{align*}
\operatorname{Var}(X) &=E(X-\mu)^{2} \\
&=E\left(X^{2}-2 X \mu+\mu^{2}\right) \\
&=E\left(X^{2}\right)-2 \mu E(X)+\mu^{2} \\
&=E\left(X^{2}\right)-2 \mu \mu+\mu^{2} \\
&=E\left(X^{2}\right)-\mu^{2}.
\end{align*}
Thus:
<!-- TODO: highlight as key point -->
the variance of a random variable is the expected value of its square
minus the square of its expected value.

We usually denote the variance by $\sigma^2$. 
The square is there to emphasise that the variance of any
random variable is always non-negative. When can the variance be zero? 
When there is no variation
at all in the random variable, i.e. it takes only a single value µ with probability 1.
Hence, there is
nothing random about the random variable --- we can predict its outcome with certainty.

<!-- TODO: highlight as key point/definition -->
The square root of the variance is called the standard deviation of the
random variable.

<!-- TODO: fix this to not refer to uniform distribution until defined -->
:::{.example name="Uniform"}
Consider the uniform distribution which has the pdf $f(x)=\frac{1}{b-a}, \, a<x<b$.
\begin{align*}
E\left(X^{2}\right) &=\int_{a}^{b} \frac{x^{2}}{b-a} d x \\
&=\frac{b^{3}-a^{3}}{3(b-a)} \\
&=\frac{b^{2}+a b+a^{2}}{3}.
\end{align*}
Hence
\[\operatorname{Var}(X)=\frac{b^{2}+a b+a^{2}}{3}-\left(\frac{b+a}{2}\right)^{2}=\frac{(b-a)^{2}}{12},\]
after simplification.
:::

We prove one important property of the variance.

<!-- TODO: put in theorem environment -->
Suppose $Y=a X+b$ then $\operatorname{Var}(Y)=a^{2} \operatorname{Var}(X)$

The proof of this is simple and is given below for the continuous case. In the discrete case replace integral $\left(\int\right)$ by summation $\left(\sum\right)$.
\begin{align*}
\operatorname{Var}(Y) &=E(Y-E(Y))^{2} \\
&=\int_{-\infty}^{\infty}(a x+b-a \mu-b)^{2} f(x) d x \\
&=a^{2} \int_{-\infty}^{\infty}(x-\mu)^{2} f(x) d x \\
&=a^{2} \operatorname{Var}(X).
\end{align*}
<!-- TODO: redo proof just using linearity of expectation? -->

This is a very useful result, e.g. suppose $\operatorname{Var}(X)=25$ and $Y=-X+5,000,000$; then $\operatorname{Var}(Y)=$ $\operatorname{Var}(X)=25$ and the standard deviation, $\sigma=5$. In words a location shift, $b$, does not change variance but a multiplicative constant, $a$ say, gets squared in variance, $a^{2}$.

## Standard discrete distributions

In this section we will learn about the Bernoulli, binomial, hypergeometric, geometric,
negative binomial and Poisson
distributions and their properties.


### Bernoulli distribution

<!-- TODO: refer back to Bernoulli trials, S/F as below? -->
<!-- TODO: describe parameter $p$ -->

The Bernoulli distribution has pmf \[f(x)=p^{x}(1-p)^{1-x}, x=0,1.\]
Hence \[E(X)=0 \cdot(1-p)+1 \cdot p=p,\]
\[E\left(X^{2}\right)=0^{2} \cdot(1-p)+1^{2} \cdot p=p\] and
\[\operatorname{Var}(X)=E\left(X^{2}\right)-(E(X))^{2}=p-p^{2}=p(1-p).\]
Hence $\operatorname{Var}(X)<E(X)$.

### Binomial distribution

#### Introduction and definition
Suppose that we have a sequence of $n$ Bernoulli trials (defined in Section \@ref(bernoulli-trials),
e.g. coin tosses) such that we get a success $(S)$ or failure $(F)$ with probabilities $P\{S\}=p$ and $P\{F\}=1-p$ respectively. Let $X$ be the number of successes in the $n$ trials. Then $X$ is called a binomial random variable with parameters $n$ and $p$.

An outcome of the experiment (of carrying out $n$ such independent trials) is represented by a sequence of $S$'s and $F$'s (such as $S S \ldots F S \ldots S F)$ 
that comprises $x$ $S$ 's, and $(n-x)$ $F$ 's.
The probability associated with this outcome is
\[P\{S S \ldots F S \ldots S F\}=p p \cdots(1-p) p \cdots p(1-p)=p^{x}(1-p)^{n-x}.\]
For this sequence, $X=x$, but there are many other sequences which will also give $X=x$. 
In fact there are $\binom{n}{x}$ such sequences. Hence
\[P(X=x)= \binom{n}{x} p^{x}(1-p)^{n-x}, x=0,1, \ldots, n .\]
This is the pmf of the binomial Distribution with parameters $n$ and $p$, often written as $\operatorname{Bin}(n, p)$.
<!-- TODO: describe $\sim$ notation -->

How can we guarantee that $\sum_{x=0}^{n} P(X=x)=1$? This guarantee is provided by the binomial theorem:
\[(a+b)^{n}=b^{n}+ \binom{n}{1} a b^{n-1}+\cdots+\binom{n}{x} a^x b^{n-x}+\cdots +a^{n}.\]
To prove $\sum_{x=0}^{n} P(X=x)=1$, i.e. $\sum_{x=0}^{n}\binom{n}{x} p^{x}(1-p)^{n-x}=1$, 
choose $a=p$ and $b=1-p$ in the binomial theorem.

:::{.example #widgets}
Suppose that widgets are manufactured in a mass production process with $1\%$
defective. The widgets are packaged in bags of 10 with a money-back guarantee if more than 1
widget per bag is defective. For what proportion of bags would the company have to provide a
refund?

First, we find the probability that a randomly selected bag has at most 1 defective
widget. Let $X$ be the number of defective widgets in a bag, then 
$X \sim \operatorname{Bin}(n = 10, p = 0.01).$
So this
probability is equal to
\[P (X = 0) + P (X = 1) = (0.99)10 + 10(0.01)1 (0.99)9 = 0.9957.\]
Hence the probability that a refund is required is $1 - 0.9957 = 0.0043$, 
i.e. only just over 4 in 1000
bags will incur the refund on average.
:::

:::{.example}
A binomial random variable can also be described using the urn model. Suppose
we have an urn (population) containing $N$ individuals, a proportion $p$ of which are of type $S$ and a
proportion $1 - p$ of type $F$. If we select a sample of $n$ individuals at random with replacement,
then the number, $X$, of type $S$ individuals in the sample follows the binomial distribution with
parameters $n$ and $p$.
:::

#### Using R to calculate probabilities
<!-- TODO:consider whether or not this should go into section structure -->

Probabilities under all the standard distributions have been calculated in R and will be used
throughout MATH1024. You will not be required to use any tables. 
For the binomial distribution
the command 
```{r, eval = FALSE}
dbinom(x=3, size=5, prob=0.34)
```
calculates the pmf of $\operatorname{Bin}(n = 5, p = 0.34)$ at $x=3$,
with value $P (X = 3) = \binom{5}{3} (0.34)^3 (1 - 0.34)^{5-3}$. 
The command `pbinom` returns the cdf or the probability up to
and including the argument. Thus 
```{r, eval = FALSE}
pbinom(q=3, size=5, prob=0.34)
```
will return the value of
$P (X \leq 3)$ when $X \sim \operatorname{Bin}(n = 5, p = 0.34)$. 
As a check, in Example \@ref(exm:widgets), we may compute the probability
that a randomly selected bag has at most 1 defective
widget with the command
```{r}
pbinom(q=1, size=10, prob=0.01)
```
which matches our earlier calculations.

#### Expectation 

Let $X \sim \operatorname{Bin}(n, p)$. We have
\[E(X)=\sum_{x=0}^{n} x P(X=x)=\sum_{x=0}^{n} x \binom{n}{x} p^{x}(1-p)^{n-x}.\]
Below we prove that $E(X)=n p$. Recall that $k !=k(k-1) !$ for any $k>0$.
\begin{align*}
E(X) &=\sum_{x=0}^{n} x \binom{n}{x} p^{x}(1-p)^{n-x} \\
&=\sum_{x=1}^{n} x \frac{(n ! x) !}{x !(n-x)} p^{x}(1-p)^{n-x} \\
&=\sum_{x=1}^{n} \frac{n !}{(x-1) !(n-x) !} p^{x}(1-p)^{n-x} \\
&=n p \sum_{x=1}^{n} \frac{(n-1) !}{(x-1) !(n-1-x+1) !} p^{x-1}(1-p)^{n-1-x+1} \\
&=n p \sum_{y=0}^{n-1} \frac{(n-1) !}{(n) !(n-1) !-y) !} p^{y}(1-p)^{n-1-y} \\
&=n p(p+1-p)^{n-1}=n p
\end{align*}
where we used the substitution $y=x-1$ and then the binomial theorem to conclude that the last sum is equal to $1 .$

<!-- TODO: use pf trick rather than binomial theorem? -->

#### Variance

Let $X \sim \operatorname{Bin}(n, p)$. Then $\operatorname{Var}(X)=n p(1-p)$. 
It is difficult to find $E\left(X^{2}\right)$ directly, but the factorial structure
allows us to find $E[X(X-1)]$. Recall that $k !=k(k-1)(k-2) !$ for any $k>1$.
\begin{align*}
E[(X(X-1)]&=\sum_{x=0}^{n} x(x-1) \binom{n}{x} p^{x}(1-p)^{n-x} \\
&=\sum_{x=2}^{n} x(x-1) \frac{n !}{x !(n-x) !} p^{x}(1-p)^{n-x} \\
&=\sum_{x=2}^{n} \frac{n !}{(x-2) !(n-x) !} p^{x}(1-p)^{n-x} \\
&=n(n-1) p^{2} \sum_{x=2}^{n} \frac{(n-2) !}{(x-2) !(n-2-x+2) !} p^{x-2}(1-p)^{n-2-x+2} \\
&=n(n-1) p^{2} \sum_{y=0}^{n-2} \frac{(n-2) !}{(y) !(n-2-y) !} p^{y}(1-p)^{n-2-y} \\
&=n(n-1) p^{2}(p+1-p)^{n-2} .
\end{align*}
<!-- TODO: use pf trick rather than binomial theorem? -->
Now, $E\left(X^{2}\right)=E[X(X-1)]+E(X)=n(n-1) p^{2}+n p$. Hence,
\[\operatorname{Var}(X)=E\left(X^{2}\right)-(E(X))^{2}=n(n-1) p^{2}+n p-(n p)^{2}=n p(1-p).\]
It is illuminating to see these direct proofs. Later on we shall apply statistical theory to directly prove these! Notice that the binomial theorem is used repeatedly to prove the results.

### Geometric distribution

#### Introduction and definition
Suppose that we have the same situation as for the binomial distribution but we consider a different r.v. $X$, which is defined as the number of trials that lead to the first success. The outcomes for this experiment are:
\[\begin{array}{rll}
S & X=1, & P(X=1)=p \\
F S & X=2, & P(X=2)=(1-p) p \\
F F S & X=3, & P(X=3)=(1-p)^{2} p \\
F F F S & X=4, & P(X=4)=(1-p)^{3} p \\
\vdots & \vdots &
\end{array}\]
In general we have
\[P(X=x)=(1-p)^{x-1} p, x=1,2, \ldots\]
This is called the **geometric** distribution, and it has a (countably) infinite domain starting at $1$ rather than $0$. We write $X \sim \operatorname{Geo}(p)$.
Let us check that the probability function has the required property:
\begin{align*}
\sum_{x=1}^{\infty} P(X=x) &=\sum_{x=1}^{\infty}(1-p)^{x-1} p \\
&=p \sum_{y=0}^{\infty}(1-p)^{y} \quad \text{[substitute  $y=x-1$]} \\
&=p \frac{1}{1-(1-p)} \quad \text {[see Section ??] } \\
&=1
\end{align*}
<!-- TODO: add reference or change to avoid using -->
We can also find the probability that $X>k$ for some given natural number $k$ :
\begin{align*}
\sum_{x=k+1}^{\infty} P(X=x) &=\sum_{x=k+1}^{\infty}(1-p)^{x-1} p \\
&=p\left[(1-p)^{k+1-1}+(1-p)^{k+2-1}+(1-p)^{k+3-1}+\ldots\right.\\
&=p(1-p)^{k} \sum_{y=0}^{\infty}(1-p)^{y} \\
&=(1-p)^{k}
\end{align*}

#### Memoryless property 

Let $X$ follow the geometric distribution and suppose that $s$ and $k$ are positive integers. We then have
\[P(X>s+k \mid X>k)=P(X>s).\]

The proof is given below. In practice this means that the random variable does not remember its age (denoted by $k$) to determine how long more (denoted by $s$) it will survive! The proof below uses the definition of conditional probability
\[P\{A \mid B\}=\frac{P\{A \cap B\}}{P\{B\}}.\]
Now the proof,
\begin{align*}
P(X>s+k \mid X>k) &=\frac{P(X>s+k, X>k)}{P(X>k)} \\
&=\frac{P(X>s+k)}{P(X>k)} \\
&=\frac{(1-p)^{s+k}}{(1-p)^{k}} \\
&=(1-p)^{s},
\end{align*}
which does not depend on $k$. Note that the event $X>s+k$ and $X>k$ implies and is implied by $X>s+k$ since $s>0$.


#### Expectation and variance
Let $X \sim \operatorname{Geo}(p)$. We can show that $E(X)=p$ using the negative binomial series, 
see Section ???, as follows:
<!-- TODO: insert reference to negative binomial series (or avoid use) -->
\begin{align*}
E(X) &=\sum_{x=1}^{\infty} x P(X=x) \\
&=\sum_{x=1}^{\infty} x p(1-p)^{x-1} \\
&=p\left[1+2(1-p)+3(1-p)^{2}+4(1-p)^{3}+\ldots\right]
\end{align*}
For $n>0$ and $|x|<1$, the negative binomial series is given by:
\[(1-x)^{-n}=1+n x+\frac{1}{2} n(n+1) x^{2}+\frac{1}{6} n(n+1)(n+2) x^{3}+\cdots+\frac{n(n+1)(n+2) \cdots(n+k-1)}{k !} x^{k}+\cdots\]
With $n=2$ and $x=1-p$ the general term is given by:
\[\frac{n(n+1)(n+2)(n+k-1)}{k !}=\frac{2 \times 3 \times 4 \times \cdots \times(2+k-1)}{k !}=k+1.\]
Thus $E(X)=p(1-1+p)^{-2}=1 / p$. It can be shown that $\operatorname{Var}(X)=(1-p) / p^{2}$ using negative binomial series. But this is more complicated and is not required here. 
The second-year module MATH2011 will provide an alternative proof.

### Hypergeometric distribution

Suppose we have an urn (population) containing $N$ individuals, a proportion $p$ of which are of type $S$ and a proportion $1-p$ of type $F$. If we select a sample of $n$ individuals at random without replacement, then the number, $X$, of type $S$ individuals in the sample has the hypergeometric distribution,
with pmf
\[P(X=x)=\frac{\binom{Np}{x} \binom{N(1-p)}{n-x}}{\binom{N}{n}}, \quad x=0,1, \ldots, n,\]
assuming that $x \leq N p$ and $n-x \leq N(1-p)$ so that the above combinations are well defined. 
The mean and variance of the hypergeometric distribution are given by
\[E(X)=n p, \quad \operatorname{Var}(X)=n p q \frac{N-n}{N-1}.\]

### Negative binomial distribution

Still in the Bernoulli trials set-up, we define the random variable $X$ to be the total number
of trials
until the $r$th success occurs, where $r$ is a given natural number. This is known as the negative
binomial distribution with parameters $p$ and $r$.
[Note: if $r = 1$, the negative binomial distribution is just the geometric distribution.]
Firstly we need to identify the possible values of $X$. Possible values for $X$ are $x = r, r + 1, r +
2, \ldots$. Secondly, the probability mass function is
\begin{align*}
P (X = x) &= \binom{x-1}{r-1}
p^{r-1} (1 - p)^{(x-1)-(r-1)} \times p \\
&=\binom{x-1}{r-1}
p^r (1 - p)^{x-r}, \quad x = r, r + 1, \ldots
\end{align*}

:::{.example}
In a board game that uses a single fair die, a player cannot start until they have
rolled a six. Let $X$ be the number of rolls needed until they get a six. Then $X$ is a Geometric
random variable with success probability $p = 1/6$.
:::

:::{.example} 
A man plays roulette, betting on red each time. He decides to keep playing until
he achieves his second win. The success probability for each game is $18/37$ and the results of games
are independent. Let $X$ be the number of games played until he gets his second win. Then $X$ is
a Negative Binomial random variable with $r = 2$ and p = $18/37$. What is the probability he plays
more than $3$ games? i.e. find $P (X > 3)$.
:::
<!-- TODO: add solution here? -->

Derivation of the mean and variance of the negative binomial distribution involves complicated negative binomial series and will be skipped for now, but will be proved in Lecture ???. 
<!-- TODO: add reference -->
For
completeness we note down the mean and variance:
\[E(X) = \frac{r}{p}, \quad \operatorname{Var}(X) = r \, \frac{1-p}{p^2}.\]
Thus when r = 1, the mean and variance of the negative binomial distribution are equal to those
of the geometric distribution.

### Poisson distribution

#### Introduction and definition

The Poisson distribution can be obtained as the limit of the binomial distribution with parameters $n$ and $p$ when $n \rightarrow \infty$ and $p \rightarrow 0$ simultaneously, but the product $\lambda=n p$ remains finite. In practice this means that the Poisson distribution counts rare events (since $p \rightarrow 0$ ) in an infinite population (since $n \rightarrow \infty$ ). Theoretically, a random variable following the Poisson distribution can take any integer value from 0 to $\infty$. Examples of the Poisson distribution include: the number of breast cancer patients in Southampton; the number of text messages sent (or received) per day by a randomly selected first-year student; the number of credit cards a randomly selected person has in their wallet.

<!-- TODO: check whether this limiting part needed? -->
Let us find the pmf of the Poisson distribution as the limit of the pmf of the binomial distribution. Recall that if $X \sim \operatorname{Bin}(n, p)$ then $P(X=x)=\left(\begin{array}{l}n \\ x\end{array}\right) p^{x}(1-p)^{n-x}$. Now:
\begin{align*}
P(X=x) &=\binom{n}{x} p^{x}(1-p)^{n-x} \\
&=\binom{n}{x} \frac{n^{n}}{n^{n}} p^{x}(1-p)^{n-x} \\
&=\frac{n(n-1) \cdots(n-x+1)}{n^{x} x !}(n p)^{x}(n(1-p))^{n-x} \frac{1}{n^{n-x}} \\
&=\frac{n}{n} \frac{(n-1)}{n} \cdots \frac{(n-x+1)}{n} \frac{\lambda^{x}}{x !}\left(1-\frac{\lambda}{n}\right)^{n-x}\\
&=\frac{n}{n} \frac{(n-1)}{n} \cdots \frac{(n-x+1)}{n} \frac{\lambda^{x}}{x !}\left(1-\frac{\lambda}{n}\right)^{n}\left(1-\frac{\lambda}{n}\right)^{-x}.
\end{align*}
Now it is easy to see that the above tends to
\[e^{-\lambda} \frac{\lambda^{x}}{x!}\]
as $n \rightarrow \infty$ for any fixed value of $x$ in the range $0,1,2, \ldots .$ Note that we have used the exponential limit:
\[e^{-\lambda}=\lim _{n \rightarrow \infty}\left(1-\frac{\lambda}{n}\right)^{n},\]
and
\[ \lim _{n \rightarrow \infty}\left(1-\frac{\lambda}{n}\right)^{-x}=1\]
and
\[ \lim _{n \rightarrow \infty} \frac{n}{n} \frac{(n-1)}{n} \cdots \frac{(n-x+1)}{n}=1.\]
A random variable $X$ has the Poisson distribution with parameter $\lambda$ if it has the pmf:
\[P(X=x)=e^{-\lambda} \frac{\lambda^{x}}{x !}, \quad x=0,1,2, \ldots\]
We write $X \sim \operatorname{Poisson}(\lambda)$. It is easy to show $\sum_{x=0}^{\infty} P(X=x)=1$, i.e. $\sum_{x=0}^{\infty} e^{-\lambda} \frac{\lambda^{x}}{x !}=1$. The identity you need is simply the expansion of $e^{\lambda}$.

#### Expectation

Let $X \sim \operatorname{Poisson}(\lambda)$. Then
\begin{align*}
E(X) &=\sum_{x=0}^{\infty} x P(X=x) \\
&=\sum_{x=0}^{\infty} x e^{-\lambda} \frac{\lambda^{x}}{x^{!} !} \\
&=e^{-\lambda} \sum_{x=1}^{\infty} x \frac{\lambda^{x}}{x !} \\
&=e^{-\lambda} \sum_{x=1}^{\infty} \frac{\lambda \cdot \lambda^{(x-1)}}{(x-1) !} \\
&=\lambda e^{-\lambda \sum_{x=1}^{\infty} \frac{\lambda^{(x-1)}}{(x-1) !}} \\
&=\lambda e^{-\lambda} \sum_{y=0}^{\infty} \frac{\lambda^{y}}{y !}[y=x-1] \\
&=\lambda e^{-\lambda} e^{\lambda} \quad [\text {using the expansion of $e^{\lambda}$}  \\
&=\lambda .
\end{align*}

#### Variance
Let $X \sim \operatorname{Poisson}(\lambda)$. Then
\begin{align*}
E[X(X-1)] &=\sum_{x=0}^{\infty} x(x-1) P(X=x) \\
&=\sum_{x=0}^{\infty} x(x-1) e^{-\lambda} \frac{\lambda^{x}}{x^{\prime}} \\
&=e^{-\lambda} \sum_{x=2}^{\infty} x(x-1) \frac{\lambda^{\lambda^{x}}}{x !} \\
&=e^{-\lambda} \sum_{x=2}^{\infty} \lambda^{2} \frac{\lambda^{x-2}}{(x-2) !} \\
&=\lambda^{2} e^{-\lambda} \sum_{y=0}^{\infty} \frac{\lambda^{y}}{y !}[y=x-2] \\
&=\lambda^{2} e^{-\lambda} e^{\lambda}=\lambda^{2} \quad \text { [using the expansion of $e^{\lambda}$].} 
\end{align*}
<!-- TODO: remove square brackets in explanation in displays throughout -->
Now, $E\left(X^{2}\right)=E[X(X-1)]+E(X)=\lambda^{2}+\lambda$. Hence,
\[\operatorname{Var}(X)=E\left(X^{2}\right)-(E(X))^{2}=\lambda^{2}+\lambda-\lambda^{2}=\lambda.\]
Hence, the mean and variance are the same for the Poisson distribution.

#### Using R to calculate probabilities

For the Poisson distribution the command 
```{r, eval = FALSE}
dpois(x=3, lambda=5)
```
calculates the pmf of $\operatorname{Poisson}(\lambda = 5)$ at $x = 3$. 
That is, the command will return the value $P (X = 3) = e^{-5} \frac{5^3}{3!}$. The command `ppois`
returns the cdf or the probability up to and including the argument. Thus 
```{r, eval = FALSE}
ppois(q=3, lambda=5)
```
will return the value of $P (X \leq 3)$ when $X \sim \operatorname{Poisson}(\lambda = 5)$.

# Random Variables and Their Probability Distributions

## Chapter mission

Last chapter's combinatorial probabilities are difficult to find and very problem-specific. Instead,
in this chapter we shall find easier ways to calculate probability in structured cases. The outcomes
of random experiments will be represented as values of a variable which will be random since the
outcomes are random (or un-predictable with certainty). In so doing, we will make our life a lot
easier in calculating probabilities in many stylised situations which represent reality. 

## Definition of a random variable

In this section we will learn about the probability distribution of a random variable defined by
its probability function. The probability function will be called the probability mass function for
discrete random variables and the probability density function for continuous random variables.

### Introduction

A random variable defines a one-to-one mapping of the sample space consisting of all possible
outcomes of a random experiment to the set of real numbers. For example, I toss a coin. Assuming
the coin is fair, there are two possible equally likely outcomes: head or tail. These two outcomes
must be mapped to real numbers. For convenience, I may define the mapping which assigns the
value 1 if head turns up and 0 otherwise. Hence, we have the mapping
\[\text{Head} \rightarrow 1, \text{Tail} \rightarrow 0.\]

We can conveniently denote the random variable by $X$ which is the number of heads obtained by
tossing a single coin. The possible values of $X$ are $0$ and $1$.

You will say that this is a trivial example. Indeed it is. But it is very easy to generalise the
concept of random variables. Simply define a mapping of the outcomes of a random experiment
to the real number space. For example, I toss the coin $n$ times and count the number of heads
and denote that to be $X$. $X$ can take any real positive integer value between $0$ and
$n$. Among other examples, suppose I select a University of Southampton student at random and
measure their height. The outcome in metres will probably be a number between one metre 
and two metres. 
But I can't exactly tell which value it will be since I do not know which student will be
selected in the first place. However, when a student has been selected I can measure their height
and get a value such as $1.432$ metres.

We now introduce two notations: $X$ (or in general the capital letters $Y, Z$ etc.) to denote the
random variable, e.g. height of a randomly selected student, and the corresponding lower case letter
$x$ (or $y$, $z$) to denote a particular value, e.g. 1.432 metres. We will follow this convention throughout.
For a random variable, say $X$, we will also adopt the notation $P (X \in A)$, read
probability that $X$
belongs to $A$, instead of the previous $P \{A\}$ for any event $A$.


### Discrete or continuous random variable

If a random variable has a finite or countably infinite set of values it is called discrete. For example,
the number of Apple computer users among 20 randomly selected students, or the number of credit
cards a randomly selected person has in their wallet.
When the random variable can take any value on the real line it is called a continuous random
variable. For example, the height of a randomly selected student. A random variable can also take
a mixture of discrete and continuous values, e.g. volume of precipitation collected in a day; some
days it could be zero, on other days it could be a continuous measurement, e.g. 1.234 mm.

### Probability distribution of a random variable


Recall the first axiom of probability ($P \{S\} = 1$), which means total probability equals 1. 
Since a random variable is merely a mapping from the outcome space to the real line, the combined
probability of all possible values of the random variable must be equal to 1.
A probability distribution distributes the total probability 1 among the
possible values of the random variable.

:::{.example}
Returning to the coin-tossing experiment, if the probability of getting a head with
a coin is $p$ (and therefore the probability of getting a tail is $1 - p$), 
then the probability that $Y = 0$
is $1 - p$ and the probability that $Y = 1$ is $p$. 
This gives us the probability distribution of $Y$, and we
say that $Y$ has the probability function
\[P(Y = y) = \begin{cases} 1-p & \text{for $y = 0$} \\
p & \text{for $y = 1$.} \end{cases}\]
This is an example of the **Bernoulli distribution** with parameter $p$, the simplest discrete
distribution.
:::

:::{.example #two-coins}
Suppose we consider tossing the coin twice and again defining the random variable
$X$ to be the number of heads obtained. The values that $X$ can take are $0$, $1$ and $2$ 
with probabilities
$(1 - p)^2$ , $2p(1 - p)$ and $p^2$, respectively. Here the probability function is
\[P(X = x) = \begin{cases}
  (1 - p)^2 & \text{for $x = 0$} \\
  2p(1 - p) & \text{for $x = 1$} \\
  p^2 & \text{for $x = 2$.}
  \end{cases}\]
his is a particular case of the Binomial distribution. We will learn about it soon.
:::

In general, for a discrete random variable we define a function $f(x)$ to denote $P (X = x)$
(or $f (y)$ to denote $P (Y = y)$) and call the function $f (x)$
the probability function (pf) or probability
mass function (pmf) of the random variable $X$. Arbitrary functions cannot be a pmf since the
total probability must be 1 and all probabilities are non-negative. Hence, for $f (x)$ to be the pmf
of a random variable $X$, we require:

1. $f (x) \geq 0$ for all possible values of $x$.
2. $\sum_{\text{all $x$}} f (x) = 1$

In Example \@ref(exm:two-coins), we may rewrite the probability function in the general form
\[f(x) = \binom{2}{x} p^x (1 - p)^{2-x}, \text{for $x = 0, 1, 2$},\]
where $f (x) = 0$ for any other value of $x$.

### Continuous random variable

In many situations (both theoretical and practical) we often encounter random variables that are
inherently continuous because they are measured on a continuum (such as time, length, weight)
or can be conveniently well-approximated by considering them as continuous (such as the annual
income of adults in a population, closing share prices).

For a continuous random variable, $P (X = x)$ is defined to be zero since we assume that the
measurements are continuous and there is zero probability of observing a particular value, e.g. $1.2$.
The argument goes that a finer measuring instrument will give us an even more precise 
measurement than $1.2$ and so on. Thus for a continuous random variable we adopt the
convention that
$P (X = x) = 0$ for any particular value $x$ on the real line. 
But we define probabilities for positive
length intervals, e.g. $P (1.2 < X < 1.9)$.

For a continuous random variable $X$ we define its probability by using a continuous function
$f(x)$ which we call its probability density function, abbreviated as its pdf. With the pdf we define
probabilities as integrals, e.g.
\[P (a < X < b) = \int_a^b f (u) du,\]
which is naturally interpreted as the area under the curve $f (x)$ inside the interval $(a, b)$. 
This is demonstrated in Figure \@ref(fig:interval-probs-from-pdf).
Recall
that we do not use $f (x) = P (X = x)$ for any $x$ as by convention we set $P(X = x) = 0$.

```{r interval-probs-from-pdf, echo = FALSE, fig.cap = "The shaded area is $P (a < X < b)$ if the pdf of $X$ is the drawn curve."}
library(ggplot2)
f <- function(x) {
    dgamma(x, shape = 3, rate = 0.5)
}

a <- 6
b <- 8

plot_data <- tibble(x = seq(a, b, length.out = 100),
                    ymax = f(x),
                    ymin = 0)

plot_data %>%
    ggplot(mapping = aes(x = x, ymin = ymin, ymax = ymax)) + 
    geom_function(fun = f) +
    geom_ribbon(alpha = 0.3) +
    xlab("x") +
    ylab("f(x)") +
    scale_x_continuous(breaks = c(0, 5, a, b, 10, 15, 20),
                       labels = c(0, 5, "a", "b", 10, 15, 20),
                       limits = c(0, 20)) +
    geom_hline(aes(yintercept = 0)) +
    theme(panel.grid.minor = element_blank(),
          panel.grid = element_blank())
```

Since we are dealing with probabilities which are always between $0$ and $1$, just any arbitrary
function f (x) cannot be a pdf of some random variable. For $f (x)$ to be a pdf, as in the discrete
case, we must have

1. $f (x) \geq 0$ for all possible values of $x$, i.e. $-\infty < x < \infty$,
2. $\int_{-\infty}^{\infty} f (u)du = 1$.

### Cumulative distribution function (cdf)

Along with the pdf we also frequently make use of another function which is called the cumulative
distribution function, abbreviated as the cdf. The cdf simply calculates the probability of the
random variable up to its argument.

For a discrete random variable $X$, the cdf is the cumulative sum
of the pmf $f(u)$ up to (and including) $u = x$. That is,
\[P (X \leq x) ≡ F (x) = \sum_{u \leq x} f (u).\]

:::{.example}
Let $X$ be the number of heads in the experiment of tossing two fair coins. Then
the probability function is
\[P (X = 0) = 1/4, \; P (X = 1) = 1/2, \; P (X = 2) = 1/4.\]

From the definition, the CDF is given by
\[F(x)= \begin{cases}0 & \text { if } x<0 \\ 1 / 4 & \text { if } 0 \leq x<1 \\ 3 / 4 & \text { if } 1 \leq x<2 \\ 1 & \text { if } x \geq 2\end{cases}\]
:::

<!-- TODO: check use of r.v. as abbreviation -->
The cdf for a discrete random variable is a step function. The jump-points are the
possible values of the random variable, and the height of a jump gives the probability of
the random variable taking that value. It is clear that the probability mass function is uniquely
determined by the cdf.

For a continuous random variable $X$, the cdf is defined as
\[P (X \leq x) ≡ F (x) = \int_{-\infty}^x f (u)du.\]

The fundamental theorem of calculus then tells us that
\[f (x) = \frac{dF (x)}{dx},\]
so for a continuous random variable the pdf is the derivative of the cdf. Also for any random
variable $X$, $P (c < X < d) = F (d) - F (c)$. Let us consider an example.

:::{.example name="Uniform distribution"}
Suppose
\[f(x)= \begin{cases}\frac{1}{b-a} & \text { if } a<x<b \\ 0 & \text { otherwise.}\end{cases}\]
We now have the cdf 
\[F(x)=\int_{a}^{x} \frac{1}{b-a} du =\frac{x-a}{b-a}, \; \; a<x<b.\]
A quick check confirms that $F^{\prime}(x)=f(x)$. If $a=0, b=1$ 
then 
\[P(0.5<X<0.75)=F(0.75)-F(0.5)=0.25.\] We shall see many more examples later.
:::

## Expectation and variance of a random variable

### Mean or expectation

In Chapter \@ref(intro-stats), we defined the mean and variance of sample data $x_1 , \ldots , x_n$. 
The random variables
with either a pmf $f (x)$ or a pdf $f (x)$ also have their own mean (which can be called
the expectation of $X$) and variance. The mean is called an expectation since it is a value we
can 'expect'! The expectation is defined as
\[E(X)= \begin{cases}\sum_{2 \ln x} x f(x) & \text { if } X \text { is discrete } \\ \int_{-\infty}^{\infty} x f(x) d x & \text { if } X \text { is continuous }\end{cases}\]
Thus, roughly spenking:
the expected value is either sum or integral of value times probability.
We use the $E(\cdot)$ notation to denote expectation. The argument is in upper case since it is the expected value of the random variable which is denoted by an upper case letter. We often use the Greek letter $\mu$ to denote $E(X)$.

:::{.example text="Discrete"}
Consider the fair-die tossing experiment, with each of the six sides having a probability of $1 / 6$ of landing face up. Let $X$ be the number on the up-face of the die. Then
\[E(X)=\sum_{x=1}^{6} x P(X=x)=\sum_{x=1}^{6} x / 6=3.5.\]
:::

:::{.example text="Continuous"}
Consider the uniform distribution which has the pdf $f(x)=\frac{1}{b-a}, \, a<x<b.$
\begin{align*}
E(X) &=\int_{-\infty}^{\infty} x f(x) d x \\
&=\int_{a}^{b} \frac{x}{b-a} d x \\
&=\frac{b^{2}-a^{2}}{2(b-a)}=\frac{b+a}{2},
\end{align*}
the mid-point of the interval $(a, b)$.
:::

If $Y=g(X)$ for any function $g(\cdot)$, then $Y$ is a random variable as well. To find $E(Y)$ we simply use the value times probability rule, i.e. the expected value of $Y$ is either sum or integral of its value, $g(x)$, times probability $f(x)$:
\[E(Y)=E(g(X))=\begin{cases}
\sum_{\text{all $x$}} g(x) f(x) & \text { if $X$  is discrete, } \\
\int_{-\infty}^{\infty} g(x) f(x) d x & \text { if $X$ is continuous.}
 \end{cases}\]
 
For example, if $X$ is continuous, then $E(X^2) = \int_{-\infty}{\infty} x^2 f(x) dx$.
We prove one important property
of expectation, namely expectation is a linear operator.

<!-- TODO: put in theorem environment -->
Suppose $Y = g(X) = aX +b$; then $E(Y ) = aE(X)+b$.

The proof of this is simple and
given below for the continuous case. In the discrete case replace
integral ($\int$) by summation ($\sum$).
\begin{align*}
E(Y ) &= \int_{-\infty}^\infty (ax + b) f(x)dx \\
&= a \int_{-\infty}^\infty x f (x)dx + b \int_{-\infty}^\infty f (x)dx \\
&= aE(X) + b,
\end{align*}
using the total probability is 1 property
($-\infty f (x)dx = 1$) in the last integral.
 This is very convenient, e.g. suppose $E(X) = 5$ and
$Y = -2X + 549$ then $E(Y) = 539$.

### Variance

The variance measures the variability of a random variable and is defined by
\[\operatorname{Var}(X)=E(X-\mu)^{2}= \begin{cases}
\sum_{\text {all } x}(x-\mu)^{2} f(x) & \text { if $X$ is discrete} \\
\int_{-\infty}^{\infty}(x-\mu)^{2} f(x) dx & \text { if $X$ is continuous,}
\end{cases}\]
where $\mu=E(X)$, and when the sum or integral exists. 
They can't always be assumed to exist! 
<!-- TODO: check for similar note about expectation sometimes not existing? -->
When the variance exists, 
it is the expectation of $(X-\mu)^{2}$ where $\mu$ is the mean of $X$.
We now derive an easy formula to calculate the variance:
<!-- TODO: put into theorem environment -->
\[\operatorname{Var}(X)=E(X-\mu)^{2}=E\left(X^{2}\right)-\mu^{2}.\]
**Proof**:
\begin{align*}
\operatorname{Var}(X) &=E(X-\mu)^{2} \\
&=E\left(X^{2}-2 X \mu+\mu^{2}\right) \\
&=E\left(X^{2}\right)-2 \mu E(X)+\mu^{2} \\
&=E\left(X^{2}\right)-2 \mu \mu+\mu^{2} \\
&=E\left(X^{2}\right)-\mu^{2}.
\end{align*}
Thus:
<!-- TODO: highlight as key point -->
the variance of a random variable is the expected value of its square
minus the square of its expected value.

We usually denote the variance by $\sigma^2$. 
The square is there to emphasise that the variance of any
random variable is always non-negative. When can the variance be zero? 
When there is no variation
at all in the random variable, i.e. it takes only a single value µ with probability 1.
Hence, there is
nothing random about the random variable --- we can predict its outcome with certainty.

<!-- TODO: highlight as key point/definition -->
The square root of the variance is called the standard deviation of the
random variable.

<!-- TODO: fix this to not refer to uniform distribution until defined -->
:::{.example name="Uniform"}
Consider the uniform distribution which has the pdf $f(x)=\frac{1}{b-a}, \, a<x<b$.
\begin{align*}
E\left(X^{2}\right) &=\int_{a}^{b} \frac{x^{2}}{b-a} d x \\
&=\frac{b^{3}-a^{3}}{3(b-a)} \\
&=\frac{b^{2}+a b+a^{2}}{3}.
\end{align*}
Hence
\[\operatorname{Var}(X)=\frac{b^{2}+a b+a^{2}}{3}-\left(\frac{b+a}{2}\right)^{2}=\frac{(b-a)^{2}}{12},\]
after simplification.
:::

We prove one important property of the variance.

<!-- TODO: put in theorem environment -->
Suppose $Y=a X+b$ then $\operatorname{Var}(Y)=a^{2} \operatorname{Var}(X)$

The proof of this is simple and is given below for the continuous case. In the discrete case replace integral $\left(\int\right)$ by summation $\left(\sum\right)$.
\begin{align*}
\operatorname{Var}(Y) &=E(Y-E(Y))^{2} \\
&=\int_{-\infty}^{\infty}(a x+b-a \mu-b)^{2} f(x) d x \\
&=a^{2} \int_{-\infty}^{\infty}(x-\mu)^{2} f(x) d x \\
&=a^{2} \operatorname{Var}(X).
\end{align*}
<!-- TODO: redo proof just using linearity of expectation? -->

This is a very useful result, e.g. suppose $\operatorname{Var}(X)=25$ and $Y=-X+5,000,000$; then $\operatorname{Var}(Y)=$ $\operatorname{Var}(X)=25$ and the standard deviation, $\sigma=5$. In words a location shift, $b$, does not change variance but a multiplicative constant, $a$ say, gets squared in variance, $a^{2}$.

# Statistical Inference {#inference}

## Statistical modelling

### Introduction 

Statistical analysis (or inference) involves drawing conclusions, and making predictions and decisions, using the evidence provided to us by observed data. To do this we use probability distributions, often called statistical models, to describe the process by which the observed data were
generated. For example, we may suppose that the true proportion of
mature students is $p$,
$0 < p < 1$, and if we have selected $n$ students at random, 
that each of those students gives rise to a
Bernoulli distribution which takes the value $1$ if the student is a
mature student and $0$
otherwise. The
success probability of the Bernoulli distribution will be the unknown $p$. The underlying statistical
model is then the Bernoulli distribution.

To illustrate with another example, suppose we have observed fast food waiting times in the
morning and afternoon. If we treat time as continuous then 
the waiting time for each customer could potentially be modelled as a normal random variable. 

In general:

- The form of the assumed model helps us to understand the real-world process by which the
data were generated.
- If the model explains the observed data well, then it should also inform us about future
(or unobserved) data, and hence help us to make predictions (and decisions contingent on
unobserved data).
- The use of statistical models, together with a carefully constructed methodology for their
analysis, also allows us to quantify the uncertainty associated with any conclusions, predictions or decisions we make.

We will use the notation $x_1 , x_2 , \ldots , x_n$ to denote $n$ observations
of the random variables $X_1 , X_2 , \ldots , X_n$ (corresponding capital letters). 
For the fast food waiting
time example, we have $n = 20$, $x_1 = 38, x_2 = 100, \ldots, x_{20} = 70$, and 
$X_i$ is the waiting time for the $i$th person in the sample.
 
 
### Statistical models

Suppose we denote the complete data by the vector $\boldsymbol{x} = (x_1 , x_2 , \ldots , x_n)$ and use 
$\boldsymbol{X} = (X_1 , X_2 , \ldots , X_n)$
for the corresponding random variables. A statistical model specifies a probability distribution for
the random variables $\boldsymbol{X}$ corresponding to the data observations $\boldsymbol{x}$. Providing a specification for
the distribution of $n$ jointly varying random variables can be a daunting task, particularly if $n$ is
large. However, this task is made much easier if we can make some simplifying assumptions, such
as

1. $X_1 , X_2 , \ldots , X_n$ are independent random variables,
2. $X_1 , X_2 , \ldots , X_n$ have the same probability distribution 
(so $x_1 , x_2 , \ldots , x_n$ are observations of a single random variable $X$).

Assumption 1 depends on the sampling mechanism and is very common in practice. If we are
to make this assumption for the Southampton student sampling experiment, we need to select
randomly among all possible students. The assumption will be violated when samples are correlated either in time or in space, e.g. the daily air pollution level in
Southampton for the last year or the air pollution levels in two nearby locations in Southampton.
In this module we will only consider data sets where Assumption 1 is reasonable.

Assumption 2 is not
always appropriate, but is often reasonable when we are modelling a single variable. In the fast
food waiting time example, we must assume that there are no differences between the AM and PM
waiting times for Assumption 2 to hold.


If Assumption 1 and 2 both hold, we say that $X_1 , \ldots , X_{n}$ are 
independent and identically distributed (or i.i.d. for short).

### A fully specified model

Sometimes a model completely specifies the probability distribution of $X_1 , X_2 , \ldots , X_n$. 
For example, if we assume that the waiting time $X \sim N (\mu, \sigma^2)$ where $\mu = 100$, and 
$\sigma^2 = 100$, then this
is a fully specified model. In this case, there is no need to collect any data as there is no need
to make any inference about any unknown quantities, although we may use the data to judge the
plausibility of the model.
A fully specified model might be appropriate when there is some external
(to the data) theory as to why the model (in particular the values of $\mu$ and $\sigma^2$)
was appropriate.
Fully specified models such as this are uncommon as we rarely have external theory which allows
us to specify a model so precisely.

### A parametric statistical model

A parametric statistical model specifies a probability distribution for a random sample apart from
the value of a number of parameters in that distribution. This could be confusing in the first
instance -- a parametric model does not specify parameters! Here the word parametric signifies the
fact that the probability distribution is completely specified by a few parameters in the first place.
For example, the Poisson distribution is parameterised by the parameter $\lambda$ which happens 
to be the
mean of the distribution; the normal distribution is parameterised by two parameters, the mean $\mu$
and the variance $\sigma^2$.
When a parametric statistical model is assumed with some unknown parameters, statistical
inference methods use data to estimate the unknown parameters, e.g. $\lambda$, $\mu$, $\sigma^2$. 
Estimation will be
discussed in more detail in the following sections.


### A nonparametric statistical model

Sometimes it is not appropriate, or we want to avoid, making a precise specification for the 
distribution which generated $X_1 , X_2 , \ldots , X_n$. For example, when the data histogram 
does not show
a bell-shaped distribution, it would be wrong to assume a normal distribution for the data. In
such a case, although we can attempt to use some other non-bell-shaped parametric model, we can
decide altogether to abandon parametric models. We may then still assume that 
$X_1 , X_2 , \ldots , X_n$
are i.i.d. random variables, but from a nonparametric statistical model which cannot be written
down, having a probability function which only depends on a finite number of parameters. Such
analysis approaches are also called distribution-free methods.

:::{.example name="Computer failures"}
Let X denote the count of computer failures per week
from Example \@ref(exm:compfail), summarised in the following
table:

```{r, echo = FALSE}
center_tab(t(table(compfail)))
```

We want to estimate how often will the
computer system fail at least once per week in the next year? The answer is $52 \times (1 - P (X = 0))$.
But how would you estimate $P (X = 0)$? Consider two approaches.

1. **Nonparametric**. Estimate $P (X = 0)$ by the relative frequency of number of zeros in the
above sample, which is 12 out of 104. Thus our estimate of $P (X = 0)$ is $12/104$. Hence,
our estimate of the number of weeks when there will be at least one computer failure is
$52 \times (1 - 12/104) = 46$.
2. **Parametric**. Suppose we assume that $X$ follows the Poisson distribution with parameter 
$\lambda$.
Then the answer to the above question is
\[52 \times (1 - P (X = 0)) = 52 \times \left(1 - e^{-\lambda} \frac{\lambda^0}{0!} \right)
= 52 \times 1 - e^{-\lambda}\]
which involves the unknown parameter $\lambda$. For the Poisson distribution we know that $E(X) =
\lambda$. Hence we could use the sample mean $\bar X$ to estimate $E(X) = \lambda$. Thus we estimate
$\hat \lambda = \bar x = 3.75.$ This type of estimator is called a moment estimator.
Now our estimate of the number of weeks when there will be at least one computer failure is
$52 \times (1 - e^{-3.75}) = 50.78 \approx 51$, which is very different from our
answer of 46 from the nonparametric approach.
:::


### Should we prefer parametric or nonparametric and why?

The parametric approach should be preferred if the assumption of the Poisson distribution can
be justified for the data. For example, we can look at the data histogram or compare the fitted
probabilities of different values of $X$, i.e. 
\[\hat P (X = x) = e^{-\hat \lambda} \frac{\hat \lambda}{x!},\]
with the relative frequencies from the
sample. In general, often model-based analysis is preferred because it is more precise and accurate,
and we can find estimates of uncertainty in such analysis based on the structure of the model. We
shall see this later.

The nonparametric approach should be preferred if the model cannot be justified for the data,
as in that case the parametric approach will provide incorrect answers.


## Estimation

### Introduction 

Once we have collected data and proposed a statistical model for our data, the initial statistical
analysis usually involves estimation.

- For a parametric model, we need to estimate the unknown (unspecified) parameter $\lambda$. For
example, if our model for the computer failure data is that they are i.i.d. Poisson, we need to
estimate the mean ($\lambda$) of the Poisson distribution.
- For a nonparametric model, we may want to estimate the properties of the data-generating
distribution. For example, if our model for the computer failure data is that they are i.i.d.,
following the distribution of an unspecified common random variable $X$, then we may want
to estimate $\mu = E(X)$ or $\sigma^2 = \operatorname{Var}(X)$.

In the following, we use the generic notation $\theta$ to denote the *estimand*
(what we want to estimate
or the parameter). For example, $\theta$ is the parameter $\lambda$ in the first example, and 
$\theta$ may be either $\mu$
or $\sigma^2$ or both in the second example.



### Population and sample


Recall that a statistical model specifies a probability distribution for the random variables $\boldsymbol{X}$ corresponding to the data observations $\boldsymbol{x}$.

- The observations $\boldsymbol{x} = (x_1 , \ldots , x_n )$ are called the sample, and quantities derived from the
sample are sample quantities. For example, as in Chapter 1, we call
\[\bar x = \frac{1}{n} \sum_{i=1}^n x_i\]
the sample mean.
- The probability distribution for $X$ specified in our model represents all possible observations
which might have been observed in our sample, and is therefore sometimes referred to as the
population. Quantities derived from this distribution are population quantities.
For example, if our model is that $X_1 , \ldots , X_n$ are i.i.d., following 
the common distribution of
a random variable $X$, then we call $E(X)$ the *population mean*.

### Statistic and estimator

A statistic $T(\boldsymbol{x})$ is any function of the observed data $x_{1}, \ldots, x_{n}$ alone (and therefore does not depend on any parameters or other unknowns).

An estimate of $\theta$ is any statistic which is used to estimate $\theta$ under a particular statistical model. We will use $\tilde{\theta}(\boldsymbol{x})$ (sometimes shortened to $\tilde{\theta}$ ) to denote an estimate of $\theta$.

An estimate $\tilde{\theta}(\boldsymbol{x})$ is an observation of a corresponding random variable $\tilde{\theta}(\boldsymbol{X})$ which is called an estimator. Thus an estimate is a particular observed value, e.g. 1.2, but an estimator is a random variable which can take values which are called estimates.

An estimate is a particular numerical value, e.g. $\bar{x}$; an estimator is a random variable, e.g. $\bar{X}$.

The probability distribution of any estimator $\tilde{\theta}(\boldsymbol{X})$ is called its sampling distribution. The estimate $\tilde{\theta}(\boldsymbol{x})$ is an observed value (a number), and is a single observation from the sampling distribution of $\tilde{\theta}(\boldsymbol{X})$.

:::{.example}
Suppose that we have a random sample $X_{1}, \ldots, X_{n}$ from the uniform distribution on the interval $(0, \theta)$ where $\theta>0$ is unknown. Suppose that $n=5$ and we have the sample observations $x_{1}=2.3, x_{2}=3.6, x_{3}=20.2, x_{4}=0.9, x_{5}=17.2$. Our objective is to estimate $\theta$. How can we proceed?

Here the pdf $f(x)=\frac{1}{\theta}$ for $0 \leq x \leq \theta$ and 0 otherwise. Hence $E(X)=\int_{0}^{\theta} \frac{1}{\theta} x d x=\frac{\theta}{2}$. There are many possible estimators for $\theta$, e.g. $\hat{\theta}_{1}(\boldsymbol{X})=2 \bar{X}$, which is motivated by the method of moments because $\theta=2 E(X)$. A second estimator is $\hat{\theta}_{2}(\boldsymbol{X})=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$, which is intuitive since $\theta$ must be greater than or equal to all observed values and thus the maximum of the sample value will be closest to $\theta$. 

How could we choose between the two estimators $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ ? This is where we need to learn the sampling distribution of an estimator to determine which estimator will be unbiased, i.e. correct on average, and which will have minimum variability. We will formally define these in a minute, but first let us derive the sampling distribution, i.e. the pdf, of $\hat{\theta}_{2}$. Note that $\hat{\theta}_{2}$ is a random variable since the sample $X_{1}, \ldots, X_{n}$ is random. We will first find its cdf and then differentiate the cdf to get the pdf. For ease of notation, suppose $Y=\hat{\theta}_{2}(\boldsymbol{X})=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$. For any $0<y<\theta$, the cdf of $Y, F(y)$ is given by
\begin{align*}
P(Y \leq y) &=P\left(\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\} \leq y\right) \\
&\left.=P\left(X_{1} \leq y, X_{2} \leq y, \ldots, X_{n} \leq y\right)\right) \quad \text{since max $\leq y$ if and only if each $\leq y$}] \\
&=P\left(X_{1} \leq y\right) P\left(X_{2} \leq y\right) \cdots P\left(X_{n} \leq y\right) \quad \text {since the $X_i$ are independent}\\
&=\frac{y}{\theta} \times \frac{y}{\theta} \times \cdots \times \frac{y}{\theta} \\
&=\left(\frac{y}{\theta}\right)^{n} .
\end{align*}


Now the pdf of $Y$ is 
\[f(y)=\frac{d F(y)}{d y}=n \frac{y^{n-1}}{\theta^{n}}, \quad 0 \leq y \leq \theta.\]
Using this pdf, it can be shown that $E\left(\hat{\theta}_{2}\right)=E(Y)=\frac{n}{n+1} \theta,$ and 
\[\operatorname{Var}\left(\hat{\theta}_{2}\right)=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}.\] 
:::


### Bias and mean square error

In the uniform distribution example we saw that the estimator 
$\hat{\theta}_{2}=Y=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ is a random variable and its 
pdf is given by $f(y)=n \frac{y^{n-1}}{\theta^{n}}$ for $0 \leq y \leq \theta$. 
This probability distribution is called the sampling distribution of $\hat{\theta}_{2}$.
From this we have seen that $E\left(\hat{\theta}_{2}\right)=\frac{n}{n+1} \theta$.

In general, we define the bias of an estimator $\tilde{\theta}(\boldsymbol{X})$ of $\theta$ to be
\[\operatorname{bias}(\tilde{\theta})=E(\tilde{\theta})-\theta.\]

An estimator $\tilde{\theta}(\boldsymbol{X})$ is said to be unbiased if
\[\operatorname{bias}(\tilde{\theta})=0 \text {, i.e. if } E(\tilde{\theta})=\theta.\]

So an estimator is unbiased if the expectation of its sampling distribution is equal to the quantity we are trying to estimate. Unbiased means "getting it right on average", i.e. under repeated sampling (relative frequency interpretation of probability).

Thus for the uniform distribution example, $\hat{\theta}_{2}$ is a biased estimator of $\theta$ and
\[\operatorname{bias}\left(\hat{\theta}_{2}\right)=E\left(\hat{\theta}_{2}\right)-\theta=\frac{n}{n+1} \theta-\theta=-\frac{1}{n+1} \theta,\]
which goes to zero as $n \rightarrow \infty$. However, $\hat{\theta}_{1}=2 \bar{X}$ is unbiased since $E\left(\hat{\theta}_{1}\right)=2 E(\bar{X})=2 \frac{\theta}{2}=\theta$. 

Unbiased estimators are "correct on average", but that does not mean that they are guaranteed to provide estimates which are close to the estimand $\theta$. A better measure of the quality of an estimator than bias is the mean squared error (or MSE), defined as
\[\operatorname{MSE}(\tilde{\theta})=E\left[(\tilde{\theta}-\theta)^{2}\right]\]

 Therefore, if $\tilde{\theta}$ is unbiased for $\theta$, i.e. if $E(\tilde{\theta})=\theta$,
  then MSE $(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})$. In general, we have the following result:

::: {.theorem}
\[\operatorname{MSE}(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2}\]
:::
  
The proof is similar to the proof of Theorem \@ref(thm:sse).

::: {.proof}
\begin{align*}
\operatorname{MSE}(\tilde{\theta}) &=E\left[(\tilde{\theta}-\theta)^{2}\right] \\
&=E\left[(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}\right] \\
&=E\left[(\tilde{\theta}-E(\tilde{\theta}))^{2}+(E(\tilde{\theta})-\theta)^{2}+2(\tilde{\theta}-E(\tilde{\theta}))(E(\tilde{\theta})-\theta)\right] \\
&=E[\tilde{\theta}-E(\tilde{\theta})]^{2}+E[E(\tilde{\theta})-\theta]^{2}+2 E[(\tilde{\theta}-E(\tilde{\theta}))(E(\tilde{\theta})-\theta)] \\
&=\operatorname{Var}(\tilde{\theta})+[E(\tilde{\theta})-\theta]^{2}+2(E(\tilde{\theta})-\theta) E[(\tilde{\theta}-E(\tilde{\theta}))] \\
&=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2}+2(E(\tilde{\theta})-\theta)[E(\tilde{\theta})-E(\tilde{\theta})] \\
&=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2} .
\end{align*}
:::

:::{ .example}
Continuing with the uniform distribution $U(0, \theta)$ example, we have seen that $\hat{\theta}_{1}=2 \bar{X}$ is unbiased for $\theta$ but $\operatorname{bias}\left(\hat{\theta}_{2}\right)=-\frac{1}{n+1} \theta$. How do these estimators compare with respect to the MSE? Since $\hat{\theta}_{1}$ is unbiased, its MSE is its variance. Later, we will prove that for random sampling from any population
\[\operatorname{Var}(\bar{X})=\frac{\operatorname{Var}(X)}{n},\]
where $\operatorname{Var}(X)$ is the variance of the population sampled from. 
Returning to our example, we know that if $X \sim U(0, \theta)$ then $\operatorname{Var}(X)=\frac{\theta^{2}}{12}$. Therefore we have
\[
\operatorname{MSE}\left(\hat{\theta}_{1}\right)=\operatorname{Var}\left(\hat{\theta}_{1}\right)=\operatorname{Var}(2 \bar{X})=4 \operatorname{Var}(\bar{X})=4 \frac{\theta^{2}}{12 n}=\frac{\theta^{2}}{3 n} .
\]

Now, for $\hat{\theta}_{2}$ we know that:

1. $\operatorname{Var}\left(\hat{\theta}_{2}\right)=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}$
2. $\operatorname{bias}\left(\hat{\theta}_{2}\right)=-\frac{1}{n+1} \theta$.

Now
\begin{align*}
\operatorname{MSE}\left(\hat{\theta}_{2}\right) &=\operatorname{Var}\left(\hat{\theta}_{2}\right)+\operatorname{bias}\left(\hat{\theta}_{2}\right)^{2} \\
&=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}+\frac{\theta^{2}}{(n+1)^{2}} \\
&=\frac{\theta^{2}}{(n+1)^{2}}\left(\frac{n}{n+2}+1\right) \\
&=\frac{\theta^{2}}{(n+1)^{2}} \frac{2 n+2}{n+2} .
\end{align*}
The MSE of $\hat{\theta}_{2}$ is an order of magnitude smaller than the MSE of $\hat{\theta}_{1}$
(of order $1/n^{2}$ rather than $1/n$), providing justification for the preference of $\hat{\theta}_{2}=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ as an estimator of $\theta$.
:::

## Estimating the population mean

### Introduction

Often, one of the main tasks of a statistician is to estimate a population average or mean. However the estimates, using whatever procedure, will not be usable or scientifically meaningful if we do not know their associated uncertainties. For example, a statement such as: "the Arctic ocean will be completely ice-free in the summer in the next few decades" provides little information as it does not communicate the extent or the nature of the uncertainty in it. Perhaps a more precise statement could be: "the Arctic ocean will be completely ice-free in the summer some time in the next 20-30 years". This last statement not only gives a numerical value for the number of years for complete ice-melt in the summer, but also acknowledges the uncertainty of $\pm 5$ years in the estimate. A statistician's main job is to estimate such uncertainties. In this lecture, we will get started with estimating uncertainties when we estimate a population mean. We will introduce the standard error of an estimator.

### Estimation of a population mean

Suppose that $X_{1}, \ldots, X_{n}$ is a random sample from any probability distribution $f(x)$, which may be discrete or continuous. Suppose that we want to estimate the unknown population mean $E(X)=\mu$ and variance, $\operatorname{Var}(X)=\sigma^{2}$. In order to do this, it is not necessary to make any assumptions about $f(x)$, so this may be thought of as nonparametric inference.

We have the following results: 

::: {.theorem #sample-mean-unbiased}
Suppose $X_1, \ldots, X_n$ is a random sample, 
with $E(X)=\mu$. The sample mean
\[\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}\]
is an unbiased estimator of $\mu=E(X)$, i.e. $E(\bar{X})=\mu$.
:::

In other words, the sample mean is an unbiased estimator of the population mean.

::: {.proof}
We have
\[E[\bar{X}]=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} E(X)=E(X)\]
so $\bar{X}$ is an unbiased estimator of $E(X)$.
:::

::: {.theorem #var-sample-mean}
Suppose $X_1, \ldots, X_n$ is a random sample, with $\operatorname{Var}(X)=\sigma^{2}$.
Then
\[\operatorname{Var}(\bar{X})=\frac{\sigma^{2}}{n}.\]
:::

::: {.proof}
We use the result that for independent random variables the variance of the sum is the sum of the variances from Section \@ref(sec:sum-rvs) . Thus,
\[\operatorname{Var}[\bar{X}]=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}(X)=\frac{n}{n^{2}} \operatorname{Var}(X)=\frac{\sigma^{2}}{n},\]
:::

Together, Theorems \@ref(thm:sample-mean-unbiased) and \@ref(thm:var-sample-mean)
imply that the MSE of $\bar{X}$ is $\operatorname{Var}(X) / n$.

::: {.theorem}
Suppose $X_1, \ldots, X_n$ is a random sample, 
with $\operatorname{Var}(X)=\sigma^{2}$. The sample variance with divisor $n-1$
\[S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\]
is an unbiased estimator of $\sigma^{2}$, i.e. $E\left(S^{2}\right)=\sigma^{2}$.
:::

In other words, the sample variance is an unbiased estimator of the population variance.

::: {.proof}
We need to show
$E\left(S^{2}\right)=\sigma^{2}$. We have
\[S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right].\]
To evaluate the expectation of the above, we need $E\left(X_{i}^{2}\right)$ and $E\left(\bar{X}^{2}\right)$. In general, we know for any random variable,
\[\operatorname{Var}(Y)=E\left(Y^{2}\right)-(E(Y))^{2} \quad \Rightarrow E\left(Y^{2}\right)=\operatorname{Var}(Y)+(E(Y))^{2}.\]
Thus, we have
\[E\left(X_{i}^{2}\right)=\operatorname{Var}\left(X_{i}\right)+\left(E\left(X_{i}\right)\right)^{2}=\sigma^{2}+\mu^{2},\]
and
\[E\left(\bar{X}^{2}\right)=\operatorname{Var}(\bar{X})+(E(\bar{X}))^{2}=\sigma^{2} / n+\mu^{2},\]
from Theorems \@ref(thm:sample-mean-unbiased) and \@ref(thm:var-sample-mean).
So
\begin{align*}
E\left(S^{2}\right) &=E\left\{\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right]\right\} \\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n} E\left(X_{i}^{2}\right)-n E\left(\bar{X}^{2}\right)\right] \\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n}\left(\sigma^{2}+\mu^{2}\right)-n\left(\sigma^{2} / n+\mu^{2}\right)\right] \\
&=\frac{1}{n-1}\left[n \sigma^{2}+n \mu^{2}-\sigma^{2}-n \mu^{2}\right] \\
&=\sigma^{2} \equiv \operatorname{Var}(X) .
\end{align*}
:::


### Standard deviation and standard error

For an unbiased  estimator $\tilde{\theta}$,
\[\operatorname{MSE}(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})\]
and therefore the sampling variance of the estimator is an important summary of its quality.

We usually prefer to focus on the standard deviation of the sampling distribution of $\tilde{\theta}$,
\[\text {s.d.}(\tilde{\theta})=\sqrt{\operatorname{Var}(\tilde{\theta})}.\]

In practice we will not know s.d.$(\tilde{\theta})$, as it will typically depend on unknown features of the distribution of $X_{1}, \ldots, X_{n}$. However, we may be able to estimate s.d.$(\tilde{\theta})$ using the observed sample $x_{1}, \ldots, x_{n}$. We define the standard error, s.e.$(\tilde{\theta})$, of an estimator $\tilde{\theta}$ to be an estimate of the standard deviation of its sampling distribution, s.d.$(\tilde{\theta})$.

Standard error of an estimator is an estimate of the standard deviation of its sampling distribution

We proved that
\[\operatorname{Var}[\bar{X}]=\frac{\sigma^{2}}{n} \Rightarrow \text { s.d. }(\bar{X})=\frac{\sigma}{\sqrt{n}}.\]

As $\sigma$ is unknown, we cannot calculate this standard deviation. However, we know that $E\left(S^{2}\right)=\sigma^{2}$, i.e. that the sample variance is an unbiased estimator of the population variance. Hence $S^{2} / n$ is an unbiased estimator for $\operatorname{Var}(\bar{X})$. Therefore we obtain the standard error of the mean, s.e. $(\bar{X})$, by plugging in the estimate
\[s=\left(\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)^{1 / 2}\]
   of $\sigma$ into s.d.$(\bar{X})$ to obtain
   \[\text {s.e.}(\bar{X})=\frac{s}{\sqrt{n}}.\]

<!-- TODO: refer back to computer failure data in C1 -->
Therefore, for the computer failure data, our estimate, $\bar{x}=3.75$, for the population mean is associated with a standard error
\[\text{s.e.}(\bar{X})=\frac{3.381}{\sqrt{104}}=0.332.\]

Note that this is "a" standard error, so other standard errors may be available. Indeed, for parametric inference, where we make assumptions about $f(x)$, alternative standard errors are available. 
For example, if $X_{1}, \ldots, X_{n}$ are i.i.d. $\operatorname{Poisson}(\lambda)$ random variables,
$E(X)=\lambda$, so $\bar{X}$ is an unbiased estimator of $\lambda$. $\operatorname{Var}(X)=\lambda$, so another $\text{s.e.}(\bar{X})=\sqrt{\hat{\lambda} / n}=\sqrt{\bar{x} / n}$.
   In the computer failure data example, this is $\sqrt{\frac{3.75}{104}}=0.19$.

## Interval estimation

### Introduction

An estimate $\tilde{\theta}$ of a parameter $\theta$ is sometimes referred to as a point estimate. The usefulness of a point estimate is enhanced if some kind of measure of its precision can also be provided. Usually, for an unbiased estimator, this will be a standard error, an estimate of the standard deviation of the associated estimator, as we have discussed previously. An alternative summary of the information provided by the observed data about the location of a parameter $\theta$ and the associated precision is an interval estimate or confidence interval.

Suppose that $x_{1}, \ldots, x_{n}$ are observations of random variables $X_{1}, \ldots, X_{n}$ whose joint pdf is specified apart from a single parameter $\theta$. To construct a confidence interval for $\theta$, we need to find a random variable $T(\mathbf{X}, \theta)$ whose distribution does not depend on $\theta$ and is therefore known. This random variable $T(\mathbf{X}, \theta)$ is called a *pivot* for $\theta$. Hence we can find numbers $h_{1}$ and $h_{2}$ such that
\begin{equation}
P\left(h_{1} \leq T(\mathbf{X}, \theta) \leq h_{2}\right)=1-\alpha
(\#eq:pivot)
\end{equation}
where $1-\alpha$ is any specified probability. If \@ref(eq:pivot)  can be 'inverted',we can write it as
\[P\left(g_{1}(\mathbf{X}) \leq \theta \leq g_{2}(\mathbf{X})\right)=1-\alpha.\]

Hence with probability $1-\alpha$, the parameter $\theta$ will lie between the random variables $g_{1}(\mathbf{X})$ and $g_{2}(\mathbf{X})$. Alternatively, the random interval $\left(g_{1}(\mathbf{X}), g_{2}(\mathbf{X})\right)$ includes $\theta$ with probability $1-\alpha$. Now, when we observe $x_{1}, \ldots, x_{n}$, we observe a single observation of the random interval $\left(g_{1}(\mathbf{X}), g_{2}(\mathbf{X})\right)$, which can be evaluated as $\left(g_{1}(\mathbf{x}), g_{2}(\mathbf{x})\right)$. We do not know if $\theta$ lies inside or outside this interval, but we do know that if we observed repeated samples, then $100(1-\alpha) \%$ of the resulting intervals would contain $\theta$. Hence, if $1-\alpha$ is high, we can be reasonably confident that our observed interval contains $\theta$. We call the observed interval $\left(g_{1}(\mathbf{x}), g_{2}(\mathbf{x})\right)$ a $100(1-\alpha) \%$ confidence interval for $\theta$. It is common to present intervals with high confidence levels, usually $90 \%, 95 \%$ or $99 \%$, so that $\alpha=0.1,0.05$ or $0.01$ respectively.

### Confidence interval for a normal mean

Let $X_{1}, \ldots, X_{n}$ be i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables. 
From the Central Limit Theorem (Section \@ref(sec:clt)), we know that for large $n$,
\[\bar{X} \sim N\left(\mu, \sigma^{2} / n\right) \quad \Rightarrow \quad \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \sim N(0,1).\]

Suppose we know that $\sigma=10$, so $\sqrt{n}(\bar{X}-\mu) / \sigma$ is a pivot for $\mu$. Then we can use the distribution function of the standard normal distribution to find values $h_{1}$ and $h_{2}$ such that
\[P\left(h_{1} \leq \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \leq h_{2}\right)=1-\alpha\]
for a chosen value of $1-\alpha$ which is called the confidence level. So $h_{1}$ and $h_{2}$ are chosen so that the shaded area in Figure \@ref(fig:normal-quantiles) is equal to the confidence level $1-\alpha$.

```{r normal-quantiles, echo = FALSE, fig.cap = "$h_1$ and $h_2$ are chosen to make the shaded area equal to the confidence level $1-\\alpha$", fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
library(ggplot2)

lim <- 4
h <- qnorm(0.975)
plot_data <- tibble(x = seq(-h, h, length.out = 100),
                    ymax = dnorm(x),
                    ymin = 0)

plot_data %>%
    ggplot(mapping = aes(x = x, ymin = ymin, ymax = ymax)) + 
    geom_function(fun = dnorm) +
    geom_ribbon(alpha = 0.3) +
    xlab("x") +
    ylab("f(x)") +
    scale_x_continuous(breaks = c(0, -h, h),
                       labels = c(0, "h1", "h2"),
                       limits = c(-lim, lim)) +
    geom_hline(aes(yintercept = 0)) +
    theme(panel.grid.minor = element_blank(),
          panel.grid = element_blank())
```

It is common practice to make the interval symmetric, so that the two unshaded areas are equal
(to $\alpha / 2$ ), in which case
\[-h_{1}=h_{2} \equiv h \quad \text { and } \quad \Phi(h)=1-\frac{\alpha}{2}.\]

The most common choice of confidence level is $1-\alpha=0.95$, in which case $h=1.96=\texttt{qnorm(0.975)}$. You may also occasionally see $90\%$ ($h=1.645=\texttt{qnorm(0.95)}$) or 
$99\%$ ($h=2.58=\texttt{qnorm(0.995)}$) intervals. 

Therefore we have
\begin{align*}
& P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \leq 1.96\right)=0.95 \\
\Rightarrow & P\left(\bar{X}-1.96 \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}+1.96 \frac{\sigma}{\sqrt{n}}\right)=0.95 .
\end{align*}

Hence, $\bar{X}-1.96 \frac{\sigma}{\sqrt{n}}$ and $\bar{X}+1.96 \frac{\sigma}{\sqrt{n}}$ are the endpoints of a random interval which includes $\mu$ with probability $0.95$. The observed value of this interval, $\left(\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}\right)$, is called a 95\% confidence interval for $\mu$.

:::{.example name="Fast food waiting times"} 
For the fast food waiting time data, we have $n=20$ data points combined from the morning and afternoon data sets. We have $\bar{x}=67.85$ and $n=20$. Hence, under the normal model assuming (just for the sake of illustration) $\sigma=18$, a 95\% confidence interval for $\mu$ is

\[\begin{gathered}
67.85-1.96(18 / \sqrt{20}) \leq \mu \leq 67.85+1.96(18 / \sqrt{20}) \\
\Rightarrow 59.96 \leq \mu \leq 75.74
\end{gathered}\]

The R command is 
```{r, eval = FALSE}
mean(fastfood) + c(-1, 1) * qnorm(0.975) * 18 / sqrt (20)
```
assuming `fastfood` is the vector containing 20 waiting times.

In reality, it is more likely that $\sigma$ is unknown in this example,
and we need to seek alternative methods for finding the confidence intervals.
:::


### Some remarks about confidence intervals

1. Notice that $\bar{x}$ is an unbiased estimate of $\mu, \sigma / \sqrt{n}$ is the standard error of the estimate and $1.96$ (in general $h$ in the above discussion) is a critical value from the associated known sampling distribution. The formula $(\bar{x} \pm 1.96 \sigma / \sqrt{n})$ for the confidence interval is then generalised as:
\[\text { Estimate } \pm \text { Critical value } \times \text { Standard error }\]
where the estimate is $\bar{x}$, the critical value is $1.96$ and the standard error is $\sigma / \sqrt{n}$. This is so much easier to remember. We will see that this formula holds in many of the following examples, but not all.

2. Confidence intervals are frequently used, but also frequently misinterpreted. A $100(1-\alpha) \%$ confidence interval for $\theta$ is a single observation of a random interval which, under repeated sampling, would include $\theta$ $100(1-\alpha) \%$ of the time.

3. A confidence interval is not a probability interval. You should avoid making statements like $P(1.3<\theta<2.2)=0.95$. In the classical approach to statistics you can only make probability statements about random variables, and $\theta$ is assumed to be a constant.

4. If a confidence interval is interpreted as a probability interval, this may lead to problems. For example, suppose that $X_{1}$ and $X_{2}$ are i.i.d. $U\left(\theta-\frac{1}{2}, \theta+\frac{1}{2}\right)$ random variables. Then $P\left(\min \left(X_{1}, X_{2}\right)<\theta<\max \left(X_{1}, X_{2}\right)\right)=\frac{1}{2}$ so $\left(\min \left(x_{1}, x_{2}\right), \max \left(x_{1}, x_{2}\right)\right)$ is a $50 \%$ confidence interval for $\theta$, where $x_{1}$ and $x_{2}$ are the observed values of $X_{1}$ and $X_{2}$. Now suppose that $x_{1}=0.3$ and $x_{2}=0.9$. What is $P(0.3<\theta<0.9)$ ?

### Confidence intervals using the CLT

#### Introduction 

Confidence intervals are generally difficult to find. The difficulty lies in finding a pivot, i.e. a statistic $T(\mathbf{X}, \theta)$ such that
\[P\left(h_{1} \leq T(\mathbf{X}, \theta) \leq h_{2}\right)=1-\alpha\]
for two suitable numbers $h_{1}$ and $h_{2}$, and also that the above can be inverted to put the unknown $\theta$ in the middle of the inequality inside the probability statement. One solution to this problem is to use the powerful Central Limit Theorem (CLT) to claim normality, and then basically follow the above normal example for known variance.

#### Confidence intervals for $\mu$ using the CLT

The CLT allows us to assume the large sample approximation
\[\sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \stackrel{\text { approx }}{\sim} N(0,1) \text { as } n \rightarrow \infty.\]
Thus an (approximate) $95 \%$ confidence interval (CI) for $\mu$ is given by $\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}$. But note that $\sigma$ is unknown so this CI cannot be used unless we can estimate $\sigma$, i.e. replace the unknown s.d. of $\bar{X}$ by its estimated standard error. In this case, we get the CI in the familiar form:
\[\text { Estimate } \pm \text { Critical value } \times \text { Standard error }\]

Suppose that we do not assume any distribution for the sampled random variable $X$ but assume only that $X_{1}, \ldots, X_{n}$ are i.i.d, following the distribution of $X$ where $E(X)=\mu$ and $\operatorname{Var}(X)=\sigma^{2}$. We know that the standard error of $\bar{X}$ is $s / \sqrt{n}$ where $s$ is the sample standard deviation with divisor $n-1$. Then the following provides an (approximate) $95 \%$ CI for $\mu$:
\[\bar{x} \pm 1.96 \frac{s}{\sqrt{n}}.\]

:::{.example name="Computer failures"}
For the computer failure data, $\bar{x}=3.75, s=3.381$ and $n=104$. Under the model that the data are observations of i.i.d. random variables with population mean $\mu$ (but no other assumptions about the underlying distribution), we compute a 95\% confidence interval for $\mu$ to be
\[\left(3.75-1.96 \frac{3.381}{\sqrt{104}}, 3.75+1.96 \frac{3.381}{\sqrt{104}}\right)=(3.10,4.40).\]
:::

If we can assume a distribution for $X$, i.e. a parametric model for $X$, then we can do slightly better in estimating the standard error of $\bar{X}$ and as a result we can improve upon the previously obtained $95 \%$ CI. Two examples follow.

:::{.example name="Poisson" #poisson-ci-simple}
If $X_{1}, \ldots, X_{n}$ are modelled as i.i.d. Poisson $(\lambda)$ random variables, then $\mu=\lambda$ and $\sigma^{2}=\lambda$. We know $\operatorname{Var}(\bar{X})=\sigma^{2} / n=\lambda / n$. Hence a standard error is $\sqrt{\hat{\lambda} / n}=\sqrt{\bar{x} / n}$ since $\hat{\lambda}=\bar{X}$ is an unbiased estimator of $\lambda$. Thus a 95\% CI for $\mu=\lambda$ is given by
\[\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}}{n}}.\]

For the computer failure data, $\bar{x}=3.75, s=3.381$ and $n=104$. Under the model that the data are observations of i.i.d. random variables following a Poisson distribution with population mean $\lambda$, we compute a $95 \%$ confidence interval for $\lambda$ as
\[\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}}{n}}=3.75 \pm 1.96 \sqrt{3.75 / 104}=(3.38,4.12).\]

We see that this interval is narrower $(0.74=4.12-3.38)$ than the earlier interval $(3.10,4.40)$, which has a length of $1.3$. We prefer narrower confidence intervals as they facilitate more accurate inference regarding the unknown parameter.
:::

:::{.example name="Bernoulli"}
If $X_{1}, \ldots, X_{n}$ are modelled as i.i.d. Bernoulli $(p)$ random variables, then $\mu=p$ and $\sigma^{2}=p(1-p)$. We know $\operatorname{Var}(\bar{X})=\sigma^{2} / n=p(1-p) / n$. Hence a standard error is $\sqrt{\hat{p}(1-\hat{p}) / n}=\sqrt{\bar{x}(1-\bar{x}) / n}$, since $\hat{p}=\bar{X}$ is an unbiased estimator of $p$. Thus a $95 \%$ CI for $\mu=p$ is given by
\[\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}(1-\bar{x})}{n}}.\]

For the example, suppose $\bar{x}=0.2$ and $n=10$. Then we obtain the $95 \%$ CI as
\[0.2 \pm 1.96 \sqrt{(0.2 \times 0.8) / 10}=(-0.048,0.448)\]

This is wrong as $n$ is too small for the large sample approximation to be accurate. Hence we need to look for other alternatives which may work better.
:::

#### Confidence interval for a Bernoulli $p$ by quadratic inversion

It turns out that for the Bernoulli and Poisson distributions we can find alternative confidence intervals without using the approximation for standard error but still using the CLT. This is more complicated and requires us to solve a quadratic equation. We consider the two distributions separately.

We start with the CLT and obtain the following statement:
\[\begin{array}{cc} 
& P\left(-1.96 \leq \frac{\sqrt{n}(\bar{X}-p)}{\sqrt{p(1-p)}} \leq 1.96\right)=0.95 \\
\Leftrightarrow & P(-1.96 \sqrt{p(1-p)} \leq \sqrt{n}(\bar{X}-p) \leq 1.96 \sqrt{p(1-p)})=0.95 \\
\Leftrightarrow & P(-1.96 \sqrt{p(1-p) / n} \leq(\bar{X}-p) \leq 1.96 \sqrt{p(1-p) / n})=0.95 \\
\Leftrightarrow & P(p-1.96 \sqrt{p(1-p) / n} \leq \bar{X} \leq p+1.96 \sqrt{p(1-p) / n})=0.95 \\
\Leftrightarrow & P(L(p) \leq \bar{X} \leq R(p))=0.95,
\end{array}\]
where $L(p)=p-h \sqrt{p(1-p) / n}, R(p)=p+h \sqrt{p(1-p) / n}, h=1.96$. Now, consider the inverse mappings $L^{-1}(x)$ and $R^{-1}(x)$ so that
\begin{align*}
&P[L(p) \leq \bar{X} \leq R(p)]=0.95 \\
&\Leftrightarrow P\left[R^{-1}(\bar{X}) \leq p \leq L^{-1}(\bar{X})\right]=0.95
\end{align*}

which now defines our confidence interval $\left(R^{-1}(\bar{X}), L^{-1}(\bar{X})\right)$ for $p$. We can obtain $R^{-1}(\bar{x})$ and $L^{-1}(\bar{x})$ by solving the equations $R(p)=\bar{x}$ and $L(p)=\bar{x}$ for $p$, treating $n$ and $\bar{x}$ as known quantities. Thus we have
\begin{align*}
& R(p)=\bar{x}, \quad L(p)=\bar{x} \\
& \Leftrightarrow \quad(\bar{x}-p)^{2}=h^{2} p(1-p) / n, \quad \text { where } h=1.96 \\
& \Leftrightarrow \quad p^{2}\left(1+h^{2} / n\right)-p\left(2 \bar{x}+h^{2} / n\right)+\bar{x}^{2}=0
\end{align*}
The endpoints of the confidence interval are the roots of the quadratic. Hence, the endpoints of the $95 \%$ confidence interval for $p$ are
\begin{align*}
& \frac{\left(2 \bar{x}+\frac{h^{2}}{n}\right) \pm\left[\left(2 \bar{x}+\frac{h^{2}}{n}\right)^{2}-4 \bar{x}^{2}\left(1+\frac{h^{2}}{n}\right)\right]^{1 / 2}}{2\left(1+\frac{h^{2}}{n}\right)} \\
=& \frac{\left(\bar{x}+\frac{h^{2}}{2 n}\right) \pm\left[\left(\bar{x}+\frac{h^{2}}{2 n}\right)^{2}-\bar{x}^{2}\left(1+\frac{h^{2}}{n}\right)\right]^{1 / 2}}{\left(1+\frac{h^{2}}{n}\right)} \\
=& \frac{\bar{x}+\frac{h^{2}}{2 n} \pm \frac{h}{\sqrt{n}}\left[\frac{h^{2}}{4 n}+\bar{x}(1-\bar{x})\right]^{1 / 2}}{\left(1+\frac{h^{2}}{n}\right)} .
\end{align*}

This is sometimes called the Wilson Score Interval. 

<!-- The following R code calculates this for given $n, \bar{x}$ and confidence level $\alpha$ which determines the value of $h$.  -->
<!-- TODO: find R code? -->

Returning to the previous example, $n=10$ and $\bar{x}=0.2$, the $95 \%$ CI obtained from this method is $(0.057,0.510)$ compared to the previous illegitimate one $(-0.048,0.448)$. In fact you can see that the intervals obtained by quadratic inversion are more symmetric and narrower as $n$ increases, and are also more symmetric for $\bar{x}$ closer to $0.5$:

\begin{center}
\begin{tabular}{cccccc}
\hline$n$ & $\bar{x}$ & \multicolumn{2}{c}{ Quadratic inversion } & \multicolumn{2}{c}{ Plug-in s.e. estimation } \\
& & Lower end & Upper end & Lower end & Upper end \\
\hline 10 & $0.2$ & $0.057$ & $0.510$ & $-0.048$ & $0.448$ \\
10 & $0.5$ & $0.237$ & $0.763$ & $0.190$ & $0.810$ \\
20 & $0.1$ & $0.028$ & $0.301$ & $-0.031$ & $0.231$ \\
20 & $0.2$ & $0.081$ & $0.416$ & $0.025$ & $0.375$ \\
20 & $0.5$ & $0.299$ & $0.701$ & $0.281$ & $0.719$ \\
50 & $0.1$ & $0.043$ & $0.214$ & $0.017$ & $0.183$ \\
50 & $0.2$ & $0.112$ & $0.330$ & $0.089$ & $0.311$ \\
50 & $0.5$ & $0.366$ & $0.634$ & $0.361$ & $0.639$ \\
\hline
\end{tabular}
\end{center}

For smaller $n$ and $\bar{x}$ closer to 0 (or 1), the approximation required for the plug-in estimate of the standard error is insufficiently reliable. However, for larger $n$ it is adequate.

#### Confidence interval for a Poisson $\lambda$ by quadratic inversion

Here we proceed as in the Bernoulli case and using the CLT claim that a 95\% CI for $\lambda$ is given by
\[P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-\lambda)}{\sqrt{\lambda}} \leq 1.96\right)=0.95 \quad \Rightarrow P\left(n \frac{(\bar{X}-\lambda)^{2}}{\lambda} \leq 1.96^{2}\right)=0.95.\]

Now the confidence interval for $\lambda$ is found by solving the (quadratic) equality for $\lambda$ by treating $n, \bar{x}$ and $h$ to be known:
\begin{align*}
& n \frac{(\bar{x}-\lambda)^{2}}{\lambda}=h^{2}, \quad \text { where } h=1.96 \\
\Rightarrow & \bar{x}^{2}-2 \lambda \bar{x}+\lambda^{2}=h^{2} \lambda / n \\
\Rightarrow & \lambda^{2}-\lambda\left(2 \bar{x}+h^{2} / n\right)+\bar{x}^{2}=0 .
\end{align*}
Hence, the endpoints of the $95 \%$ confidence interval for $\lambda$ are:
\[\frac{\left(2 \bar{x}+\frac{h^{2}}{n}\right) \pm\left[\left(2 \bar{x}+\frac{h^{2}}{n}\right)^{2}-4 \bar{x}^{2}\right]^{1 / 2}}{2}=\bar{x}+\frac{h^{2}}{2 n} \pm \frac{h}{n^{1 / 2}}\left[\frac{h^{2}}{4 n}+\bar{x}\right]^{1 / 2}.\]

:::{.example name="Computer failures"}
For the computer failure data, $\bar{x}=3.75$ and $n=104$. For a $95 \%$ confidence interval (CI), $h=1.96$. Hence, we calculate the above CI using the R commands:

```{r}
n <- length(compfail)
h <- qnorm(0.975)
x_bar <- mean(compfail)
x_bar + h^2/(2 * n) + c(-1, 1) * h / sqrt(n) * sqrt(h^2 / (4 * n) + x_bar)
```
The result is (3.40, 4.14), which is quite close to the earlier interval (3.38, 4.12)
from Example \@ref(exm:poisson-ci-simple).
:::


###  Exact confidence interval for the normal mean


For normal models we do not have to rely on large sample approximations, because it turns out that the distribution of
\[T=\frac{\sqrt{n}(\bar{X}-\mu)}{S},\]
where $S^{2}$ is the sample variance with divisor $n-1$, is standard (easily calculated) and thus the statistic $T=T(\mathbf{X}, \mu)$ can be an exact pivot for any sample size $n>1$.

The point about easy calculation is that for any given $1-\alpha$, e.g. $1-\alpha=0.95$, we can calculate the critical value $h$ such that $P(-h<T<h)=1-\alpha$. Note also that the pivot $T$ does not involve the other unknown parameter of the normal model, namely the variance $\sigma^{2}$. If indeed, we can find $h$ for any given $1-\alpha$, then proceed as follows to find the exact CI for $\mu$ :
\begin{align*}
& P(-h \leq T \leq h)=1-\alpha \\
\text { i.e. } & P\left(-h \leq \sqrt{n} \frac{(\bar{X}-\mu)}{S} \leq h\right)=0.95 \\
\Rightarrow & P\left(\bar{X}-h \frac{S}{\sqrt{n}} \leq \mu \leq \bar{X}+h \frac{S}{\sqrt{n}}\right)=0.95
\end{align*}

The observed value of this interval, $\left(\bar{x} \pm h \frac{s}{\sqrt{n}}\right)$, is the $95 \%$ confidence interval for $\mu$. Remarkably, this also of the general form, 
\[\text { Estimate } \pm \text { Critical value } \times \text { Standard error },\]
where the critical value is $h$ and the standard error of the sample mean is $\frac{s}{\sqrt{n}}$.
Now, how do we find the critical value $h$ for a given $1-\alpha$ ? We need to introduce the $t$-distribution.

Let $X_{1}, \ldots, X_{n}$ be i.i.d $N\left(\mu, \sigma^{2}\right)$ random variables. Define $\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$ and
\[S^{2}=\frac{1}{n-1}\left(\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right).\]
Then, it can be shown (and will be in MATH2011) that
\[\sqrt{n} \frac{(\bar{X}-\mu)}{S} \sim t_{n-1},\]
where $t_{n-1}$ denotes the standard $t$ distribution with $n-1$ degrees of freedom. The standard $t$ distribution is a family of distributions which depend on one parameter called the degrees-of-freedom (df) which is $n-1$ here. The concept of degrees of freedom is that it is usually the number of independent random samples, $n$ here, minus the number of linear parameters estimated, 1 here for $\mu$. Hence the df is $n-1$.


The probability density function of the $t_{k}$ distribution is similar to a standard normal, in that it is symmetric around zero and 'bell-shaped', but the t-distribution is more heavy-tailed, giving greater probability to observations further away from zero. Figure \@ref(fig:t-pdf) illustrates the $t_{k}$ density function for various values of $k$.

```{r t-pdf, echo = FALSE, fig.cap = "The pdf of the $t_k$ distribution for various values of $k$. The dashed line is the standard normal pdf.", fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
plot_data <- crossing(x = seq(-10, 10, length.out = 1000),
                      k = c(1, 2, 5, 20)) %>%
    mutate(pdf = dt(x, df = k),
           k = as.factor(k))

plot_data %>%
    ggplot(aes(x = x, y = pdf, colour = k)) +
    geom_line() +
    geom_function(fun = dnorm, linetype = "dashed", colour = "black")
        
```


The values of $h$ for a given $1-\alpha$ can be obtained using the R command \texttt{qt} (abbreviation for quantile of $t$ ). For example, we can find $h$ for $1-\alpha=0.95$ and $n=20$ 
```{r}
qt(0.975, df = 19)
```
Note that it should be $0.975$ so that we are splitting $0.05$ probability between the two tails equally and the df should be $n-1=19$. Indeed, modifying the above command, we obtain the following critical values for the $95 \%$ interval for different values of the sample size $n$.

\begin{center}
\begin{tabular}{c|ccccccccc}
$n$ & 2 & 5 & 10 & 15 & 20 & 30 & 50 & 100 & 1000 \\
\hline$h$ & $12.71$ & $2.78$ & $2.26$ & $2.14$ & $2.09$ & $2.05$ & $2.01$ & $1.98$ & $1.96$
\end{tabular}
\end{center}

Note that the critical value approaches $1.96$ (which is the critical value for the normal distribution) as $n \rightarrow \infty$, since the $t$-distribution itself approaches the normal distribution for large values of its df parameter.

If you can justify that the underlying distribution is normal then you
can use the $t$-distribution-based confidence interval.


:::{.example name="Fast food waiting times"}
We would like to find a confidence interval for the true mean waiting time. If $X$ denotes the waiting time in seconds, we have $n=20, \bar{x}=67.85$, $s=18.36$. Hence, recalling that the critical value $h=2.093$, from the command \texttt{qt(0.975, df = 19)}, a $95 \%$ confidence interval for $\mu$ is
\begin{align*}
67.85-2.093 \times 18.36 / \sqrt{20} \leq \mu & \leq 67.85+2.093 \times 18.36 / \sqrt{20} \\
\Rightarrow 59.26 & \leq \mu \leq 76.44 .
\end{align*}

In R, if the vector `fastfood` contains all the service times,
we can find a $95 \%$ confidence interval with

```{r, echo = FALSE}
fastfood <- fastfood$service
```

```{r}
mean(fastfood) + c(-1, 1) * qt(0.975, df = 19) * sd(fastfood) / sqrt(20)
```

or a $90 \%$ confidence interval with

```{r}
mean(fastfood) + c(-1, 1) * qt(0.95, df = 19) * sd(fastfood) / sqrt(20)
```

or a $90 \%$ confidence interval with
```{r}
mean(fastfood) + c(-1, 1) * qt(0.995, df = 19) * sd(fastfood) / sqrt(20)
```

We can see clearly that the interval gets wider as the level of confidence is gets higher.
:::


:::{.example name="Weight gain" #wtgain-ci}
We would like to find a confidence interval for the true average weight gain (final weight - initial weight). Here $n=68, \bar{x}=0.8672$ and $s=0.9653$. Hence, a $95 \%$ confidence interval for $\mu$ is
\begin{align*}
0.8672-1.996 \times 0.9653 / \sqrt{68} \leq \mu & \leq 0.8672+1.996 \times 0.9653 / \sqrt{68} \\
\Rightarrow 0.6335 & \leq \mu \leq 1.1008
\end{align*}

In R, we obtain the critical value $1.996$ by
```{r}
qt(0.975, df = 67)
```

Note that the interval here does not include the value 0, so it is very likely that the weight gain is significantly positive, which we will justify using what is called hypothesis testing.
:::

## Hypothesis testing

### Hypothesis testing in general

#### Introduction

The manager of a new fast food chain claims that the average waiting time to be served in their restaurant is less than a minute. The marketing department of a mobile phone company claims that their phones never break down in the first three years of their lifetime. A professor of nutrition claims that students gain significant weight in the first year of their life in college away form home. How can we verify these claims? We will learn the procedures of hypothesis testing for such problems.


In statistical inference, we use observations $x_{1}, \ldots, x_{n}$ of univariate random variables $X_{1}, \ldots, X_{n}$ in order to draw inferences about the probability distribution $f(x)$ of the underlying random variable $X$. So far, we have mainly been concerned with estimating features (usually unknown parameters) of $f(x)$. It is often of interest to compare alternative specifications for $f(x)$. If we have a set of competing probability models which might have generated the observed data, we may want to determine which of the models is most appropriate. A proposed (hypothesised) model for $X_{1}, \ldots, X_{n}$ is then referred to as a hypothesis, and pairs of models are compared using hypothesis tests.

For example, we may have two competing alternatives, $f^{(0)}(x)$ (model $H_{0}$) and $f^{(1)}(x)$ (model $H_{1}$) for $f(x)$, both of which completely specify the joint distribution of the sample $X_{1}, \ldots, X_{n}$. Completely specified statistical models are called simple hypotheses. Usually, $H_{0}$ and $H_{1}$ both take the same parametric form $f(x, \theta)$, but with different values $\theta^{(0)}$ and $\theta^{(1)}$ of $\theta$. Thus the joint distribution of the sample given by $f(\mathbf{X})$ is completely specified apart from the values of the unknown parameter $\theta$ and $\theta^{(0)} \neq \theta^{(1)}$ are specified alternative values.

More generally, competing hypotheses often do not completely specify the joint distribution of $X_{1}, \ldots, X_{n}$. For example, a hypothesis may state that $X_{1}, \ldots, X_{n}$ is a random sample from the probability distribution $f(x ; \theta)$ where $\theta<0$. This is not a completely specified hypothesis, since it is not possible to calculate probabilities such as $P\left(X_{1}<2\right)$ when the hypothesis is true, as we do not know the exact value of $\theta$. Such an hypothesis is called a composite hypothesis.

Examples of hypotheses:

- $X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)$ with $\mu=0, \sigma^{2}=2$.
- $X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)$ with $\mu=0, \sigma^{2} \in \mathcal{R}_{+}$.
- $X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)$ with $\mu \neq 0, \sigma^{2} \in \mathcal{R}_{+}$.
- $X_{1}, \ldots, X_{n} \sim \operatorname{Bernoulli}(p)$ with $p=\frac{1}{2}$.
- $X_{1}, \ldots, X_{n} \sim \operatorname{Bernoulli}(p)$ with $p \neq \frac{1}{2}$.
- $X_{1}, \ldots, X_{n} \sim \operatorname{Bernoulli}(p)$ with $p>\frac{1}{2}$.
- $X_{1}, \ldots, X_{n} \sim \operatorname{Poisson}(\lambda)$ with $\lambda=1$.
- $X_{1}, \ldots, X_{n} \sim \operatorname{Poisson}(\theta)$ with $\theta>1$

#### Hypothesis testing procedure

A hypothesis test provides a mechanism for comparing two competing statistical models, $H_{0}$ and $H_{1}$. A hypothesis test does not treat the two hypotheses (models) symmetrically. One hypothesis, $H_{0}$, is given special status, and referred to as the null hypothesis. The null hypothesis is the reference model, and is assumed to be appropriate unless the observed data strongly indicate that $H_{0}$ is inappropriate, and that $H_{1}$ (the alternative hypothesis) should be preferred. Hence, the fact that a hypothesis test does not reject $H_{0}$ should not be taken as evidence that $H_{0}$ is true and $H_{1}$ is not, or that $H_{0}$ is better-supported by the data than $H_{1}$, merely that the data does not provide significant evidence to reject $H_{0}$ in favour of $H_{1}$.

A hypothesis test is defined by its critical region or rejection region, which we shall denote by $C$. $C$ is a subset of $\mathcal{R}^{n}$ and is the set of possible observed values of $\mathbf{X}$ which, if observed, would lead to rejection of $H_{0}$ in favour of $H_{1}$, i.e.

$$
\begin{array}{ll}
\text { If } \mathbf{x} \in C & H_{0} \text { is rejected in favour of } H_{1} \\
\text { If } \mathbf{x} \notin C & H_{0} \text { is not rejected }
\end{array}
$$

As $\mathbf{X}$ is a random variable, there remains the possibility that a hypothesis test will give an erroneous result. We define two types of error:

\begin{center}
\begin{tabular}{|l|}
\hline Type I error: $H_{0}$ is rejected when it is true \\
Type II error: $H_{0}$ is not rejected when it is false \\
\hline
\end{tabular}
\end{center}

The following table helps to understand further:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline & $H_{0}$ true & $H_{0}$ false \\
\hline Reject $H_{0}$ & Type I error & Correct decision \\
\hline Do not reject $H_{0}$ & Correct decision & Type II error \\
\hline
\end{tabular}
\end{center}

When $H_{0}$ and $H_{1}$ are simple hypotheses, we can define
\begin{align*}
& \alpha=P(\text {Type I error})=P(\mathbf{X} \in C) \quad \text { if $H_{0}$ is true } \\
& \beta=P(\text {Type II error})=P(\mathbf{X} \notin C) \quad \text { if $H_{1}$ is true }
\end{align*}

:::{.example name="Uniform"}
Suppose that we have one observation from the uniform distribution on the range $(0, \theta)$. In this case, $f(x)=1 / \theta$ if $0<x<\theta$ and $P(X \leq x)=\frac{x}{\theta}$ for $0<x<\theta$. We want to test $H_{0}: \theta=1$ against the alternative $H_{1}: \theta=2$. Suppose we decide arbitrarily that we will reject $H_{0}$ if $X>0.75$. Then
\begin{align*}
& \alpha=P(\text {Type I error}=P(X>0.75) \text { if } H_{0} \text { is true } \\
& \beta=P(\text {Type II error})=P(X<0.75) \text { if } H_{1} \text { is true }
\end{align*}
which will imply:
\[\begin{array}{r}
\alpha=P(X>0.75 \mid \theta=1)=1-0.75 0.25, \\
\beta=P(X<0.75 \mid \theta=2)=0.75 / 2= 0.375.
\end{array}\]
:::

:::{.example name="Poisson"}
The daily demand for a product has a Poisson distribution with mean $\lambda$, the demands on different days being statistically independent. It is desired to test the hypotheses $H_{0}: \lambda=0.7, H_{1}: \lambda=0.3$. The null hypothesis is to be accepted if in 20 days the number of days with no demand is less than 15. Calculate the Type I and Type II error probabilities.

Let $p$ denote the probability that the demand on a given day is zero. Then
\[p=e^{-\lambda}= \begin{cases}e^{-0.7} & \text { under } H_{0} \\ e^{-0.3} & \text { under } H_{1}\end{cases}\]

If $X$ denotes the number of days out of 20 with zero demand, it follows that
\begin{align*}
&X \sim \operatorname{Binomial}\left(20, e^{-0.7}\right) \text { under } H_{0}, \\
&X \sim \operatorname{Binomial}\left(20, e^{-0.3}\right) \text { under } H_{1}.
\end{align*}
Thus
\begin{align*}
\alpha &=P\left(\text { Reject } H_{0} \mid H_{0} \text { true }\right) \\
&=P\left(X \geq 15 \mid X \sim \operatorname{Binomial}\left(20, e^{-0.7}\right)\right) \\
&=1-P(X \leq 14 \mid X \sim \operatorname{Binomial}(20,0.4966)) \\
&=1-0.98028 \\
&=0.01923 \, (\texttt{1 - pbinom(14, size = 20, prob = 0.4966)})
\end{align*}

Furthermore
\begin{align*}
\beta &=P\left(\text { Accept } H_{0} \mid H_{1} \text { true }\right) \\
&=P\left(X \leq 14 \mid X \sim \operatorname{Binomial}\left(20, e^{-0.3}\right)\right) \\
&=P(X \leq 14 \mid X \sim \operatorname{Binomial}(20,0.7408)) \\
&=0.42023  \, (\texttt{1 - pbinom(14, size = 20, prob = 0.7408)})
\end{align*}
:::

Sometimes $\alpha$ is called the size (or significance level) of the test and $\omega \equiv 1-\beta$ is called the power of the test. Ideally, we would like to avoid error so we would like to make both $\alpha$ and $\beta$ as small as possible. In other words, a good test will have small size, but large power. However, it is not possible to make $\alpha$ and $\beta$ both arbitrarily small. For example if $C=\emptyset$ then $\alpha=0$, but $\beta=1$. On the other hand if $C=\mathbf{S}=\mathcal{R}^{n}$ then $\beta=0$, but $\alpha=1$.

The general hypothesis testing procedure is to fix $\alpha$ to be some small value (often 0.05), so that the probability of a Type I error is limited. In doing this, we are giving $H_{0}$ precedence over $H_{1}$, and acknowledging that Type I error is potentially more serious than Type II error. (Note that for discrete random variables, it may be difficult to find $C$ so that the test has exactly the required size). Given our specified $\alpha$, we try to choose a test, defined by its rejection region $C$, to make $\beta$ as small as possible, i.e. we try to find the most powerful test of a specified size. Where $H_{0}$ and $H_{1}$ are simple hypotheses this can be achieved easily.

Note that tests are usually based on a one-dimensional test statistic $T(\mathbf{X})$ whose sample space is some subset of $\mathcal{R}$. The rejection region is then a set of possible values for $T(\mathbf{X})$, so we also think of $C$ as a subset of $\mathcal{R}$. In order to be able to ensure the test has size $\alpha$, the distribution of the test statistic under $H_{0}$ should be known.

#### The test statistic

We perform a hypothesis test by computing a test statistic, $T(\boldsymbol{X})$. A test statistic must (obviously) be a statistic (i.e. a function of $\boldsymbol{X}$ and other known quantities only). Furthermore, the random variable $T(\boldsymbol{X})$ must have a distribution which is known under the null hypothesis. The easiest way to construct a test statistic is to obtain a pivot for $\theta$. If $T(\boldsymbol{X}, \theta)$ is a pivot for $\theta$ then its sampling distribution is known and, therefore, under the null hypothesis $\left(\theta=\theta_{0}\right)$ the sampling distribution of $T\left(\boldsymbol{X}, \theta_{0}\right)$ is known. Hence $T\left(\boldsymbol{x}, \theta_{0}\right)$ is a test statistic, as it depends on observed data $\boldsymbol{x}$ and the hypothesised value $\theta_{0}$ only. We then assess the plausibility of $H_{0}$ by evaluating whether $T\left(\boldsymbol{x}, \theta_{0}\right)$ seems like a reasonable observation from its (known) distribution. This is all rather abstract. How does it work in a concrete example?

### Testing a normal mean (t-test)

#### Introduction

Suppose that we observe data $x_{1}, \ldots, x_{n}$ which are modelled as observations of i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables $X_{1}, \ldots, X_{n}$, and we want to test the null hypothesis
\[H_{0}: \mu=\mu_{0}\]
against the alternative hypothesis
\[H_{1}: \mu \neq \mu_{0}.\]
We recall that
\[\sqrt{n} \frac{(\bar{X}-\mu)}{S} \sim t_{n-1}\]
and therefore, when $H_{0}$ is true, often written as under $H_{0}$,
\[\sqrt{n} \frac{\left(\bar{X}-\mu_{0}\right)}{S} \sim t_{n-1}\]
so $\sqrt{n}\left(\bar{X}-\mu_{0}\right) / s$ is a test statistic for this test. The sampling distribution of the test statistic when the null hypothesis is true is called the null distribution of the test statistic. In this example, the null distribution is the t-distribution with $n-1$ degrees of freedom.

This test is called a $t$-test. We reject the null hypothesis $H_{0}$ in favour of the alternative $H_{1}$ if the observed test statistic seems unlikely to have been generated by the null distribution.

::: {.example name="Weight gain"}
For the weight gain data, if $x$ denotes the differences in weight gain, we have $\bar{x}=0.8672$, $s=0.9653$ and $n=68$. Hence our test statistic for the null hypothesis $H_{0}: \mu=\mu_{0}=0$ is
\[\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}=7.41.\]

The observed value of $7.41$ does not seem reasonable from Figure \@ref(fig:t-pdf-wtgain), which shows the density of the $t$-distribution with 67 degrees of freedom, and a vertical line at the observed value of $7.41$. So there may be evidence here to reject $H_{0}: \mu=0$.

```{r t-pdf-wtgain, echo = FALSE, fig.cap = "The density of the $t_{67}$ distribution. The dashed line shows the observed value of the test statistic for the weight gain data.", fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
plot_data <- tibble(x = seq(-10, 10, length.out = 1000)) %>%
    mutate(pdf = dt(x, df = 67))

plot_data %>%
    ggplot(aes(x = x, y = pdf)) +
    geom_line() +
    geom_vline(xintercept = 7.41, linetype = "dashed")
        
```
:::

::: {.example name="Fast food waiting times"}
Suppose the manager of the fast food outlet claims that the average waiting time is only 60 seconds. So, we want to test $H_{0}: \mu=60$. We have $n=20, \bar{x}=67.85, s=18.36$. Hence our test statistic for the null hypothesis $H_{0}: \mu=\mu_{0}=60$ is
\[\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}=\sqrt{20} \frac{(67.85-60)}{18.36}=1.91.\]

The observed value of $1.91$ may or may not be reasonable from Figure \@ref(fig:t-pdf-fastfood), which shows the density of the $t$-distribution with 19 degrees of freedom, and a vertical line at the observed value of $1.91$. This value is a bit out in the tail but we are not sure, unlike in the previous weight gain example. So how can we decide whether to reject the null hypothesis?

```{r t-pdf-fastfood, echo = FALSE, fig.cap = "The density of the $t_{19}$ distribution. The dashed line shows the observed value of the test statistic for the fast food data.", fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
plot_data <- tibble(x = seq(-7, 7, length.out = 1000)) %>%
    mutate(pdf = dt(x, df = 19))

plot_data %>%
    ggplot(aes(x = x, y = pdf)) +
    geom_line() +
    geom_vline(xintercept = 1.91, linetype = "dashed")
        
```
:::


#### The significance level

In the weight gain example, it seems that there is enough evidence to reject $H_{0}$, but how extreme (far from the mean of the null distribution) should the test statistic be in order for $H_{0}$ to be rejected? The significance level of the test, $\alpha$, is the probability that we will erroneously reject $H_{0}$ (called Type I error as discussed before). Clearly we would like $\alpha$ to be small, but making it too small risks failing to reject $H_{0}$ even when it provides a poor model for the observed data (Type II error). Conventionally, $\alpha$ is usually set to a value of $0.05$, or $5 \%$. Therefore we reject $H_{0}$ when the test statistic lies in a rejection region which has probability $\alpha=0.05$ under the null distribution.

#### Rejection region

For the t-test, the null distribution is $t_{n-1}$ where $n$ is the sample size, so the rejection region for the test corresponds to a region of total probability $\alpha=0.05$ comprising the 'most extreme' values in the direction of the alternative hypothesis. If the alternative hypothesis is two-sided, e.g. $H_{1}: \mu \neq \mu_{0}$, then this is obtained as below, where the two shaded regions both have area (probability) $\alpha / 2=0.025$.

```{r t-rejection, echo = FALSE, fig.cap = "The shaded area shows the rejection region for a t-test. $h$ is chosen to make the shaded area equal to the significance level $1-\\alpha$", fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
library(ggplot2)

lim <- 5
h <- qt(0.975, df = 9)
plot_data_low <- tibble(x = c(seq(-lim, -h, length.out = 100)),
                        ymax = dt(x, df = 9),
                        ymin = 0)
plot_data_high <- tibble(x = c(seq(h, lim, length.out = 100)),
                        ymax = dt(x, df = 9),
                        ymin = 0)

plot_data_low %>%
    ggplot(mapping = aes(x = x, ymin = ymin, ymax = ymax)) + 
    geom_function(fun = dt, args = list(df = 9)) +
    geom_ribbon(alpha = 0.3) +
    geom_ribbon(data = plot_data_high, alpha = 0.3) +
    xlab("x") +
    ylab("f(x)") +
    scale_x_continuous(breaks = c(0, -h, h),
                       labels = c(0, "-h", "h"),
                       limits = c(-lim, lim)) +
    geom_hline(aes(yintercept = 0)) +
    theme(panel.grid.minor = element_blank(),
          panel.grid = element_blank())
```

The value of $h$ depends on the sample size $n$ and can be found in R with the \texttt{qt} command. 
Note that we need to put $n-1$ in the `df` argument of `qt`
So for $n = 100$, we can find $h$ with
```{r}
qt(0.975, df = 99)
```
For some other values of $n$, we have
\begin{tabular}{cccccccccc}
$n$ & 2 & 5 & 10 & 15 & 20 & 30 & 50 & 100 & $\infty$ \\
\hline$h$ & $12.71$ & $2.78$ & $2.26$ & $2.14$ & $2.09$ & $2.05$ & $2.01$ & $1.98$ & $1.96$
\end{tabular}
where the last value for $n=\infty$ is obtained from the normal distribution.

However, if the alternative hypothesis is one-sided, e.g. $H_{1}: \mu>\mu_{0}$, then the critical region will only be in the right tail. Consequently, we need to leave an area $\alpha$ on the right and as a result the critical values will be from a command such as
```{r, eval = FALSE}
qt(0.95, df = 99)
```
for $n = 100$. For some other values of $n$, we have

\begin{tabular}{cccccccccc}
$n$ & 2 & 5 & 10 & 15 & 20 & 30 & 50 & 100 & $\infty$ \\
\hline$h$ & $6.31$ & $2.13$ & $1.83$ & $1.76$ & $1.73$ & $1.70$ & $1.68$ & $1.66$ & $1.64$
\end{tabular}

#### Summary of the t-test procedure

Suppose that we observe data $x_{1}, \ldots, x_{n}$ which are modelled as observations of i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables $X_{1}, \ldots, X_{n}$ and we want to test the null hypothesis $H_{0}: \mu=\mu_{0}$ against the alternative hypothesis $H_{1}: \mu \neq \mu_{0}$ :

1. Compute the test statistic
\[t=\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}.\]

2. For chosen significance level $\alpha$ (usually 0.05) calculate the rejection region for $t$, which is of the form $|t|>h$ where $-h$ is the $\alpha / 2$ percentile of the null distribution, $t_{n-1}$.

3. If your computed $t$ lies in the rejection region, i.e. $|t|>h$, you report that $H_{0}$ is rejected in favour of $H_{1}$ at the chosen level of significance. If $t$ does not lie in the rejection region, you report that $H_{0}$ is not rejected. (Never refer to 'accepting' a hypothesis.)

#### Examples

::: {.example name="Fast food waiting times"}
We would like to test $H_{0}: \mu=60$ against the alternative $H_{1}: \mu>60$, as this alternative will refute the claim of the store manager that customers only wait for a maximum of one minute. We calculated the observed value to be $1.91$. This is a one-sided test and for a $5 \%$ level of significance, the critical value $h$ will come from $\texttt{qt(0.95, df = 19)}=1.73$. Thus the observed value is higher than the critical value so we will reject the null hypothesis, disputing the manager's claim regarding a minute wait.
:::

::: {.example name="Weight gain"}
For the weight gain example $\bar{x}=0.8671, s=0.9653, n=68$. Then, we would be interested in testing $H_{0}: \mu=0$ against the alternative hypothesis $H_{1}: \mu \neq 0$ in the model that the data are observations of i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables.

- We obtain the test statistic
\[t=\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}=\sqrt{68} \frac{(0.8671-0)}{0.9653}=7.41.\]
- Under $H_{0}$ this is an observation from a $t_{67}$ distribution. For significance level $\alpha=0.05$ the rejection region is $|t|>1.996$.
- Our computed test statistic lies in the rejection region, i.e. $|t|>1.996$, so $H_{0}$ is rejected in favour of $H_{1}$ at the $5 \%$ level of significance.

In R we can perform the test as follows:
```{r}
wtgain$diff <- wtgain$final - wtgain$initial
t.test(wtgain$diff)
```
This gives the results $\mathrm{t}=7.4074$, and $\mathrm{df}=67$. 


#### p-values

The result of a test is most commonly summarised by rejection or non-rejection of $H_{0}$ at the stated level of significance. An alternative, which you may see in practice, is the computation of a p-value. This is the probability that the reference distribution would have generated the actual observed value of the statistic or something more extreme. A small p-value is evidence against the null hypothesis, as it indicates that the observed data were unlikely to have been generated by the reference distribution. In many examples a threshold of $0.05$ is used, below which the null hypothesis is rejected as being insufficiently well-supported by the observed data. 

For the t-test with a two-sided alternative, the p-value is given by:\[p=P\left(|T|>\left|t_{\text{obs}}\right|\right)=2 P\left(T>\left|t_{\text{obs}}\right|\right),\]
where $T$ has a $t_{n-1}$ distribution and $t_{\text{obs }}$ is the observed sample value.

However, if the alternative is one-sided and to the right then the p-value is given by:
\[p=P\left(T>t_{\text{obs}}\right),\]
where $T$ has a $t_{n-1}$ distribution and $t_{\text{obs}}$ is the observed sample value.

A small p-value corresponds to an observation of $T$ that is improbable (since it is far out in the low probability tail area) under $H_{0}$ and hence provides evidence against $H_{0}$. The p-value should not be misinterpreted as the probability that $H_{0}$ is true. $H_{0}$ is not a random event (under our models) and so cannot be assigned a probability. The null hypothesis is rejected at significance level $\alpha$ if the p-value for the test is less than $\alpha$.
\[\text { Reject } H_{0} \text { if p-value }<\alpha.\]

<!-- TODO: check for consistent naming of fast food / weight gain examples -->
::: {.example name="Fast food waiting times"}
In the fast food example, a test of $H_{0}: \mu=60$ resulted in a test statistic $t=1.91$. Then the p-value is given by
\[p=P(T>1.91)=0.036 \text {, when } T \sim t_{19}.\]

This is the area of the shaded region in Figure \@ref(fig:p-value-fastfood).
In R it is
```{r}
1 - pt(1.91, df = 19) 
```
The p-value $0.036$ indicates some evidence against the manager's claim at the $5 \%$ level of significance but not the $1 \%$ level of significance.

```{r p-value-fastfood, echo = FALSE, fig.cap = "The area of the shaded region (under the $t_{19}$ pdf) is the p-value for the one-sided hypothesis test in the fast food example" , fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
library(ggplot2)

lim <- 5
t <- 1.91
plot_data_low <- tibble(x = c(seq(-lim, -t, length.out = 100)),
                        ymax = dt(x, df = 19),
                        ymin = 0)
plot_data_high <- tibble(x = c(seq(t, lim, length.out = 100)),
                        ymax = dt(x, df = 19),
                        ymin = 0)

plot_data_high %>%
    ggplot(mapping = aes(x = x, ymin = ymin, ymax = ymax)) + 
    geom_function(fun = dt, args = list(df = 19)) +
    geom_ribbon(alpha = 0.3) +
    xlab("x") +
    ylab("f(x)") +
    scale_x_continuous(breaks = c(0, 1.91),
                       labels = c(0, "t = 1.91"),
                       limits = c(-lim, lim)) +
    geom_hline(aes(yintercept = 0)) +
    theme(panel.grid.minor = element_blank(),
          panel.grid = element_blank())
```
:::

When the alternative hypothesis is two-sided the p-value has to be calculated from $P\left(|T|>t_{\text {obs }}\right)$, where $t_{\text {obs }}$ is the observed value and $T$ follows the $t$-distribution with $n-1$ df.

::: {.example name="Weight gain"}
Because the alternative is two-sided, the p-value is given by:
\[p=P(|T|>7.41)=2.78 \times 10^{-10} \approx 0.0, \text { when } T \sim t_{67}.\]
This very small p-value indicates very strong evidence against the null hypothesis of no weight gain in the first year of college.
:::

#### Equivalence of testing and interval estimation

Note that the $95 \%$ confidence interval for $\mu$ in the weight gain example has previously been calculated to be $(0.6335,1.1008)$ in
Example \@ref(exm:wtgain-ci).
This interval does not include the hypothesised value 0 of $\mu$. Hence we can conclude that the hypothesis test at the $5 \%$ level of significance will reject the null hypothesis $H_{0}: \mu=0$. 

This is because \[\left|T_\text{obs}=\frac{\sqrt{n}\left(\bar{x}-\mu_{0}\right)}{s}\right|>h\] implies and is implied by $\mu_{0}$ being outside the interval \[(\bar{x}-h s / \sqrt{n}, \bar{x}+h s / \sqrt{n}).\] Notice that $h$ is the same in both. For this reason we often just calculate the confidence interval and take the reject/do not reject decision merely by inspection.

### Two sample t-tests


Suppose that we observe two samples of data, $x_{1}, \ldots, x_{n}$ and $y_{1}, \ldots, y_{m}$, and that we propose to model them as observations of
\[X_{1}, \ldots, X_{n} \stackrel{i.i.d.}{\sim} N\left(\mu_{X}, \sigma_{X}^{2}\right)\]
and
\[Y_{1}, \ldots, Y_{m} \stackrel{\text { i.i.d. }}{\sim} N\left(\mu_{Y}, \sigma_{Y}^{2}\right)\]
respectively, where it is also assumed that the $X$ and $Y$ variables are independent of each other. Suppose that we want to test the hypothesis that the distributions of $X$ and $Y$ are identical, that is
\[H_{0}: \mu_{X}=\mu_{Y}, \quad \sigma_{X}=\sigma_{Y}=\sigma\]
against the alternative hypothesis
\[H_{1}: \mu_{X} \neq \mu_{Y}.\]

<!-- TODO: change to remove $\sigma_X$, $\sigma_Y$ from setup? -->

<!-- TODO more specific ref? -->
In the Chapter 3 we proved that
\[\bar{X} \sim N\left(\mu_{X}, \sigma_{X}^{2} / n\right) \text { and } \bar{Y} \sim N\left(\mu_{Y}, \sigma_{Y}^{2} / m\right)\]
and therefore
\[\bar{X}-\bar{Y} \sim N\left(\mu_{X}-\mu_{Y}, \frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}\right).\]

Hence, under $H_{0}$,
\[\bar{X}-\bar{Y} \sim N\left(0, \sigma^{2}\left[\frac{1}{n}+\frac{1}{m}\right]\right) \Rightarrow \sqrt{\frac{n m}{n+m}} \frac{(\bar{X}-\bar{Y})}{\sigma} \sim N(0,1).\]

The involvement of the (unknown) $\sigma$ above means that this is not a pivotal test statistic. It will be proved in MATH2011 that if $\sigma$ is replaced by its unbiased estimator $S$, which here is the two-sample estimator of the common standard deviation, given by
\[S^{2}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{i=1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}}{n+m-2},\]
then
\[\sqrt{\frac{n m}{n+m}} \frac{(\bar{X}-\bar{Y})}{S} \sim t_{n+m-2}.\]
Hence
\[t=\sqrt{\frac{n m}{n+m}} \frac{(\bar{x}-\bar{y})}{s}\]
is a test statistic for this test. The rejection region is $|t|>h$ where $-h$ is the $\alpha / 2$ (usually $0.025$ ) percentile of $t_{n+m-2}$.

From the hypothesis testing, a $100(1-\alpha) \%$ confidence interval
for $\mu_{X}-\mu_{Y}$
is given by
\[\bar{x}-\bar{y} \pm h \sqrt{\frac{n+m}{n m}} s,\]
where $-h$ is the $\alpha / 2$ (usually $0.025$ ) percentile of $t_{n+m-2}$.

:::{ .example name="Fast food waiting times"}
In this example, we would like to know if there are significant differences between the AM and PM waiting times. Here $n=m=10$, $\bar{x}=68.9$, $\bar{y}=66.8$, $s_{x}^{2}=538.22$ and $s_{y}^{2}=171.29$. From this we calculate,
\[s^{2}=\frac{(n-1) s_{x}^{2}+(m-1) s_{y}^{2}}{n+m-2}=354.8,\]
and
\[t_{\text{obs}}=\sqrt{\frac{n m}{n+m}} \frac{(\bar{x}-\bar{y})}{s}=0.25.\]
This is not significant as the critical value $h = \texttt{qt(0.975, df = 18)}=2.10$ is larger in absolute value than $0.25$.
We can conduct the test easily in R:
```{r}
fastfood_am <- c(38, 100, 64, 43, 63, 59, 107, 52, 86, 77)
fastfood_pm <- c(45, 62, 52, 72, 81, 88, 64, 75, 59, 70)
t.test(fastfood_am, fastfood_pm)
```
It automatically calculates the test statistic as $0.249$ and a p-value of $0.8067$. It also obtains the $95 \%$ CI given by $(-15.94,20.14)$.
:::

### Paired t-test

Sometimes the assumption that the $X$ and $Y$ variables are independent of each other is unlikely to be valid, due to the design of the study. The most common example of this is where $n=m$ and data are paired. For example, a measurement has been made on patients before treatment $(X)$ and then again on the same set of patients after treatment $(Y)$. Recall the weight gain example is exactly of this type. In such examples, we proceed by computing data on the differences
\[z_{i}=x_{i}-y_{i}, \quad i=1, \ldots, n\]
and modelling these differences as observations of i.i.d. $N\left(\mu_{z}, \sigma_{Z}^{2}\right)$ variables $Z_{1}, \ldots, Z_{n}$. Then, a test of the hypothesis $\mu_{X}=\mu_{Y}$ is achieved by testing $\mu_{Z}=0$, which is just a standard (one sample) t-test, as described previously.

::: {.example}
Water-quality researchers wish to measure the biomass to chlorophyll ratio for phytoplankton (in milligrams per litre of water). There are two possible tests, one less expensive than the other. To see whether the two tests give the same results, ten water samples were taken and each was measured both ways. The results are as follows:

\begin{tabular}{l|llllllllll} 
Test 1 $(x)$ & $45.9$ & $57.6$ & $54.9$ & $38.7$ & $35.7$ & $39.2$ & $45.9$ & $43.2$ & $45.4$ & $54.8$ \\
Test 2 (y) & $48.2$ & $64.2$ & $56.8$ & $47.2$ & $43.7$ & $45.7$ & $53.0$ & $52.0$ & $45.1$ & $57.5$
\end{tabular}
:::

To test the null-hypothesis
\[H_{0}: \mu_{Z}=0 \text { against } H_{1}: \mu_{Z} \neq 0\]
we use the test statistic $t=\sqrt{n} \frac{\bar{z}}{s_{z}}$, where $s_{z}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(z_{i}-\bar{z}\right)^{2}$.

From the hypothesis testing, a $100(1-\alpha) \%$ confidence interval
for $\mu_Z$ is given by $\bar{z} \pm h \frac{s_{z}}{\sqrt{n}}$, where $h$ is the critical value of the $t$ distribution with $n-1$ degrees of freedom. In R we perform the test as follows:

```{r}
x <- c(45.9, 57.6, 54.9, 38.7, 35.7, 39.2, 45.9, 43.2, 45.4, 54.8)
y <- c(48.2, 64.2, 56.8, 47.2, 43.7, 45.7, 53.0, 52.0, 45.1, 57.5)

t.test(x, y, paired = TRUE)
```

This gives the test statistic $t_{\text{obs}}=-5.0778$ with a df of 9 and a p-value $=0.0006649$. Thus we reject the null hypothesis. The associated $95 \%$ CI is $(-7.53,-2.89)$.

The values of the second test are significantly higher than the ones of the first test, and so the second test cannot be considered as a replacement for the first.

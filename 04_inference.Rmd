# Statistical Inference {#inference}

## Statistical modelling

### Introduction 

Statistical analysis (or inference) involves drawing conclusions, and making predictions and decisions, using the evidence provided to us by observed data. To do this we use probability distributions, often called statistical models, to describe the process by which the observed data were
generated. For example, we may suppose that the true proportion of Indian origin students is $p$,
$0 < p < 1$, and if we have selected $n$ students at random, 
that each of those students gives rise to a
Bernoulli distribution which takes the value $1$ if the student is of Indian origin and $0$
otherwise. The
success probability of the Bernoulli distribution will be the unknown $p$. The underlying statistical
model is then the Bernoulli distribution.
<!-- TODO: change example here? -->

To illustrate with another example, suppose we have observed fast food waiting times in the
morning and afternoon. If we assume time (number of whole seconds) to be discrete, then a
suitable model for the random variable $X =$ "the number of seconds waited" would be the Poisson
distribution. However, if we treat time as continuous then the random variable $X =$ "the waiting
time" could be modelled as a normal random variable. 
<!-- TODO: add reference to exponential distribution? -->

In general:

- The form of the assumed model helps us to understand the real-world process by which the
data were generated.
- If the model explains the observed data well, then it should also inform us about future
(or unobserved) data, and hence help us to make predictions (and decisions contingent on
unobserved data).
- The use of statistical models, together with a carefully constructed methodology for their
analysis, also allows us to quantify the uncertainty associated with any conclusions, predictions or decisions we make.

We will use the notation $x_1 , x_2 , \ldots , x_n$ to denote $n$ observations
of the random variables $X_1 , X_2 , \ldots , X_n$ (corresponding capital letters). 
For the fast food waiting
time example, we have $n = 20$, $x_1 = 38, x_2 = 100, \ldots, x_{20} = 70$, and 
$X_i$ is the waiting time for the $i$th person in the sample.
 
<!-- TODO refer back to data -->
 
### Statistical models

Suppose we denote the complete data by the vector $\boldsymbol{x} = (x_1 , x_2 , \ldots , x_n)$ and use 
$\boldsymbol{X} = (X_1 , X_2 , \ldots , X_n)$
for the corresponding random variables. A statistical model specifies a probability distribution for
the random variables $\boldsymbol{X}$ corresponding to the data observations $\boldsymbol{x}$. Providing a specification for
the distribution of $n$ jointly varying random variables can be a daunting task, particularly if $n$ is
large. However, this task is made much easier if we can make some simplifying assumptions, such
as

1. $X_1 , X_2 , \ldots , X_n$ are independent random variables,
2. $X_1 , X_2 , \ldots , X_n$ have the same probability distribution 
(so $x_1 , x_2 , \ldots , x_n$ are observations of a single random variable $X$).

Assumption 1 depends on the sampling mechanism and is very common in practice. If we are
to make this assumption for the Southampton student sampling experiment, we need to select
randomly among all possible students. We should not get the sample from an event in the Indian
or Chinese Student Association as that will give us a biased result. The assumption will be violated when samples are correlated either in time or in space, e.g. the daily air pollution level in
Southampton for the last year or the air pollution levels in two nearby locations in Southampton.
In this module we will only consider data sets where Assumption 1 is reasonable.
<!-- TODO: update to new example -->

Assumption 2 is not
always appropriate, but is often reasonable when we are modelling a single variable. In the fast
food waiting time example, we must assume that there are no differences between the AM and PM
waiting times for Assumption 2 to hold.


If Assumption 1 and 2 both hold, we say that $X_1 , \ldots , X_{n}$ are 
independent and identically distributed (or i.i.d. for short).

### A fully specified model

Sometimes a model completely specifies the probability distribution of $X_1 , X_2 , \ldots , X_n$. 
For example, if we assume that the waiting time $X \sim N (\mu, \sigma^2)$ where $\mu = 100$, and 
$\sigma^2 = 100$, then this
is a fully specified model. In this case, there is no need to collect any data as there is no need
to make any inference about any unknown quantities, although we may use the data to judge the
plausibility of the model.
A fully specified model might be appropriate when there is some external
(to the data) theory as to why the model (in particular the values of $\mu$ and $\sigma^2$)
was appropriate.
Fully specified models such as this are uncommon as we rarely have external theory which allows
us to specify a model so precisely.

### A parametric statistical model

A parametric statistical model specifies a probability distribution for a random sample apart from
the value of a number of parameters in that distribution. This could be confusing in the first
instance -- a parametric model does not specify parameters! Here the word parametric signifies the
fact that the probability distribution is completely specified by a few parameters in the first place.
For example, the Poisson distribution is parameterised by the parameter $\lambda$ which happens 
to be the
mean of the distribution; the normal distribution is parameterised by two parameters, the mean $\mu$
and the variance $\sigma^2$.
When a parametric statistical model is assumed with some unknown parameters, statistical
inference methods use data to estimate the unknown parameters, e.g. $\lambda$, $\mu$, $\sigma^2$. 
Estimation will be
discussed in more detail in the following sections.


### A nonparametric statistical model

Sometimes it is not appropriate, or we want to avoid, making a precise specification for the 
distribution which generated $X_1 , X_2 , \ldots , X_n$. For example, when the data histogram 
does not show
a bell-shaped distribution, it would be wrong to assume a normal distribution for the data. In
such a case, although we can attempt to use some other non-bell-shaped parametric model, we can
decide altogether to abandon parametric models. We may then still assume that 
$X_1 , X_2 , \ldots , X_n$
are i.i.d. random variables, but from a nonparametric statistical model which cannot be written
down, having a probability function which only depends on a finite number of parameters. Such
analysis approaches are also called distribution-free methods.

<!-- TODO: add reference to previous example -->
:::{.example name="Return to the computer failure example"}
Let X denote the count of computer failures per week. We want to estimate how often will the
computer system fail at least once per week in the next year? The answer is $52 \times (1 - P (X = 0))$.
But how would you estimate $P (X = 0)$? Consider two approaches.

1. **Nonparametric**. Estimate $P (X = 0)$ by the relative frequency of number of zeros in the
above sample, which is 12 out of 104. Thus our estimate of $P (X = 0)$ is $12/104$. Hence,
our estimate of the number of weeks when there will be at least one computer failure is
$52 \times (1 - 12/104) = 46$.
2. **Parametric**. Suppose we assume that $X$ follows the Poisson distribution with parameter 
$\lambda$.
Then the answer to the above question is
\[52 \times (1 - P (X = 0)) = 52 \times \left(1 - e^{-\lambda} \frac{\lambda^0}{0!} \right)
= 52 \times 1 - e^{-\lambda}\]
which involves the unknown parameter $\lambda$. For the Poisson distribution we know that $E(X) =
\lambda$. Hence we could use the sample mean $\bar X$ to estimate $E(X) = \lambda$. Thus we estimate
$\hat \lambda = \bar x = 3.75.$ This type of estimator is called a moment estimator.
Now our estimate of the number of weeks when there will be at least one computer failure is
$52 \times (1 - e^{-3.75}) = 50.78 \approx 51$, which is very different from our
answer of 46 from the nonparametric approach.
:::


### Should we prefer parametric or nonparametric and why?

The parametric approach should be preferred if the assumption of the Poisson distribution can
be justified for the data. For example, we can look at the data histogram or compare the fitted
probabilities of different values of $X$, i.e. 
\[\hat P (X = x) = e^{-\hat \lambda} \frac{\hat \lambda}{x!},\]
with the relative frequencies from the
sample. In general, often model-based analysis is preferred because it is more precise and accurate,
and we can find estimates of uncertainty in such analysis based on the structure of the model. We
shall see this later.

The nonparametric approach should be preferred if the model cannot be justified for the data,
as in that case the parametric approach will provide incorrect answers.

<!-- TODO: actually do this with the data? -->


## Estimation

### Introduction 

Once we have collected data and proposed a statistical model for our data, the initial statistical
analysis usually involves estimation.

- For a parametric model, we need to estimate the unknown (unspecified) parameter $\lambda$. For
example, if our model for the computer failure data is that they are i.i.d. Poisson, we need to
estimate the mean ($\lambda$) of the Poisson distribution.
- For a nonparametric model, we may want to estimate the properties of the data-generating
distribution. For example, if our model for the computer failure data is that they are i.i.d.,
following the distribution of an unspecified common random variable $X$, then we may want
to estimate $\mu = E(X)$ or $\sigma^2 = \operatorname{Var}(X)$.

In the following, we use the generic notation $\theta$ to denote the *estimand*
(what we want to estimate
or the parameter). For example, $\theta$ is the parameter $\lambda$ in the first example, and 
$\theta$ may be either $\mu$
or $\sigma^2$ or both in the second example.



### Population and sample


Recall that a statistical model specifies a probability distribution for the random variables $\boldsymbol{X}$ corresponding to the data observations $\boldsymbol{x}$.

- The observations $\boldsymbol{x} = (x_1 , \ldots , x_n )$ are called the sample, and quantities derived from the
sample are sample quantities. For example, as in Chapter 1, we call
\[\bar x = \frac{1}{n} \sum_{i=1}^n x_i\]
the sample mean.
- The probability distribution for $X$ specified in our model represents all possible observations
which might have been observed in our sample, and is therefore sometimes referred to as the
population. Quantities derived from this distribution are population quantities.
For example, if our model is that $X_1 , \ldots , X_n$ are i.i.d., following 
the common distribution of
a random variable $X$, then we call $E(X)$ the *population mean*.

### Statistic and estimator

A statistic $T(\boldsymbol{x})$ is any function of the observed data $x_{1}, \ldots, x_{n}$ alone (and therefore does not depend on any parameters or other unknowns).

An estimate of $\theta$ is any statistic which is used to estimate $\theta$ under a particular statistical model. We will use $\tilde{\theta}(\boldsymbol{x})$ (sometimes shortened to $\tilde{\theta}$ ) to denote an estimate of $\theta$.

An estimate $\tilde{\theta}(\boldsymbol{x})$ is an observation of a corresponding random variable $\tilde{\theta}(\boldsymbol{X})$ which is called an estimator. Thus an estimate is a particular observed value, e.g. 1.2, but an estimator is a random variable which can take values which are called estimates.

An estimate is a particular numerical value, e.g. $\bar{x}$; an estimator is a random variable, e.g. $\bar{X}$.

The probability distribution of any estimator $\tilde{\theta}(\boldsymbol{X})$ is called its sampling distribution. The estimate $\tilde{\theta}(\boldsymbol{x})$ is an observed value (a number), and is a single observation from the sampling distribution of $\tilde{\theta}(\boldsymbol{X})$.

:::{.example}
Suppose that we have a random sample $X_{1}, \ldots, X_{n}$ from the uniform distribution on the interval $(0, \theta)$ where $\theta>0$ is unknown. Suppose that $n=5$ and we have the sample observations $x_{1}=2.3, x_{2}=3.6, x_{3}=20.2, x_{4}=0.9, x_{5}=17.2$. Our objective is to estimate $\theta$. How can we proceed?

Here the pdf $f(x)=\frac{1}{\theta}$ for $0 \leq x \leq \theta$ and 0 otherwise. Hence $E(X)=\int_{0}^{\theta} \frac{1}{\theta} x d x=\frac{\theta}{2}$. There are many possible estimators for $\theta$, e.g. $\hat{\theta}_{1}(\boldsymbol{X})=2 \bar{X}$, which is motivated by the method of moments because $\theta=2 E(X)$. A second estimator is $\hat{\theta}_{2}(\boldsymbol{X})=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$, which is intuitive since $\theta$ must be greater than or equal to all observed values and thus the maximum of the sample value will be closest to $\theta$. 

How could we choose between the two estimators $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ ? This is where we need to learn the sampling distribution of an estimator to determine which estimator will be unbiased, i.e. correct on average, and which will have minimum variability. We will formally define these in a minute, but first let us derive the sampling distribution, i.e. the pdf, of $\hat{\theta}_{2}$. Note that $\hat{\theta}_{2}$ is a random variable since the sample $X_{1}, \ldots, X_{n}$ is random. We will first find its cdf and then differentiate the cdf to get the pdf. For ease of notation, suppose $Y=\hat{\theta}_{2}(\boldsymbol{X})=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$. For any $0<y<\theta$, the cdf of $Y, F(y)$ is given by
\begin{align*}
P(Y \leq y) &=P\left(\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\} \leq y\right) \\
&\left.=P\left(X_{1} \leq y, X_{2} \leq y, \ldots, X_{n} \leq y\right)\right) \quad \text{since max $\leq y$ if and only if each $\leq y$}] \\
&=P\left(X_{1} \leq y\right) P\left(X_{2} \leq y\right) \cdots P\left(X_{n} \leq y\right) \quad \text {since the $X_i$ are independent}\\
&=\frac{y}{\theta} \times \frac{y}{\theta} \times \cdots \times \frac{y}{\theta} \\
&=\left(\frac{y}{\theta}\right)^{n} .
\end{align*}


Now the pdf of $Y$ is 
\[f(y)=\frac{d F(y)}{d y}=n \frac{y^{n-1}}{\theta^{n}}, \quad 0 \leq y \leq \theta.\]
Using this pdf, it can be shown that $E\left(\hat{\theta}_{2}\right)=E(Y)=\frac{n}{n+1} \theta,$ and 
\[\operatorname{Var}\left(\hat{\theta}_{2}\right)=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}.\] 
:::


### Bias and mean square error

In the uniform distribution example we saw that the estimator 
$\hat{\theta}_{2}=Y=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ is a random variable and its 
pdf is given by $f(y)=n \frac{y^{n-1}}{\theta^{n}}$ for $0 \leq y \leq \theta$. 
This probability distribution is called the sampling distribution of $\hat{\theta}_{2}$.
From this we have seen that $E\left(\hat{\theta}_{2}\right)=\frac{n}{n+1} \theta$.

In general, we define the bias of an estimator $\tilde{\theta}(\boldsymbol{X})$ of $\theta$ to be
\[\operatorname{bias}(\tilde{\theta})=E(\tilde{\theta})-\theta.\]

An estimator $\tilde{\theta}(\boldsymbol{X})$ is said to be unbiased if
\[\operatorname{bias}(\tilde{\theta})=0 \text {, i.e. if } E(\tilde{\theta})=\theta.\]

So an estimator is unbiased if the expectation of its sampling distribution is equal to the quantity we are trying to estimate. Unbiased means "getting it right on average", i.e. under repeated sampling (relative frequency interpretation of probability).

Thus for the uniform distribution example, $\hat{\theta}_{2}$ is a biased estimator of $\theta$ and
\[\operatorname{bias}\left(\hat{\theta}_{2}\right)=E\left(\hat{\theta}_{2}\right)-\theta=\frac{n}{n+1} \theta-\theta=-\frac{1}{n+1} \theta,\]
which goes to zero as $n \rightarrow \infty$. However, $\hat{\theta}_{1}=2 \bar{X}$ is unbiased since $E\left(\hat{\theta}_{1}\right)=2 E(\bar{X})=2 \frac{\theta}{2}=\theta$. 

Unbiased estimators are "correct on average", but that does not mean that they are guaranteed to provide estimates which are close to the estimand $\theta$. A better measure of the quality of an estimator than bias is the mean squared error (or MSE), defined as
\[\operatorname{MSE}(\tilde{\theta})=E\left[(\tilde{\theta}-\theta)^{2}\right]\]

 Therefore, if $\tilde{\theta}$ is unbiased for $\theta$, i.e. if $E(\tilde{\theta})=\theta$,
  then MSE $(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})$. In general, we have the following result:

::: {.theorem}
\[\operatorname{MSE}(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2}\]
:::
  
The proof is similar to the proof of Theorem \@ref(thm:sse).

::: {.proof}
\begin{align*}
\operatorname{MSE}(\tilde{\theta}) &=E\left[(\tilde{\theta}-\theta)^{2}\right] \\
&=E\left[(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}\right] \\
&=E\left[(\tilde{\theta}-E(\tilde{\theta}))^{2}+(E(\tilde{\theta})-\theta)^{2}+2(\tilde{\theta}-E(\tilde{\theta}))(E(\tilde{\theta})-\theta)\right] \\
&=E[\tilde{\theta}-E(\tilde{\theta})]^{2}+E[E(\tilde{\theta})-\theta]^{2}+2 E[(\tilde{\theta}-E(\tilde{\theta}))(E(\tilde{\theta})-\theta)] \\
&=\operatorname{Var}(\tilde{\theta})+[E(\tilde{\theta})-\theta]^{2}+2(E(\tilde{\theta})-\theta) E[(\tilde{\theta}-E(\tilde{\theta}))] \\
&=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2}+2(E(\tilde{\theta})-\theta)[E(\tilde{\theta})-E(\tilde{\theta})] \\
&=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2} .
\end{align*}
:::

:::{ .example}
Continuing with the uniform distribution $U(0, \theta)$ example, we have seen that $\hat{\theta}_{1}=2 \bar{X}$ is unbiased for $\theta$ but $\operatorname{bias}\left(\hat{\theta}_{2}\right)=-\frac{1}{n+1} \theta$. How do these estimators compare with respect to the MSE? Since $\hat{\theta}_{1}$ is unbiased, its MSE is its variance. Later, we will prove that for random sampling from any population
\[\operatorname{Var}(\bar{X})=\frac{\operatorname{Var}(X)}{n},\]
where $\operatorname{Var}(X)$ is the variance of the population sampled from. 
Returning to our example, we know that if $X \sim U(0, \theta)$ then $\operatorname{Var}(X)=\frac{\theta^{2}}{12}$. Therefore we have
\[
\operatorname{MSE}\left(\hat{\theta}_{1}\right)=\operatorname{Var}\left(\hat{\theta}_{1}\right)=\operatorname{Var}(2 \bar{X})=4 \operatorname{Var}(\bar{X})=4 \frac{\theta^{2}}{12 n}=\frac{\theta^{2}}{3 n} .
\]

Now, for $\hat{\theta}_{2}$ we know that:

1. $\operatorname{Var}\left(\hat{\theta}_{2}\right)=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}$
2. $\operatorname{bias}\left(\hat{\theta}_{2}\right)=-\frac{1}{n+1} \theta$.

Now
\begin{align*}
\operatorname{MSE}\left(\hat{\theta}_{2}\right) &=\operatorname{Var}\left(\hat{\theta}_{2}\right)+\operatorname{bias}\left(\hat{\theta}_{2}\right)^{2} \\
&=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}+\frac{\theta^{2}}{(n+1)^{2}} \\
&=\frac{\theta^{2}}{(n+1)^{2}}\left(\frac{n}{n+2}+1\right) \\
&=\frac{\theta^{2}}{(n+1)^{2}} \frac{2 n+2}{n+2} .
\end{align*}
The MSE of $\hat{\theta}_{2}$ is an order of magnitude smaller than the MSE of $\hat{\theta}_{1}$
(of order $1/n^{2}$ rather than $1/n$), providing justification for the preference of $\hat{\theta}_{2}=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ as an estimator of $\theta$.

## Estimating the population mean

### Introduction

Often, one of the main tasks of a statistician is to estimate a population average or mean. However the estimates, using whatever procedure, will not be usable or scientifically meaningful if we do not know their associated uncertainties. For example, a statement such as: "the Arctic ocean will be completely ice-free in the summer in the next few decades" provides little information as it does not communicate the extent or the nature of the uncertainty in it. Perhaps a more precise statement could be: "the Arctic ocean will be completely ice-free in the summer some time in the next 20-30 years". This last statement not only gives a numerical value for the number of years for complete ice-melt in the summer, but also acknowledges the uncertainty of $\pm 5$ years in the estimate. A statistician's main job is to estimate such uncertainties. In this lecture, we will get started with estimating uncertainties when we estimate a population mean. We will introduce the standard error of an estimator.

### Estimation of a population mean

Suppose that $X_{1}, \ldots, X_{n}$ is a random sample from any probability distribution $f(x)$, which may be discrete or continuous. Suppose that we want to estimate the unknown population mean $E(X)=\mu$ and variance, $\operatorname{Var}(X)=\sigma^{2}$. In order to do this, it is not necessary to make any assumptions about $f(x)$, so this may be thought of as nonparametric inference.

We have the following results: 

::: {.theorem #sample-mean-unbiased}
Suppose $X_1, \ldots, X_n$ is a random sample, 
with $E(X)=\mu$. The sample mean
\[\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}\]
is an unbiased estimator of $\mu=E(X)$, i.e. $E(\bar{X})=\mu$.
:::

In other words, the sample mean is an unbiased estimator of the population mean.

::: {.proof}
We have
\[E[\bar{X}]=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} E(X)=E(X)\]
so $\bar{X}$ is an unbiased estimator of $E(X)$.
:::

::: {.theorem #var-sample-mean}
Suppose $X_1, \ldots, X_n$ is a random sample, with $\operatorname{Var}(X)=\sigma^{2}$.
Then
\[\operatorname{Var}(\bar{X})=\frac{\sigma^{2}}{n}.\]
:::

::: {.proof}
We use the result that for independent random variables the variance of the sum is the sum of the variances from Section \@ref(sec:sum-rvs) . Thus,
\[\operatorname{Var}[\bar{X}]=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}(X)=\frac{n}{n^{2}} \operatorname{Var}(X)=\frac{\sigma^{2}}{n},\]
:::

Together, Theorems \@ref(thm:sample-mean-unbiased) and \@ref(thm:var-sample-mean)
imply that the MSE of $\bar{X}$ is $\operatorname{Var}(X) / n$.

::: {.theorem}
Suppose $X_1, \ldots, X_n$ is a random sample, 
with $\operatorname{Var}(X)=\sigma^{2}$. The sample variance with divisor $n-1$
\[S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\]
is an unbiased estimator of $\sigma^{2}$, i.e. $E\left(S^{2}\right)=\sigma^{2}$.
:::

In other words, the sample variance is an unbiased estimator of the population variance.

::: {.proof}
We need to show
$E\left(S^{2}\right)=\sigma^{2}$. We have
\[S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right].\]
To evaluate the expectation of the above, we need $E\left(X_{i}^{2}\right)$ and $E\left(\bar{X}^{2}\right)$. In general, we know for any random variable,
\[\operatorname{Var}(Y)=E\left(Y^{2}\right)-(E(Y))^{2} \quad \Rightarrow E\left(Y^{2}\right)=\operatorname{Var}(Y)+(E(Y))^{2}.\]
Thus, we have
\[E\left(X_{i}^{2}\right)=\operatorname{Var}\left(X_{i}\right)+\left(E\left(X_{i}\right)\right)^{2}=\sigma^{2}+\mu^{2},\]
and
\[E\left(\bar{X}^{2}\right)=\operatorname{Var}(\bar{X})+(E(\bar{X}))^{2}=\sigma^{2} / n+\mu^{2},\]
from Theorems \@ref(thm:sample-mean-unbiased) and \@ref(thm:var-sample-mean).
So
\begin{align*}
E\left(S^{2}\right) &=E\left\{\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right]\right\} \\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n} E\left(X_{i}^{2}\right)-n E\left(\bar{X}^{2}\right)\right] \\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n}\left(\sigma^{2}+\mu^{2}\right)-n\left(\sigma^{2} / n+\mu^{2}\right)\right] \\
&=\frac{1}{n-1}\left[n \sigma^{2}+n \mu^{2}-\sigma^{2}-n \mu^{2}\right] \\
&=\sigma^{2} \equiv \operatorname{Var}(X) .
\end{align*}
:::


### Standard deviation and standard error

For an unbiased  estimator $\tilde{\theta}$,
\[\operatorname{MSE}(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})\]
and therefore the sampling variance of the estimator is an important summary of its quality.

We usually prefer to focus on the standard deviation of the sampling distribution of $\tilde{\theta}$,
\[\text {s.d.}(\tilde{\theta})=\sqrt{\operatorname{Var}(\tilde{\theta})}.\]

In practice we will not know s.d.$(\tilde{\theta})$, as it will typically depend on unknown features of the distribution of $X_{1}, \ldots, X_{n}$. However, we may be able to estimate s.d.$(\tilde{\theta})$ using the observed sample $x_{1}, \ldots, x_{n}$. We define the standard error, s.e.$(\tilde{\theta})$, of an estimator $\tilde{\theta}$ to be an estimate of the standard deviation of its sampling distribution, s.d.$(\tilde{\theta})$.

Standard error of an estimator is an estimate of the standard deviation of its sampling distribution

We proved that
\[\operatorname{Var}[\bar{X}]=\frac{\sigma^{2}}{n} \Rightarrow \text { s.d. }(\bar{X})=\frac{\sigma}{\sqrt{n}}.\]

As $\sigma$ is unknown, we cannot calculate this standard deviation. However, we know that $E\left(S^{2}\right)=\sigma^{2}$, i.e. that the sample variance is an unbiased estimator of the population variance. Hence $S^{2} / n$ is an unbiased estimator for $\operatorname{Var}(\bar{X})$. Therefore we obtain the standard error of the mean, s.e. $(\bar{X})$, by plugging in the estimate
\[s=\left(\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)^{1 / 2}\]
   of $\sigma$ into s.d.$(\bar{X})$ to obtain
   \[\text {s.e.}(\bar{X})=\frac{s}{\sqrt{n}}.\]

<!-- TODO: refer back to computer failure data in C1 -->
Therefore, for the computer failure data, our estimate, $\bar{x}=3.75$, for the population mean is associated with a standard error
\[\text{s.e.}(\bar{X})=\frac{3.381}{\sqrt{104}}=0.332.\]

Note that this is "a" standard error, so other standard errors may be available. Indeed, for parametric inference, where we make assumptions about $f(x)$, alternative standard errors are available. 
For example, if $X_{1}, \ldots, X_{n}$ are i.i.d. $\operatorname{Poisson}(\lambda)$ random variables,
$E(X)=\lambda$, so $\bar{X}$ is an unbiased estimator of $\lambda$. $\operatorname{Var}(X)=\lambda$, so another $\text{s.e.}(\bar{X})=\sqrt{\hat{\lambda} / n}=\sqrt{\bar{x} / n}$.
   In the computer failure data example, this is $\sqrt{\frac{3.75}{104}}=0.19$.

## Interval estimation

### Introduction

An estimate $\tilde{\theta}$ of a parameter $\theta$ is sometimes referred to as a point estimate. The usefulness of a point estimate is enhanced if some kind of measure of its precision can also be provided. Usually, for an unbiased estimator, this will be a standard error, an estimate of the standard deviation of the associated estimator, as we have discussed previously. An alternative summary of the information provided by the observed data about the location of a parameter $\theta$ and the associated precision is an interval estimate or confidence interval.

Suppose that $x_{1}, \ldots, x_{n}$ are observations of random variables $X_{1}, \ldots, X_{n}$ whose joint pdf is specified apart from a single parameter $\theta$. To construct a confidence interval for $\theta$, we need to find a random variable $T(\mathbf{X}, \theta)$ whose distribution does not depend on $\theta$ and is therefore known. This random variable $T(\mathbf{X}, \theta)$ is called a *pivot* for $\theta$. Hence we can find numbers $h_{1}$ and $h_{2}$ such that
\begin{equation}
P\left(h_{1} \leq T(\mathbf{X}, \theta) \leq h_{2}\right)=1-\alpha
(\#eq:pivot)
\end{equation}
where $1-\alpha$ is any specified probability. If \@ref(eq:pivot)  can be 'inverted',we can write it as
\[P\left(g_{1}(\mathbf{X}) \leq \theta \leq g_{2}(\mathbf{X})\right)=1-\alpha.\]

Hence with probability $1-\alpha$, the parameter $\theta$ will lie between the random variables $g_{1}(\mathbf{X})$ and $g_{2}(\mathbf{X})$. Alternatively, the random interval $\left(g_{1}(\mathbf{X}), g_{2}(\mathbf{X})\right)$ includes $\theta$ with probability $1-\alpha$. Now, when we observe $x_{1}, \ldots, x_{n}$, we observe a single observation of the random interval $\left(g_{1}(\mathbf{X}), g_{2}(\mathbf{X})\right)$, which can be evaluated as $\left(g_{1}(\mathbf{x}), g_{2}(\mathbf{x})\right)$. We do not know if $\theta$ lies inside or outside this interval, but we do know that if we observed repeated samples, then $100(1-\alpha) \%$ of the resulting intervals would contain $\theta$. Hence, if $1-\alpha$ is high, we can be reasonably confident that our observed interval contains $\theta$. We call the observed interval $\left(g_{1}(\mathbf{x}), g_{2}(\mathbf{x})\right)$ a $100(1-\alpha) \%$ confidence interval for $\theta$. It is common to present intervals with high confidence levels, usually $90 \%, 95 \%$ or $99 \%$, so that $\alpha=0.1,0.05$ or $0.01$ respectively.

### Confidence interval for a normal mean

Let $X_{1}, \ldots, X_{n}$ be i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables. 
From the Central Limit Theorem (Section \@ref(sec:clt)), we know that for large $n$,
\[\bar{X} \sim N\left(\mu, \sigma^{2} / n\right) \quad \Rightarrow \quad \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \sim N(0,1).\]

Suppose we know that $\sigma=10$, so $\sqrt{n}(\bar{X}-\mu) / \sigma$ is a pivot for $\mu$. Then we can use the distribution function of the standard normal distribution to find values $h_{1}$ and $h_{2}$ such that
\[P\left(h_{1} \leq \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \leq h_{2}\right)=1-\alpha\]
for a chosen value of $1-\alpha$ which is called the confidence level. So $h_{1}$ and $h_{2}$ are chosen so that the shaded area in Figure \@ref(fig:normal-quantiles) is equal to the confidence level $1-\alpha$.

```{r normal-quantiles, echo = FALSE, fig.cap = "$h_1$ and $h_2$ are chosen to make the shaded area equal to the confidence level $1-\\alpha$", fig.width = 5, fig.height = 3, out.width = "60%", fig.align = "center"}
library(ggplot2)

lim <- 4
h <- qnorm(0.975)
plot_data <- tibble(x = seq(-h, h, length.out = 100),
                    ymax = dnorm(x),
                    ymin = 0)

plot_data %>%
    ggplot(mapping = aes(x = x, ymin = ymin, ymax = ymax)) + 
    geom_function(fun = dnorm) +
    geom_ribbon(alpha = 0.3) +
    xlab("x") +
    ylab("f(x)") +
    scale_x_continuous(breaks = c(0, -h, h),
                       labels = c(0, "h1", "h2"),
                       limits = c(-lim, lim)) +
    geom_hline(aes(yintercept = 0)) +
    theme(panel.grid.minor = element_blank(),
          panel.grid = element_blank())
```

It is common practice to make the interval symmetric, so that the two unshaded areas are equal
(to $\alpha / 2$ ), in which case
\[-h_{1}=h_{2} \equiv h \quad \text { and } \quad \Phi(h)=1-\frac{\alpha}{2}.\]

The most common choice of confidence level is $1-\alpha=0.95$, in which case $h=1.96=\texttt{qnorm(0.975)}$. You may also occasionally see $90\%$ ($h=1.645=\texttt{qnorm(0.95)}$) or 
$99\%$ ($h=2.58=\texttt{qnorm(0.995)}$) intervals. 

Therefore we have
\begin{align*}
& P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \leq 1.96\right)=0.95 \\
\Rightarrow & P\left(\bar{X}-1.96 \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}+1.96 \frac{\sigma}{\sqrt{n}}\right)=0.95 .
\end{align*}

Hence, $\bar{X}-1.96 \frac{\sigma}{\sqrt{n}}$ and $\bar{X}+1.96 \frac{\sigma}{\sqrt{n}}$ are the endpoints of a random interval which includes $\mu$ with probability $0.95$. The observed value of this interval, $\left(\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}\right)$, is called a 95\% confidence interval for $\mu$.

:::{.example name="Fast food"} 
For the fast food waiting time data, we have $n=20$ data points combined from the morning and afternoon data sets. We have $\bar{x}=67.85$ and $n=20$. Hence, under the normal model assuming (just for the sake of illustration) $\sigma=18$, a 95\% confidence interval for $\mu$ is

\[\begin{gathered}
67.85-1.96(18 / \sqrt{20}) \leq \mu \leq 67.85+1.96(18 / \sqrt{20}) \\
\Rightarrow 59.96 \leq \mu \leq 75.74
\end{gathered}\]

The R command is 
```{r, eval = FALSE}
mean(fastfood) + c(-1, 1) * qnorm(0.975) * 18 / sqrt (20)
```
assuming `fastfood` is the vector containing 20 waiting times.

In reality, it is more likely that $\sigma$ is unknown in this example,
and we need to seek alternative methods for finding the confidence intervals.
:::


### Some remarks about confidence intervals

1. Notice that $\bar{x}$ is an unbiased estimate of $\mu, \sigma / \sqrt{n}$ is the standard error of the estimate and $1.96$ (in general $h$ in the above discussion) is a critical value from the associated known sampling distribution. The formula $(\bar{x} \pm 1.96 \sigma / \sqrt{n})$ for the confidence interval is then generalised as:
\[\text { Estimate } \pm \text { Critical value } \times \text { Standard error }\]
where the estimate is $\bar{x}$, the critical value is $1.96$ and the standard error is $\sigma / \sqrt{n}$. This is so much easier to remember. We will see that this formula holds in many of the following examples, but not all.

2. Confidence intervals are frequently used, but also frequently misinterpreted. A $100(1-\alpha) \%$ confidence interval for $\theta$ is a single observation of a random interval which, under repeated sampling, would include $\theta$ $100(1-\alpha) \%$ of the time.

3. A confidence interval is not a probability interval. You should avoid making statements like $P(1.3<\theta<2.2)=0.95$. In the classical approach to statistics you can only make probability statements about random variables, and $\theta$ is assumed to be a constant.

4. If a confidence interval is interpreted as a probability interval, this may lead to problems. For example, suppose that $X_{1}$ and $X_{2}$ are i.i.d. $U\left(\theta-\frac{1}{2}, \theta+\frac{1}{2}\right)$ random variables. Then $P\left(\min \left(X_{1}, X_{2}\right)<\theta<\max \left(X_{1}, X_{2}\right)\right)=\frac{1}{2}$ so $\left(\min \left(x_{1}, x_{2}\right), \max \left(x_{1}, x_{2}\right)\right)$ is a $50 \%$ confidence interval for $\theta$, where $x_{1}$ and $x_{2}$ are the observed values of $X_{1}$ and $X_{2}$. Now suppose that $x_{1}=0.3$ and $x_{2}=0.9$. What is $P(0.3<\theta<0.9)$ ?

### Confidence intervals using the CLT

#### Introduction 

Confidence intervals are generally difficult to find. The difficulty lies in finding a pivot, i.e. a statistic $T(\mathbf{X}, \theta)$ such that
\[P\left(h_{1} \leq T(\mathbf{X}, \theta) \leq h_{2}\right)=1-\alpha\]
for two suitable numbers $h_{1}$ and $h_{2}$, and also that the above can be inverted to put the unknown $\theta$ in the middle of the inequality inside the probability statement. One solution to this problem is to use the powerful Central Limit Theorem (CLT) to claim normality, and then basically follow the above normal example for known variance.

#### Confidence intervals for $\mu$ using the CLT

The CLT allows us to assume the large sample approximation
\[\sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \stackrel{\text { approx }}{\sim} N(0,1) \text { as } n \rightarrow \infty.\]
Thus an (approximate) $95 \%$ confidence interval (CI) for $\mu$ is given by $\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}$. But note that $\sigma$ is unknown so this CI cannot be used unless we can estimate $\sigma$, i.e. replace the unknown s.d. of $\bar{X}$ by its estimated standard error. In this case, we get the CI in the familiar form:
\[\text { Estimate } \pm \text { Critical value } \times \text { Standard error }\]

Suppose that we do not assume any distribution for the sampled random variable $X$ but assume only that $X_{1}, \ldots, X_{n}$ are i.i.d, following the distribution of $X$ where $E(X)=\mu$ and $\operatorname{Var}(X)=\sigma^{2}$. We know that the standard error of $\bar{X}$ is $s / \sqrt{n}$ where $s$ is the sample standard deviation with divisor $n-1$. Then the following provides an (approximate) $95 \%$ CI for $\mu$:
\[\bar{x} \pm 1.96 \frac{s}{\sqrt{n}}.\]

:::{.example name="Computer failures"}
For the computer failure data, $\bar{x}=3.75, s=3.381$ and $n=104$. Under the model that the data are observations of i.i.d. random variables with population mean $\mu$ (but no other assumptions about the underlying distribution), we compute a 95\% confidence interval for $\mu$ to be
\[\left(3.75-1.96 \frac{3.381}{\sqrt{104}}, 3.75+1.96 \frac{3.381}{\sqrt{104}}\right)=(3.10,4.40).\]
:::

If we can assume a distribution for $X$, i.e. a parametric model for $X$, then we can do slightly better in estimating the standard error of $\bar{X}$ and as a result we can improve upon the previously obtained $95 \%$ CI. Two examples follow.

:::{.example name="Poisson" #poisson-ci-simple}
If $X_{1}, \ldots, X_{n}$ are modelled as i.i.d. Poisson $(\lambda)$ random variables, then $\mu=\lambda$ and $\sigma^{2}=\lambda$. We know $\operatorname{Var}(\bar{X})=\sigma^{2} / n=\lambda / n$. Hence a standard error is $\sqrt{\hat{\lambda} / n}=\sqrt{\bar{x} / n}$ since $\hat{\lambda}=\bar{X}$ is an unbiased estimator of $\lambda$. Thus a 95\% CI for $\mu=\lambda$ is given by
\[\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}}{n}}.\]

For the computer failure data, $\bar{x}=3.75, s=3.381$ and $n=104$. Under the model that the data are observations of i.i.d. random variables following a Poisson distribution with population mean $\lambda$, we compute a $95 \%$ confidence interval for $\lambda$ as
\[\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}}{n}}=3.75 \pm 1.96 \sqrt{3.75 / 104}=(3.38,4.12).\]

We see that this interval is narrower $(0.74=4.12-3.38)$ than the earlier interval $(3.10,4.40)$, which has a length of $1.3$. We prefer narrower confidence intervals as they facilitate more accurate inference regarding the unknown parameter.
:::

:::{.example name="Bernoulli"}
If $X_{1}, \ldots, X_{n}$ are modelled as i.i.d. Bernoulli $(p)$ random variables, then $\mu=p$ and $\sigma^{2}=p(1-p)$. We know $\operatorname{Var}(\bar{X})=\sigma^{2} / n=p(1-p) / n$. Hence a standard error is $\sqrt{\hat{p}(1-\hat{p}) / n}=\sqrt{\bar{x}(1-\bar{x}) / n}$, since $\hat{p}=\bar{X}$ is an unbiased estimator of $p$. Thus a $95 \%$ CI for $\mu=p$ is given by
\[\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}(1-\bar{x})}{n}}.\]

For the example, suppose $\bar{x}=0.2$ and $n=10$. Then we obtain the $95 \%$ CI as
\[0.2 \pm 1.96 \sqrt{(0.2 \times 0.8) / 10}=(-0.048,0.448)\]

This is wrong as $n$ is too small for the large sample approximation to be accurate. Hence we need to look for other alternatives which may work better.
:::

#### Confidence interval for a Bernoulli $p$ by quadratic inversion

It turns out that for the Bernoulli and Poisson distributions we can find alternative confidence intervals without using the approximation for standard error but still using the CLT. This is more complicated and requires us to solve a quadratic equation. We consider the two distributions separately.

We start with the CLT and obtain the following statement:
\[\begin{array}{cc} 
& P\left(-1.96 \leq \frac{\sqrt{n}(\bar{X}-p)}{\sqrt{p(1-p)}} \leq 1.96\right)=0.95 \\
\Leftrightarrow & P(-1.96 \sqrt{p(1-p)} \leq \sqrt{n}(\bar{X}-p) \leq 1.96 \sqrt{p(1-p)})=0.95 \\
\Leftrightarrow & P(-1.96 \sqrt{p(1-p) / n} \leq(\bar{X}-p) \leq 1.96 \sqrt{p(1-p) / n})=0.95 \\
\Leftrightarrow & P(p-1.96 \sqrt{p(1-p) / n} \leq \bar{X} \leq p+1.96 \sqrt{p(1-p) / n})=0.95 \\
\Leftrightarrow & P(L(p) \leq \bar{X} \leq R(p))=0.95,
\end{array}\]
where $L(p)=p-h \sqrt{p(1-p) / n}, R(p)=p+h \sqrt{p(1-p) / n}, h=1.96$. Now, consider the inverse mappings $L^{-1}(x)$ and $R^{-1}(x)$ so that
\begin{align*}
&P[L(p) \leq \bar{X} \leq R(p)]=0.95 \\
&\Leftrightarrow P\left[R^{-1}(\bar{X}) \leq p \leq L^{-1}(\bar{X})\right]=0.95
\end{align*}

which now defines our confidence interval $\left(R^{-1}(\bar{X}), L^{-1}(\bar{X})\right)$ for $p$. We can obtain $R^{-1}(\bar{x})$ and $L^{-1}(\bar{x})$ by solving the equations $R(p)=\bar{x}$ and $L(p)=\bar{x}$ for $p$, treating $n$ and $\bar{x}$ as known quantities. Thus we have
\begin{align*}
& R(p)=\bar{x}, \quad L(p)=\bar{x} \\
& \Leftrightarrow \quad(\bar{x}-p)^{2}=h^{2} p(1-p) / n, \quad \text { where } h=1.96 \\
& \Leftrightarrow \quad p^{2}\left(1+h^{2} / n\right)-p\left(2 \bar{x}+h^{2} / n\right)+\bar{x}^{2}=0
\end{align*}
The endpoints of the confidence interval are the roots of the quadratic. Hence, the endpoints of the $95 \%$ confidence interval for $p$ are
\begin{align*}
& \frac{\left(2 \bar{x}+\frac{h^{2}}{n}\right) \pm\left[\left(2 \bar{x}+\frac{h^{2}}{n}\right)^{2}-4 \bar{x}^{2}\left(1+\frac{h^{2}}{n}\right)\right]^{1 / 2}}{2\left(1+\frac{h^{2}}{n}\right)} \\
=& \frac{\left(\bar{x}+\frac{h^{2}}{2 n}\right) \pm\left[\left(\bar{x}+\frac{h^{2}}{2 n}\right)^{2}-\bar{x}^{2}\left(1+\frac{h^{2}}{n}\right)\right]^{1 / 2}}{\left(1+\frac{h^{2}}{n}\right)} \\
=& \frac{\bar{x}+\frac{h^{2}}{2 n} \pm \frac{h}{\sqrt{n}}\left[\frac{h^{2}}{4 n}+\bar{x}(1-\bar{x})\right]^{1 / 2}}{\left(1+\frac{h^{2}}{n}\right)} .
\end{align*}

This is sometimes called the Wilson Score Interval. 

<!-- The following R code calculates this for given $n, \bar{x}$ and confidence level $\alpha$ which determines the value of $h$.  -->
<!-- TODO: find R code? -->

Returning to the previous example, $n=10$ and $\bar{x}=0.2$, the $95 \%$ CI obtained from this method is $(0.057,0.510)$ compared to the previous illegitimate one $(-0.048,0.448)$. In fact you can see that the intervals obtained by quadratic inversion are more symmetric and narrower as $n$ increases, and are also more symmetric for $\bar{x}$ closer to $0.5$:

\begin{center}
\begin{tabular}{cccccc}
\hline$n$ & $\bar{x}$ & \multicolumn{2}{c}{ Quadratic inversion } & \multicolumn{2}{c}{ Plug-in s.e. estimation } \\
& & Lower end & Upper end & Lower end & Upper end \\
\hline 10 & $0.2$ & $0.057$ & $0.510$ & $-0.048$ & $0.448$ \\
10 & $0.5$ & $0.237$ & $0.763$ & $0.190$ & $0.810$ \\
20 & $0.1$ & $0.028$ & $0.301$ & $-0.031$ & $0.231$ \\
20 & $0.2$ & $0.081$ & $0.416$ & $0.025$ & $0.375$ \\
20 & $0.5$ & $0.299$ & $0.701$ & $0.281$ & $0.719$ \\
50 & $0.1$ & $0.043$ & $0.214$ & $0.017$ & $0.183$ \\
50 & $0.2$ & $0.112$ & $0.330$ & $0.089$ & $0.311$ \\
50 & $0.5$ & $0.366$ & $0.634$ & $0.361$ & $0.639$ \\
\hline
\end{tabular}
\end{center}

For smaller $n$ and $\bar{x}$ closer to 0 (or 1), the approximation required for the plug-in estimate of the standard error is insufficiently reliable. However, for larger $n$ it is adequate.

#### Confidence interval for a Poisson $\lambda$ by quadratic inversion

Here we proceed as in the Bernoulli case and using the CLT claim that a 95\% CI for $\lambda$ is given by
\[P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-\lambda)}{\sqrt{\lambda}} \leq 1.96\right)=0.95 \quad \Rightarrow P\left(n \frac{(\bar{X}-\lambda)^{2}}{\lambda} \leq 1.96^{2}\right)=0.95.\]

Now the confidence interval for $\lambda$ is found by solving the (quadratic) equality for $\lambda$ by treating $n, \bar{x}$ and $h$ to be known:
\begin{align*}
& n \frac{(\bar{x}-\lambda)^{2}}{\lambda}=h^{2}, \quad \text { where } h=1.96 \\
\Rightarrow & \bar{x}^{2}-2 \lambda \bar{x}+\lambda^{2}=h^{2} \lambda / n \\
\Rightarrow & \lambda^{2}-\lambda\left(2 \bar{x}+h^{2} / n\right)+\bar{x}^{2}=0 .
\end{align*}
Hence, the endpoints of the $95 \%$ confidence interval for $\lambda$ are:
\[\frac{\left(2 \bar{x}+\frac{h^{2}}{n}\right) \pm\left[\left(2 \bar{x}+\frac{h^{2}}{n}\right)^{2}-4 \bar{x}^{2}\right]^{1 / 2}}{2}=\bar{x}+\frac{h^{2}}{2 n} \pm \frac{h}{n^{1 / 2}}\left[\frac{h^{2}}{4 n}+\bar{x}\right]^{1 / 2}.\]

:::{.example name="Computer failures"}
For the computer failure data, $\bar{x}=3.75$ and $n=104$. For a $95 \%$ confidence interval (CI), $h=1.96$. Hence, we calculate the above CI using the $\mathrm{R}$ commands:

```{r}
n <- length(compfail)
h <- qnorm(0.975)
x_bar <- mean(compfail)
x_bar + h^2/(2 * n) + c(-1, 1) * h / sqrt(n) * sqrt(h^2 / (4 * n) + x_bar)
```
The result is (3.40, 4.14), which is quite close to the earlier interval (3.38, 4.12)
from Example \@ref(exm:poisson-ci-simple).
:::


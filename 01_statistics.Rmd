# Introduction to Statistics {#intro-stats}

## What is statistics? {#what-is-statistics}

### Early and modern definitions

The word *statistics* has its roots in the Latin word *status* which means the state, and in the
middle of the 18th century was intended to mean
*collection, processing and use of data by the state*.
With the rapid industrialisation of Europe in the first half of the 19th century, statistics
became established as a discipline. This led to the formation of the Royal Statistical Society,
the premier professional association of statisticians in the UK and also world-wide, in 1834.
During this 19th century growth period, statistics acquired a new meaning as the *interpretation of data or methods of extracting information from data for decision making*. Thus
statistics has its modern meaning as the methods for
*collection, analysis and interpretation of data*.
Indeed, the Oxford English Dictionary defines *statistics* as

> The practice or science of collecting and analysing numerical data in large quantities, especially for the purpose of inferring
proportions in a whole from those in a representative sample.

Note that the word ‘state’ has gone from its definition. Instead, statistical methods are now
essential for **everyone** wanting to answer questions using data.

For example, will it rain tomorrow? Does eating red meat make us live longer? Is smoking
harmful during pregnancy? Is the new shampoo better than the old? Will the UK economy get
better after Brexit? At a more personal level: What degree classification will I get at graduation?
How long will I live for? What prospects do I have in the career I have chosen? How do I invest
my money to maximise the return? Will the stock market crash tomorrow?


### Uncertainty: the main obstacle to decision making

The main obstacle to answering the types of questions above is *uncertainty*, which means **lack of
one-to-one correspondence between cause and effect**. For example, having a diet of (well-cooked) red meat for a period of time is not going to kill me immediately. The effect of smoking
during pregnancy is difficult to judge because of the presence of other factors, e.g. diet and lifestyle;
such effects will not be known for a long time, e.g. at least until the birth. Thus it seems:

> **Uncertainty is the only certainty!**



### Statistics tames uncertainty

It is clear that we may never be able to get to the bottom of every case to learn the full truth
and so will have to make a decision under uncertainty; thus mistakes cannot be avoided!
If mistakes cannot be avoided, it is better to know how often we make mistakes (which
provides knowledge of the amount of uncertainty) by following a particular rule of decision
making.
Such knowledge could be put to use in finding a rule of decision making which does not betray
us too often, or which minimises the frequency of wrong decisions, or which minimises the
loss due to wrong decisions.

Thus we have the equation 
\[\text{Uncertain knowledge} +
  \text{Knowledge of the extent of uncertainty in it} =
  \text{Usable knowledge.}\]

Researchers often make guesses about scientific quantities. For example, try to guess my age.
These predictions are meaningless without the associated uncertainties. Instead, appropriate
data collection and correct application of statistical methods may enable us to make statements
like: I am 90% certain that the correct age is between 29 and 39 years. Remember, 

> to guess is cheap, to guess incorrectly is expensive  
-- <cite>old Chinese proverb.</cite>


### Why should I study statistics as part of my degree?

Studying statistics will equip you with the basic skills in data analysis and doing science with
data.
A decent level of statistical knowledge is required no matter what branch of mathematics,
engineering, science and social science you will be studying.
Learning statistical theories gives you the opportunity to practice your deductive mathematical skills on real life problems. In this way, you will improve at mathematical methods while
studying statistical methods.

>All knowledge is, in final analysis, history.  
>All sciences are, in the abstract, mathematics.  
>All judgements are, in their rationale, statistics.  
>-- <cite>Prof. C. R. Rao </cite>

### Lies, Damn Lies and Statistics?

Sometimes people say, "you can prove anything in statistics!" and many such jokes. Such remarks
bear testimony to the fact that often statistics and statistical methods are mis-quoted without
proper verification and robust justification. This is even more important in recent years of the global
pandemic when every day we have been showered with a deluge of numbers. 

Statistics can be very much
mis-used and mis-interpreted, and as statisticians it is our duty to
correctly apply statistical techniques to develop scientifically robust and strong arguments.
As discussed before statistical methods are the only viable tool whenever there is uncertainty in
decision making. In scientific investigations, statistics is an inevitable instrument in search of truth
when uncertainty cannot be totally removed from decision making. Of course, a statistical method
may not yield the best predictions in a very particular situation, but a systematic and robust
application of statistical methods will eventually win over pure guesses. For example, statistical
methods prove that cigarette smoking is bad for human health.


### What's in this module?

- **Chapter 1**: We will start with the basic statistics used in everyday life, e.g. mean, median,
mode, standard deviation, etc. Statistical analysis and report writing will be discussed. We
will also learn how to explore data using graphical methods.
  * For this we will use the R statistical package. R is freely available to download. Search
"download R" or go to: https://cran.r-project.org/. We will use it as a calculator
and also as a graphics package to explore data, perform statistical analysis, illustrate
theorems and calculate probabilities. 
  * In this module we will demonstrate using the R package. A nicer experience is provided
by the commercial, but still freely available, R Studio software. It is recommended that
you use that.
- **Chapter 2: Introduction to Probability**. We will define and interpret probability as a
measure of uncertainty. We will learn the rules of probability and then explore fun examples
of probability, e.g. the probability of winning the National Lottery.
- **Chapter 3: Random variables**. We will learn that the results of different random experiments lead to different random variables following distributions such as the binomial, and
normal. etc. We will learn their basic properties, e.g. mean and variance.
- **Chapter 4: Statistical Inference**. We will discuss basic ideas of statistical inference,
including techniques of point and interval estimation and hypothesis testing.



## Basic statistics

### Introduction

In Section \@ref(what-is-statistics) we got a glimpse of the nature of uncertainty and statistics.
Now we will
hands dirty with data and learn a bit more about it.
How do I obtain data? How do I summarise it? Which is the best measure among mean, median
and mode? What do I make of the spread of the data?

How should we collect data in the first place? Unless we can ask everyone in the population we
should select individuals randomly (haphazardly) in order to get a representative sample. Otherwise we may introduce bias. For example, in order to gauge student opinion in this class I should
not only survey the international students. But there are cases when systematic sampling may
be preferable. For example, selecting every third caller in a radio phone-in show for a prize, or
sampling air pollution hourly or daily. There is a whole branch of statistics called *survey methods*
or *sample surveys*, where these issues are studied.
As well as randomness, we need to pay attention to the **design** of the study. In a *designed
experiment* the investigator controls the values of certain experimental variables and then measures
a corresponding output or response variable. In *designed surveys* an investigator collects data on a
randomly selected sample of a well-defined population. Designed studies can often be more effective
at providing reliable conclusions, but are frequently not possible because of difficulties in the study.


 In this module, we will assume that we have data from $n$ randomly selected sampling units, 
 which we will conveniently denote by  $x_1, x_2, \dots, x_n$. 
 We will assume that these values are numeric, either discrete like
counts, e.g. number of road accidents, or continuous, e.g. heights of 4-year-olds, marks obtained
in an examination. We will consider the following example:

::: {.example name="Fast food service time" #fastfood}
The service times (in seconds) of customers at a fast-food
restaurant. The first row is for customers who were served from 9–10am and the second row is for
customers who who were served from 2–3pm on the same day.


```{r, echo = FALSE}
fastfood <- tibble(time = rep(c("AM", "PM"), times = 10),
                   service = c(38, 45, 100, 62, 64, 52, 43, 72, 63, 81, 59, 88,
                               107, 64, 52, 75, 86, 59, 77, 70))
                   
fastfood %>%
    mutate(customer = rep(1:10, each = 2)) %>%
    pivot_wider(names_from = "time", values_from = "service") %>%
    select(-customer) %>%
    t %>%
    kable
```

How can we explore the data?
:::


In general, we can
summarise categorical (not numeric) data by tables. For example: 5 reds, 6 blacks etc.
For numeric data $x_1, x_2, \dots, x_n$, we would like to know the centre (measures of location or
central tendency) and the spread or variability.

### Measures of location

#### Choosing a representative value for the data

We are seeking a representative value for the data 
$x_1, x_2, \dots, x_n$ which should be a function
of the data. If $a$ is that representative value then how much error is associated with it?
The total error could be the sum of squares of the errors, 
\[\text{SSE}(a) = \sum_{i=1}^n (x_i - a)^2\]
or the sum of the absolute errors
\[\text{SAE}(a) = \sum_{i=1}^n |x_i - a|.\]
What value of $a$ will minimise the SSE or the SAE? For SSE the answer is the sample mean
and for SAE the answer is the sample median.


#### The sample mean

The *sample mean* is
\[\bar x = \frac{1}{n} (x_1 + x_2 + \dots + x_n)
  = \frac{1}{n} \sum_{i=1}^n x_i.\]
  For the service time
data from Example \@ref(exm:fastfood), 
the AM mean time is 68.9 seconds and the PM mean time is 66.8 seconds.
  
::: {.theorem}
The sample mean $\bar x$ minimises the SSE.
:::


::: {.proof}
We have
\begin{align*}
\text{SSE}(a) &= \sum_{i=1}^{n}\left(x_{i}-a\right)^{2} \\
&=\sum_{i=1}^{n}\left(x_{i}-\bar{x}+\bar{x}-a\right)^{2} \quad \text{(Add and subtract $\bar{x}$)} \\
&=\sum_{i=1}^{n}\left\{\left(x_{i}-\bar{x}\right)^{2}+2\left(x_{i}-\bar{x}\right)(\bar{x}-a)+(\bar{x}-a)^{2}\right\} \\
&=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2(\bar{x}-a) \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)+\sum_{i=1}^{n}(\bar{x}-a)^{2} \\
&=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n(\bar{x}-a)^{2},
\end{align*}
since $\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)=n \bar{x}-n \bar{x}=0$.

The first term is free of $a$ and the second term is non-negative for any value
of $a$. Hence the minimum occurs when the second term is zero, i.e. when $a = \bar x$.
:::

We have established the fact that
the sum of (or mean) squares of the deviations from any number $a$ is minimised when $a$ is the mean.
This justifies why we often use the mean as a representative value. 

#### The sample median

The *sample median* is the middle value in the ordered list of observations
$x_{(1)} \leq x_{(2)} \leq \dots \leq x_{(n)}$.
For the AM service time data \[38 < 43 < 52 < 59 < 63 < 64 < 77 < 86 < 100 < 107.\]
If $n$ is odd, there is a unique middle value.
If $n$ is even, there are two middle values (63 and 64 for the AM service time data).
By convention, we define the sample median to be the mean of these values
(63.5 for the the AM service time data).
More formally, we define the median as 
\[ \operatorname{median}(x) = \begin{cases}
  x_{\left(\frac{n+1}{2}\right)} & \text{if $n$ is odd} \\
  \frac{1}{2}\Big[x_{\left(\frac{n}{2}\right)} + x_{\left(\frac{n}{2} +1\right)}\Big] & \text{if $n$ is even.} 
\end{cases}\]

::: {.theorem}
The sample median minimises the SSE.
:::

::: {.proof}
We have
\begin{align*}
\text{SAE}(a) &=
 \sum_{i=1}^n |x_i − a| \\
 &= \sum_{i=1}^n |x_{(i)} − a| \\
 &= |x_{(1)} − a| + |x_{(n)} − a| + |x_{(2)} − a| + |x_{(n−1)} − a| + \dots.
 \end{align*}
Looking at pairs of terms, we find:

- $|x_{(1)} − a| + |x_{(n)} − a|$ is minimised when $a$ is such that $x_{(1)} \leq a \leq x_{(n)}$.
- $|x_{(2)} − a| + |x_{(n−1)} − a|$ is minimised when $a$ is such that $x_{(2)} \leq a \leq x_{(n−1)}$.
- When $n$ is odd, the last term $|x_{(n+1)} − a|$ is minimised when $a = x_{\left(\frac{n+1}{2}\right)}$ or the middle value in the ordered list, which we defined as the median.
- If however, $n$ is even, the last pair of terms will be 
$|x_{\left(\frac{n}{2}\right)} − a| + |x_{\left(\frac{n}{2} +1\right)} − a|.$ This will be
minimised when $a$ is any value between $x_{\left(\frac{n}{2}\right)}$ and $x_{\left(\frac{n}{2} +1\right)}$. We defined the median to be the mean of these two values, so the median minimises the SAE.
:::

We have
established the fact that
the sum of (or mean) of the absolute deviations from any number $a$ is
minimised when $a$ is the median. This justifies why
median is also often used as a representative value.

The mean gets more affected by extreme observations while the median does not. For example for
the AM service times, suppose the next observation is 190. The median will be 64 instead of 63.5
but the mean will shoot up to 79.9.

#### The sample mode

The mode or the most frequent (or the most likely) value in the data is taken as the most representative value if we consider a 0-1 error function instead of the SAE or SSE above. Here, one assumes
that the error is 0 if our guess $a$ is the correct answer and 1 if it is not. It can then be proved that the best guess $a$ will be the mode of the data.

#### Impact of extreme observations


### Measures of spread

A quick measure of the spread is the *range*, which is defined as the difference between the
maximum and minimum observations. For the AM service times the range is $69$ ($107 - 38$)
seconds.

The *variance* is
\[\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.\]
We have
\[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n}\left(x_{i}^{2}-2 x_{i} \bar{x}+\bar{x}^{2}\right)=\sum_{i=1}^{n} x_{i}^{2}-2 \bar{x}(n \bar{x})+n \bar{x}^{2}=\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2},\]
so we calculate variance as
$$
\operatorname{Var}(x)=\frac{1}{n-1}\left(\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2}\right).
$$
Sometimes the variance is defined with the divisor $n$ instead of $n - 1$. We have chosen $n - 1$
since this is the default in R. We will return to this in Chapter \@ref(inference).

The *standard deviation* is the square root of variance
\[s = \operatorname{sd}(x) = \sqrt{\operatorname{Var}(x)}}.\]
The standard deviation (sd) for the AM service times is 23.2 seconds. Note that it has the
same unit as the observations.

The *interquartile range* (IQR) is the difference between the third, $Q_3$ and first, $Q_1$ quartiles,
which are respectively the observations ranked $\frac{1}{4}(3n + 1)$ and $\frac{1}{4}(n + 3)$
in the ordered list.
Note that the median is the second quartile, $Q_2$. When $n$ is even, definitions of $Q_3$ and $Q_1$
are similar to that of the median, $Q_2$. The IQR for the AM service times is $83.75 - 53.75 = 30$
seconds.


<!-- TODO: redo lecture 3 on R. -->

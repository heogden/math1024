\title{
4.2.2 Population and sample
}

Recall that a statistical model specifies a probability distribution for the random variables $\boldsymbol{X}$ corresponding to the data observations $\boldsymbol{x}$.

- The observations $\boldsymbol{x}=\left(x_{1}, \ldots, x_{n}\right)$ are called the sample, and quantities derived from the sample are sample quantities. For example, as in Chapter 1, we call

$$
\bar{x}=\frac{1}{n} \sum_{i=1}^{n} x_{i}
$$

the sample mean.

- The probability distribution for $\boldsymbol{X}$ specified in our model represents all possible observations which might have been observed in our sample, and is therefore sometimes referred to as the population. Quantities derived from this distribution are population quantities.

For example, if our model is that $X_{1}, \ldots, X_{n}$ are i.i.d., following the common distribution of a random variable $X$, then we call $E(X)$ the population mean.

\subsubsection{Statistic and estimator}

A statistic $T(\boldsymbol{x})$ is any function of the observed data $x_{1}, \ldots, x_{n}$ alone (and therefore does not depend on any parameters or other unknowns).

An estimate of $\theta$ is any statistic which is used to estimate $\theta$ under a particular statistical model. We will use $\tilde{\theta}(\boldsymbol{x})$ (sometimes shortened to $\tilde{\theta}$ ) to denote an estimate of $\theta$.

An estimate $\tilde{\theta}(\boldsymbol{x})$ is an observation of a corresponding random variable $\tilde{\theta}(\boldsymbol{X})$ which is called an estimator. Thus an estimate is a particular observed value, e.g. 1.2, but an estimator is a random variable which can take values which are called estimates.

An estimate is a particular numerical value, e.g. $\bar{x}$; an estimator is a random variable, e.g. $\bar{X}$.

The probability distribution of any estimator $\tilde{\theta}(\boldsymbol{X})$ is called its sampling distribution. The estimate $\tilde{\theta}(\boldsymbol{x})$ is an observed value (a number), and is a single observation from the sampling distribution of $\tilde{\theta}(\boldsymbol{X})$.

$\Upsilon$ Example 58 Suppose that we have a random sample $X_{1}, \ldots, X_{n}$ from the uniform distribution on the interval $[0, \theta]$ where $\theta>0$ is unknown. Suppose that $n=5$ and we have the sample observations $x_{1}=2.3, x_{2}=3.6, x_{3}=20.2, x_{4}=0.9, x_{5}=17.2$. Our objective is to estimate $\theta$. How can we proceed?

Here the pdf $f(x)=\frac{1}{\theta}$ for $0 \leq x \leq \theta$ and 0 otherwise. Hence $E(X)=\int_{0}^{\theta} \frac{1}{\theta} x d x=\frac{\theta}{2}$. There are many possible estimators for $\theta$, e.g. $\hat{\theta}_{1}(\boldsymbol{X})=2 \bar{X}$, which is motivated by the method of moments because $\theta=2 E(X)$. A second estimator is $\hat{\theta}_{2}(\boldsymbol{X})=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$, which is intuitive since $\theta$ must be greater than or equal to all observed values and thus the maximum of the sample value will be closest to $\theta$. This is also the maximum likelihood estimate of $\theta$, which you will learn in MATH3044.

How could we choose between the two estimators $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ ? This is where we need to learn the sampling distribution of an estimator to determine which estimator will be unbiased, i.e. correct on average, and which will have minimum variability. We will formally define these in a minute, but first let us derive the sampling distribution, i.e. the pdf, of $\hat{\theta}_{2}$. Note that $\hat{\theta}_{2}$ is a random variable since the sample $X_{1}, \ldots, X_{n}$ is random. We will first find its cdf and then differentiate the cdf to get the pdf. For ease of notation, suppose $Y=\hat{\theta}_{2}(\boldsymbol{X})=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$. For any $0<y<\theta$, the cdf of $Y, F(y)$ is given by:

$$
\begin{aligned}
P(Y \leq y) &=P\left(\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\} \leq y\right) \\
&\left.=P\left(X_{1} \leq y, X_{2} \leq y, \ldots, X_{n} \leq y\right)\right) \quad[\max \leq y \text { if and only if each } \leq y] \\
&=P\left(X_{1} \leq y\right) P\left(X_{2} \leq y\right) \cdots P\left(X_{n} \leq y\right) \quad\left[\text { since the } X^{\prime}\right. \text { 's are independent] }\\
&=\frac{y}{\theta} \frac{y}{\theta} \cdots \frac{y}{\theta} \\
&=\left(\frac{y}{\theta}\right)^{n} .
\end{aligned}
$$

Now the pdf of $Y$ is $f(y)=\frac{d F(y)}{d y}=n \frac{y^{n-1}}{\theta^{n}}$ for $0 \leq y \leq \theta$. We can plot this as a function of $y$ to see the pdf. Now $E\left(\hat{\theta}_{2}\right)=E(Y)=\frac{n}{n+1} \theta$ and $\operatorname{Var}\left(\hat{\theta}_{2}\right)=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}$. You can prove this by easy integration.

\subsubsection{Bias and mean square error}

In the uniform distribution example we saw that the estimator $\hat{\theta}_{2}=Y=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ is a random variable and its pdf is given by $f(y)=n \frac{y^{n-1}}{\theta^{n}}$ for $0 \leq y \leq \theta$. This probability distribution is called the sampling distribution of $\hat{\theta}_{2}$. For this we have seen that $E\left(\hat{\theta}_{2}\right)=\frac{n}{n+1} \theta$.

In general, we define the bias of an estimator $\tilde{\theta}(\boldsymbol{X})$ of $\theta$ to be

$$
\operatorname{bias}(\tilde{\theta})=E(\tilde{\theta})-\theta .
$$

An estimator $\tilde{\theta}(\boldsymbol{X})$ is said to be unbiased if

$$
\operatorname{bias}(\tilde{\theta})=0 \text {, i.e. if } E(\tilde{\theta})=\theta .
$$

So an estimator is unbiased if the expectation of its sampling distribution is equal to the quantity we are trying to estimate. Unbiased means "getting it right on average", i.e. under repeated sampling (relative frequency interpretation of probability).

Thus for the uniform distribution example, $\hat{\theta}_{2}$ is a biased estimator of $\theta$ and

$$
\operatorname{bias}\left(\hat{\theta}_{2}\right)=E\left(\hat{\theta}_{2}\right)-\theta=\frac{n}{n+1} \theta-\theta=-\frac{1}{n+1} \theta,
$$

which goes to zero as $n \rightarrow \infty$. However, $\hat{\theta}_{1}=2 \bar{X}$ is unbiased since $E\left(\hat{\theta}_{1}\right)=2 E(\bar{X})=2 \frac{\theta}{2}=\theta$. Unbiased estimators are "correct on average", but that does not mean that they are guaranteed to provide estimates which are close to the estimand $\theta$. A better measure of the quality of an estimator than bias is the mean squared error (or m.s.e.), defined as

$$
\operatorname{MSE}(\tilde{\theta})=E\left[(\tilde{\theta}-\theta)^{2}\right] \text {. }
$$

Therefore, if $\tilde{\theta}$ is unbiased for $\theta$, i.e. if $E(\tilde{\theta})=\theta$, then m.s.e. $(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})$. In general, we have the following result:

$$
\operatorname{MSE}(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2} \text {. }
$$

The proof is similar to the one we did in Lecture 2.

$$
\begin{aligned}
\operatorname{MSE}(\tilde{\theta}) &=E\left[(\tilde{\theta}-\theta)^{2}\right] \\
&=E\left[(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}\right] \\
&=E\left[(\tilde{\theta}-E(\tilde{\theta}))^{2}+(E(\tilde{\theta})-\theta)^{2}+2(\tilde{\theta}-E(\tilde{\theta}))(E(\tilde{\theta})-\theta)\right] \\
&=E[\tilde{\theta}-E(\tilde{\theta})]^{2}+E[E(\tilde{\theta})-\theta]^{2}+2 E[(\tilde{\theta}-E(\tilde{\theta}))(E(\tilde{\theta})-\theta)] \\
&=\operatorname{Var}(\tilde{\theta})+[E(\tilde{\theta})-\theta]^{2}+2(E(\tilde{\theta})-\theta) E[(\tilde{\theta}-E(\tilde{\theta}))] \\
&=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2}+2(E(\tilde{\theta})-\theta)[E(\tilde{\theta})-E(\tilde{\theta})] \\
&=\operatorname{Var}(\tilde{\theta})+\operatorname{bias}(\tilde{\theta})^{2} .
\end{aligned}
$$

Hence, the mean squared error incorporates both the bias and the variability (sampling variance) of $\tilde{\theta}$. We are then faced with the bias-variance trade-off when selecting an optimal estimator. We may allow the estimator to have a little bit of bias if we can ensure that the variance of the biased estimator will be much smaller than that of any unbiased estimator.

$\odot$ Example 59 Uniform distribution Continuing with the uniform distribution $U[0, \theta]$ example, we have seen that $\hat{\theta}_{1}=2 \bar{X}$ is unbiased for $\theta$ but $\operatorname{bias}\left(\hat{\theta}_{2}\right)=-\frac{1}{n+1} \theta$. How do these estimators compare with respect to the m.s.e? Since $\hat{\theta}_{1}$ is unbiased, its m.s.e is its variance. In the next lecture, we will prove that for random sampling from any population

$$
\operatorname{Var}(\bar{X})=\frac{\operatorname{Var}(X)}{n},
$$

where $\operatorname{Var}(X)$ is the variance of the population sampled from. Returning to our example, we know that if $X \sim U[0, \theta]$ then $\operatorname{Var}(X)=\frac{\theta^{2}}{12}$. Therefore we have:

$$
\operatorname{MSE}\left(\hat{\theta}_{1}\right)=\operatorname{Var}\left(\hat{\theta}_{1}\right)=\operatorname{Var}(2 \bar{X})=4 \operatorname{Var}(\bar{X})=4 \frac{\theta^{2}}{12 n}=\frac{\theta^{2}}{3 n} .
$$

Now, for $\hat{\theta}_{2}$ we know that:

1. $\operatorname{Var}\left(\hat{\theta}_{2}\right)=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}$; 2. $\operatorname{bias}\left(\hat{\theta}_{2}\right)=-\frac{1}{n+1} \theta$.

Now

$$
\begin{aligned}
\operatorname{MSE}\left(\hat{\theta}_{2}\right) &=\operatorname{Var}\left(\hat{\theta}_{2}\right)+\operatorname{bias}\left(\hat{\theta}_{2}\right)^{2} \\
&=\frac{n \theta^{2}}{(n+2)(n+1)^{2}}+\frac{\theta^{2}}{(n+1)^{2}} \\
&=\frac{\theta^{2}}{(n+1)^{2}}\left(\frac{n}{n+2}+1\right) \\
&=\frac{\theta^{2}}{(n+1)^{2}} \frac{2 n+2}{n+2} .
\end{aligned}
$$

Clearly, the m.s.e of $\hat{\theta}_{2}$ is an order of magnitude (of order $n^{2}$ rather than $n$ ) smaller than the m.s.e of $\hat{\theta}_{1}$, providing justification for the preference of $\hat{\theta}_{2}=\max \left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ as an estimator of $\theta$.

\subsubsection{Take home points}

In this lecture we have learned the basics of estimation. We have learned that estimates are particular values and estimators have probability distributions. We have also learned the concepts of the bias and variance of an estimator. We have proved a key fact that the mean squared error of an estimator is composed of two pieces, namely bias and variance. Sometimes there may be bias-variance trade-off where a little bias can lead to much lower variance. We have illustrated this with an example.

\subsection{Lecture 21: Estimation of mean and variance and standard error}

\subsubsection{Lecture mission}

Often, one of the main tasks of a statistician is to estimate a population average or mean. However the estimates, using whatever procedure, will not be usable or scientifically meaningful if we do not know their associated uncertainties. For example, a statement such as: "the Arctic ocean will be completely ice-free in the summer in the next few decades" provides little information as it does not communicate the extent or the nature of the uncertainty in it. Perhaps a more precise statement could be: "the Arctic ocean will be completely ice-free in the summer some time in the next 20-30 years". This last statement not only gives a numerical value for the number of years for complete ice-melt in the summer, but also acknowledges the uncertainty of $\pm 5$ years in the estimate. A statistician's main job is to estimate such uncertainties. In this lecture, we will get started with estimating uncertainties when we estimate a population mean. We will introduce the standard error of an estimator.

\subsubsection{Estimation of a population mean}

Suppose that $x_{1}, \ldots, x_{n}$ is a random sample from any probability distribution $f(x)$, which may be discrete or continuous. Suppose that we want to estimate the unknown population mean $E(X)=\mu$ and variance, $\operatorname{Var}(X)=\sigma^{2}$. In order to do this, it is not necessary to make any assumptions about $f(x)$, so this may be thought of as nonparametric inference.

We have the following results: R1 the sample mean

$$
\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}
$$

is an unbiased estimator of $\mu=E(X)$, i.e. $E(\bar{X})=\mu$,

$\mathbf{R 2} \operatorname{Var}(\bar{X})=\sigma^{2} / n$,

R3 the sample variance with divisor $n-1$

$$
S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}
$$

is an unbiased estimator of $\sigma^{2}$, i.e. $E\left(S^{2}\right)=\sigma^{2}$.

We prove $\mathbf{R} \mathbf{1}$ as follows.

$$
E[\bar{X}]=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} E(X)=E(X),
$$

so $\bar{X}$ is an unbiased estimator of $E(X)$.

We prove $\mathbf{R 2}$ using the result that for independent random variables the variance of the sum is the sum of the variances from Lecture 17 . Thus,

$$
\operatorname{Var}[\bar{X}]=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}(X)=\frac{n}{n^{2}} \operatorname{Var}(X)=\frac{\sigma^{2}}{n},
$$

so the MSE of $\bar{X}$ is $\operatorname{Var}(X) / n$. This proves the following assertion we made earlier:

Variance of the sample mean = Population Variance divided by the sample size.

We now want to prove $\mathbf{R 3}$, i.e. show that the sample variance with divisor $n-1$ is an unbiased estimator of the population variance $\sigma^{2}$, i.e. $E\left(S^{2}\right)=\sigma^{2}$. We have

$$
S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right] .
$$

To evaluate the expectation of the above, we need $E\left(X_{i}^{2}\right)$ and $E\left(\bar{X}^{2}\right)$. In general, we know for any random variable,

$$
\operatorname{Var}(Y)=E\left(Y^{2}\right)-(E(Y))^{2} \quad \Rightarrow E\left(Y^{2}\right)=\operatorname{Var}(Y)+(E(Y))^{2} .
$$

Thus, we have

$$
E\left(X_{i}^{2}\right)=\operatorname{Var}\left(X_{i}\right)+\left(E\left(X_{i}\right)\right)^{2}=\sigma^{2}+\mu^{2},
$$

and

$$
E\left(\bar{X}^{2}\right)=\operatorname{Var}(\bar{X})+(E(\bar{X}))^{2}=\sigma^{2} / n+\mu^{2},
$$

from $\mathrm{R} 1$ and $\mathrm{R} 2$. Now

$$
\begin{aligned}
E\left(S^{2}\right) &=E\left\{\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right]\right\} \\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n} E\left(X_{i}^{2}\right)-n E\left(\bar{X}^{2}\right)\right] \\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n}\left(\sigma^{2}+\mu^{2}\right)-n\left(\sigma^{2} / n+\mu^{2}\right)\right] \\
&\left.=\frac{1}{n-1}\left[n \sigma^{2}+n \mu^{2}-\sigma^{2}-n \mu^{2}\right)\right] \\
&=\sigma^{2} \equiv \operatorname{Var}(X) .
\end{aligned}
$$

In words, this proves that

$$
\text { The sample variance is an unbiased estimator of the population variance. }
$$

\subsubsection{Standard deviation and standard error}

It follows that, for an unbiased (or close to unbiased) estimator $\tilde{\theta}$,

$$
\operatorname{MSE}(\tilde{\theta})=\operatorname{Var}(\tilde{\theta})
$$

and therefore the sampling variance of the estimator is an important summary of its quality.

We usually prefer to focus on the standard deviation of the sampling distribution of $\tilde{\theta}$,

$$
\text { s.d. }(\tilde{\theta})=\sqrt{\operatorname{Var}(\tilde{\theta})} \text {. }
$$

In practice we will not know s.d. $(\tilde{\theta})$, as it will typically depend on unknown features of the distribution of $X_{1}, \ldots, X_{n}$. However, we may be able to estimate s.d. $(\tilde{\theta})$ using the observed sample $x_{1}, \ldots, x_{n}$. We define the standard error, s.e. $(\tilde{\theta})$, of an estimator $\tilde{\theta}$ to be an estimate of the standard deviation of its sampling distribution, s.d. $(\tilde{\theta})$.

Standard error of an estimator is an estimate of the standard deviation of its sampling distribution

We proved that

$$
\operatorname{Var}[\bar{X}]=\frac{\sigma^{2}}{n} \Rightarrow \text { s.d. }(\bar{X})=\frac{\sigma}{\sqrt{n}} .
$$

As $\sigma$ is unknown, we cannot calculate this standard deviation. However, we know that $E\left(S^{2}\right)=\sigma^{2}$, i.e. that the sample variance is an unbiased estimator of the population variance. Hence $S^{2} / n$ is an unbiased estimator for $\operatorname{Var}(\bar{X})$. Therefore we obtain the standard error of the mean, s.e. $(\bar{X})$, by plugging in the estimate

$$
s=\left(\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)^{1 / 2}
$$

of $\sigma$ into s.d. $(\bar{X})$ to obtain

$$
\text { s.e. }(\bar{X})=\frac{s}{\sqrt{n}} \text {. }
$$

Therefore, for the computer failure data, our estimate, $\bar{x}=3.75$, for the population mean is associated with a standard error

$$
\text { s.e. }(\bar{X})=\frac{3.381}{\sqrt{104}}=0.332 \text {. }
$$

Note that this is ' $a$ ' standard error, so other standard errors may be available. Indeed, for parametric inference, where we make assumptions about $f(x)$, alternative standard errors are available. For example, $X_{1}, \ldots, X_{n}$ are i.i.d. Poisson $(\lambda)$ random variables. $E(X)=\lambda$, so $\bar{X}$ is an unbiased estimator of $\lambda . \operatorname{Var}(X)=\lambda$, so another s.e. $(\bar{X})=\sqrt{\hat{\lambda} / n}=\sqrt{\bar{x} / n}$. In the computer failure data example, this is $\sqrt{\frac{3.75}{104}}=0.19$.

\subsubsection{Take home points}

In this lecture we have defined the standard error of an estimator. This is very important in practice, as the standard error tells us how precise our estimate is through how concentrated the sampling distribution of the estimator is. For example, in the age guessing example in $\mathrm{R}$ lab session 3 , a standard error of 15 years indicates hugely inaccurate guesses. We have learned three key results: the sample mean is an unbiased estimate of the population mean; the variance of the sample mean is the population variance divided by the sample size; and the sample variance with divisor $n-1$ is an unbiased estimator of the population variance.

\subsection{Lecture 22: Interval estimation}

\subsubsection{Lecture mission}

In any estimation problem it is very hard to guess the exact true value, but it is often much better (and easier?) to provide an interval where the true value is very likely to fall. For example, think of guessing the age of a stranger. In this lecture we will learn to use the results of the previous lecture to obtain confidence intervals for a mean parameter of interest. The methods are important to learn so that we can make probability statements about the random intervals as opposed to just pure guesses, e.g. estimating my age to be somewhere between 30 and 60 . Statistical methods allow us to be much more precise by harnessing the power of the data.

\subsubsection{Basics}

An estimate $\tilde{\theta}$ of a parameter $\theta$ is sometimes referred to as a point estimate. The usefulness of a point estimate is enhanced if some kind of measure of its precision can also be provided. Usually, for an unbiased estimator, this will be a standard error, an estimate of the standard deviation of the associated estimator, as we have discussed previously. An alternative summary of the information provided by the observed data about the location of a parameter $\theta$ and the associated precision is an interval estimate or confidence interval.

Suppose that $x_{1}, \ldots, x_{n}$ are observations of random variables $X_{1}, \ldots, X_{n}$ whose joint pdf is specified apart from a single parameter $\theta$. To construct a confidence interval for $\theta$, we need to find a random variable $T(\mathbf{X}, \theta)$ whose distribution does not depend on $\theta$ and is therefore known. This random variable $T(\mathbf{X}, \theta)$ is called a pivot for $\theta$. Hence we can find numbers $h_{1}$ and $h_{2}$ such that

$$
P\left(h_{1} \leq T(\mathbf{X}, \theta) \leq h_{2}\right)=1-\alpha
$$

where $1-\alpha$ is any specified probability. If (1) can be 'inverted' (or manipulated), we can write it as

$$
P\left[g_{1}(\mathbf{X}) \leq \theta \leq g_{2}(\mathbf{X})\right]=1-\alpha .
$$

Hence with probability $1-\alpha$, the parameter $\theta$ will lie between the random variables $g_{1}(\mathbf{X})$ and $g_{2}(\mathbf{X})$. Alternatively, the random interval $\left[g_{1}(\mathbf{X}), g_{2}(\mathbf{X})\right]$ includes $\theta$ with probability $1-\alpha$. Now, when we observe $x_{1}, \ldots, x_{n}$, we observe a single observation of the random interval $\left[g_{1}(\mathbf{X}), g_{2}(\mathbf{X})\right]$, which can be evaluated as $\left[g_{1}(\mathbf{x}), g_{2}(\mathbf{x})\right]$. We do not know if $\theta$ lies inside or outside this interval, but we do know that if we observed repeated samples, then $100(1-\alpha) \%$ of the resulting intervals would contain $\theta$. Hence, if $1-\alpha$ is high, we can be reasonably confident that our observed interval contains $\theta$. We call the observed interval $\left[g_{1}(\mathbf{x}), g_{2}(\mathbf{x})\right]$ a $100(1-\alpha) \%$ confidence interval for $\theta$. It is common to present intervals with high confidence levels, usually $90 \%, 95 \%$ or $99 \%$, so that $\alpha=0.1,0.05$ or $0.01$ respectively.

\subsubsection{Confidence interval for a normal mean}

Let $X_{1}, \ldots, X_{n}$ be i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables. We know that from CLT Lecture 14

$$
\bar{X} \sim N\left(\mu, \sigma^{2} / n\right) \quad \Rightarrow \quad \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \sim N(0,1) .
$$

Suppose we know that $\sigma=10$, so $\sqrt{n}(\bar{X}-\mu) / \sigma$ is a pivot for $\mu$. Then we can use the distribution function of the standard normal distribution to find values $h_{1}$ and $h_{2}$ such that

$$
P\left(h_{1} \leq \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \leq h_{2}\right)=1-\alpha
$$

for a chosen value of $1-\alpha$ which is called the confidence level. So $h_{1}$ and $h_{2}$ are chosen so that the shaded area in the figure is equal to the confidence level $1-\alpha$.

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-08.jpg?height=326&width=694&top_left_y=1993&top_left_x=681)

It is common practice to make the interval symmetric, so that the two unshaded areas are equal (to $\alpha / 2$ ), in which case

$$
-h_{1}=h_{2} \equiv h \quad \text { and } \quad \Phi(h)=1-\frac{\alpha}{2} .
$$

The most common choice of confidence level is $1-\alpha=0.95$, in which case $h=1.96=$ qnorm(0.975). You may also occasionally see $90 \%(h=1.645=$ qnorm(0.95) $)$ or $99 \%(h=$ $2.58=$ qnorm (0.995)) intervals. We discussed these values in Lecture 15. We generally use the $95 \%$ intervals for a reasonably high level of confidence without making the interval unnecessarily wide.

Therefore we have

$$
\begin{aligned}
& P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \leq 1.96\right)=0.95 \\
\Rightarrow & P\left(\bar{X}-1.96 \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}+1.96 \frac{\sigma}{\sqrt{n}}\right)=0.95 .
\end{aligned}
$$

Hence, $\bar{X}-1.96 \frac{\sigma}{\sqrt{n}}$ and $\bar{X}+1.96 \frac{\sigma}{\sqrt{n}}$ are the endpoints of a random interval which includes $\mu$ with probability $0.95$. The observed value of this interval, $\left(\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}\right)$, is called a 95\% confidence interval for $\mu$.

Q Example 60 For the fast food waiting time data, we have $n=20$ data points combined from the morning and afternoon data sets. We have $\bar{x}=67.85$ and $n=20$. Hence, under the normal model assuming (just for the sake of illustration) $\sigma=18$, a 95\% confidence interval for $\mu$ is

$$
\begin{gathered}
67.85-1.96(18 / \sqrt{20}) \leq \mu \leq 67.85+1.96(18 / \sqrt{20}) \\
\Rightarrow 59.96 \leq \mu \leq 75.74
\end{gathered}
$$

The $\mathrm{R}$ command is mean (a) $+\mathrm{c}(-1,1) *$ qnorm $(0.975) * 18 /$ sqrt (20), assuming a is the vector containing 20 waiting times. If $\sigma$ is unknown, we need to seek alternative methods for finding the confidence intervals.

Some important remarks about confidence intervals.

1. Notice that $\bar{x}$ is an unbiased estimate of $\mu, \sigma / \sqrt{n}$ is the standard error of the estimate and $1.96$ (in general $h$ in the above discussion) is a critical value from the associated known sampling distribution. The formula $(\bar{x} \pm 1.96 \sigma / \sqrt{n})$ for the confidence interval is then generalised as:

$$
\text { Estimate } \pm \text { Critical value } \times \text { Standard error }
$$

where the estimate is $\bar{x}$, the critical value is $1.96$ and the standard error is $\sigma / \sqrt{n}$. This is so much easier to remember. We will see that this formula holds in many of the following examples, but not all.

2. Confidence intervals are frequently used, but also frequently misinterpreted. A $100(1-\alpha) \%$ confidence interval for $\theta$ is a single observation of a random interval which, under repeated sampling, would include $\theta 100(1-\alpha) \%$ of the time.

The following example from the National Lottery in the UK clarifies the interpretation. We collected 6 chosen lottery numbers (sampled at random from 1 to 49) for 20 weeks and then constructed $95 \%$ confidence intervals for the population mean $\mu=25$ and plotted the intervals along with the observed sample means in the following figure. It can be seen that exactly one out of $20(5 \%)$ of the intervals do not contain the true population mean 25. Although this is a coincidence, it explains the main point that if we construct the random intervals with $100(1-\alpha) \%$ confidence levels again and again for hypothetical repetition of the data, on average $100(1-\alpha) \%$ of them will contain the true parameter.

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-10.jpg?height=751&width=1037&top_left_y=855&top_left_x=561)

3. A confidence interval is not a probability interval. You should avoid making statements like $P(1.3<\theta<2.2)=0.95$. In the classical approach to statistics you can only make probability statements about random variables, and $\theta$ is assumed to be a constant.

4. If a confidence interval is interpreted as a probability interval, this may lead to problems. For example, suppose that $X_{1}$ and $X_{2}$ are i.i.d. $U\left[\theta-\frac{1}{2}, \theta+\frac{1}{2}\right]$ random variables. Then $P\left[\min \left(X_{1}, X_{2}\right)<\theta<\max \left(X_{1}, X_{2}\right)\right]=\frac{1}{2}$ so $\left[\min \left(x_{1}, x_{2}\right), \max \left(x_{1}, x_{2}\right)\right]$ is a $50 \%$ confidence interval for $\theta$, where $x_{1}$ and $x_{2}$ are the observed values of $X_{1}$ and $X_{2}$. Now suppose that $x_{1}=0.3$ and $x_{2}=0.9$. What is $P(0.3<\theta<0.9)$ ?

\subsubsection{Take home points}

In this lecture we have learned to obtain confidence intervals by using an appropriate statistic in the pivoting technique. The main task is then to invert the inequality so that the unknown parameter is in the middle by itself and the two end points are functions of the sample observations. The most difficult task is to correctly interpret confidence intervals, which are not probability intervals but have long-run properties. That is, the interval will contain the true parameter with the stipulated confidence level only under infinitely repeated sampling. 

\subsection{Lecture 23: Confidence intervals using the CLT}

\subsubsection{Lecture mission}

Confidence intervals are generally difficult to find. The difficulty lies in finding a pivot, i.e. a statistic $T(\mathbf{X}, \theta)$ such that

$$
P\left(h_{1} \leq T(\mathbf{X}, \theta) \leq h_{2}\right)=1-\alpha
$$

for two suitable numbers $h_{1}$ and $h_{2}$, and also that the above can be inverted to put the unknown $\theta$ in the middle of the inequality inside the probability statement. One solution to this problem is to use the powerful Central Limit Theorem (CLT) to claim normality, and then basically follow the above normal example for known variance.

\subsubsection{Confidence intervals for $\mu$ using the CLT}

The CLT allows us to assume the large sample approximation

$$
\sqrt{n} \frac{(\bar{X}-\mu)}{\sigma} \stackrel{\text { approx }}{\sim} N(0,1) \text { as } n \rightarrow \infty .
$$

So a general confidence interval for $\mu$ can be constructed, just as before in Section 4.4.3. Thus a $95 \%$ confidence interval (CI) for $\mu$ is given by $\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}}$. But note that $\sigma$ is unknown so this CI cannot be used unless we can estimate $\sigma$, i.e. replace the unknown s.d. of $\bar{X}$ by its estimated standard error. In this case, we get the CI in the familiar form:

$$
\text { Estimate } \pm \text { Critical value } \times \text { Standard error }
$$

Suppose that we do not assume any distribution for the sampled random variable $X$ but assume only that $X_{1}, \ldots, X_{n}$ are i.i.d, following the distribution of $X$ where $E(X)=\mu$ and $\operatorname{Var}(X)=\sigma^{2}$. We know that the standard error of $\bar{X}$ is $s / \sqrt{n}$ where $s$ is the sample standard deviation with divisor $n-1$. Then the following provides a $95 \%$ CI for $\mu$ :

$$
\bar{x} \pm 1.96 \frac{s}{\sqrt{n}} \text {. }
$$

$\odot$ Example 61 For the computer failure data, $\bar{x}=3.75, s=3.381$ and $n=104$. Under the model that the data are observations of i.i.d. random variables with population mean $\mu$ (but no other assumptions about the underlying distribution), we compute a 95\% confidence interval for $\mu$ to be

$$
\left(3.75-1.96 \frac{3.381}{\sqrt{104}}, 3.75+1.96 \frac{3.381}{\sqrt{104}}\right)=(3.10,4.40) .
$$

If we can assume a distribution for $X$, i.e. a parametric model for $X$, then we can do slightly better in estimating the standard error of $\bar{X}$ and as a result we can improve upon the previously obtained $95 \%$ CI. Two examples follow.

Q Example 62 Poisson If $X_{1}, \ldots, X_{n}$ are modelled as i.i.d. Poisson $(\lambda)$ random variables, then $\mu=\lambda$ and $\sigma^{2}=\lambda$. We know $\operatorname{Var}(\bar{X})=\sigma^{2} / n=\lambda / n$. Hence a standard error is $\sqrt{\hat{\lambda} / n}=\sqrt{\bar{x} / n}$ since $\hat{\lambda}=\bar{X}$ is an unbiased estimator of $\lambda$. Thus a 95\% CI for $\mu=\lambda$ is given by

$$
\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}}{n}} .
$$

For the computer failure data, $\bar{x}=3.75, s=3.381$ and $n=104$. Under the model that the data are observations of i.i.d. random variables following a Poisson distribution with population mean $\lambda$, we compute a $95 \%$ confidence interval for $\lambda$ as

$$
\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}}{n}}=3.75 \pm 1.96 \sqrt{3.75 / 104}=(3.38,4.12) .
$$

We see that this interval is narrower $(0.74=4.12-3.38)$ than the earlier interval $(3.10,4.40)$, which has a length of $1.3$. We prefer narrower confidence intervals as they facilitate more accurate inference regarding the unknown parameter.

$\odot$ Example 63 Bernoulli If $X_{1}, \ldots, X_{n}$ are modelled as i.i.d. Bernoulli $(p)$ random variables, then $\mu=p$ and $\sigma^{2}=p(1-p)$. We know $\operatorname{Var}(\bar{X})=\sigma^{2} / n=p(1-p) / n$. Hence a standard error is $\sqrt{\hat{p}(1-\hat{p}) / n}=\sqrt{\bar{x}(1-\bar{x}) / n}$, since $\hat{p}=\bar{X}$ is an unbiased estimator of $p$. Thus a $95 \%$ CI for $\mu=p$ is given by

$$
\bar{x} \pm 1.96 \sqrt{\frac{\bar{x}(1-\bar{x})}{n}} .
$$

For the example, suppose $\bar{x}=0.2$ and $n=10$. Then we obtain the $95 \%$ CI as

$$
0.2 \pm 1.96 \sqrt{(0.2 \times 0.8) / 10}=(-0.048,0.448) .
$$

This is wrong as $n$ is too small for the large sample approximation to be accurate. Hence we need to look for other alternatives which may work better.

\subsubsection{Confidence interval for a Bernoulli $p$ by quadratic inversion}

It turns out that for the Bernoulli and Poisson distributions we can find alternative confidence intervals without using the approximation for standard error but still using the CLT. This is more complicated and requires us to solve a quadratic equation. We consider the two distributions separately.

We start with the CLT and obtain the following statement:

$$
\begin{array}{cc} 
& P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-p)}{\sqrt{p(1-p)}} \leq 1.96\right)=0.95 \\
\Leftrightarrow & P(-1.96 \sqrt{p(1-p)} \leq \sqrt{n}(\bar{X}-p) \leq 1.96 \sqrt{p(1-p)})=0.95 \\
\Leftrightarrow & P(-1.96 \sqrt{p(1-p) / n} \leq(\bar{X}-p) \leq 1.96 \sqrt{p(1-p) / n})=0.95 \\
\Leftrightarrow & P(p-1.96 \sqrt{p(1-p) / n} \leq \bar{X} \leq p+1.96 \sqrt{p(1-p) / n})=0.95 \\
\Leftrightarrow & P(L(p) \leq \bar{X} \leq R(p))=0.95,
\end{array}
$$

where $L(p)=p-h \sqrt{p(1-p) / n}, R(p)=p+h \sqrt{p(1-p) / n}, h=1.96$. Now, consider the inverse mappings $L^{-1}(x)$ and $R^{-1}(x)$ so that:

$$
\begin{aligned}
&P[L(p) \leq \bar{X} \leq R(p)]=0.95 \\
&\Leftrightarrow P\left[R^{-1}(\bar{X}) \leq p \leq L^{-1}(\bar{X})\right]=0.95
\end{aligned}
$$

which now defines our confidence interval $\left(R^{-1}(\bar{X}), L^{-1}(\bar{X})\right)$ for $p$. We can obtain $R^{-1}(\bar{x})$ and $L^{-1}(\bar{x})$ by solving the equations $R(p)=\bar{x}$ and $L(p)=\bar{x}$ for $p$, treating $n$ and $\bar{x}$ as known quantities. Thus we have,

$$
\begin{aligned}
& R(p)=\bar{x}, \quad L(p)=\bar{x} \\
& \Leftrightarrow \quad(\bar{x}-p)^{2}=h^{2} p(1-p) / n, \quad \text { where } h=1.96 \\
& \Leftrightarrow \quad p^{2}\left(1+h^{2} / n\right)-p\left(2 \bar{x}+h^{2} / n\right)+\bar{x}^{2}=0
\end{aligned}
$$

The endpoints of the confidence interval are the roots of the quadratic. Hence, the endpoints of the $95 \%$ confidence interval for $p$ are:

$$
\begin{aligned}
& \frac{\left(2 \bar{x}+\frac{h^{2}}{n}\right) \pm\left[\left(2 \bar{x}+\frac{h^{2}}{n}\right)^{2}-4 \bar{x}^{2}\left(1+\frac{h^{2}}{n}\right)\right]^{1 / 2}}{2\left(1+\frac{h^{2}}{n}\right)} \\
=& \frac{\left(\bar{x}+\frac{h^{2}}{2 n}\right) \pm\left[\left(\bar{x}+\frac{h^{2}}{2 n}\right)^{2}-\bar{x}^{2}\left(1+\frac{h^{2}}{n}\right)\right]^{1 / 2}}{\left(1+\frac{h^{2}}{n}\right)} \\
=& \frac{\bar{x}+\frac{h^{2}}{2 n} \pm \frac{h}{\sqrt{n}}\left[\frac{h^{2}}{4 n}+\bar{x}(1-\bar{x})\right]^{1 / 2}}{\left(1+\frac{h^{2}}{n}\right)} .
\end{aligned}
$$

This is sometimes called the Wilson Score Interval. The following R code calculates this for given $n, \bar{x}$ and confidence level $\alpha$ which determines the value of $h$. Returning to the previous example, $n=10$ and $\bar{x}=0.2$, the $95 \%$ CI obtained from this method is $(0.057,0.510)$ compared to the previous illegitimate one $(-0.048,0.448)$. In fact you can see that the intervals obtained by quadratic inversion are more symmetric and narrower as $n$ increases, and are also more symmetric for $\bar{x}$ closer to $0.5$. See the table below:

\begin{tabular}{cccccc}
\hline$n$ & $\bar{x}$ & \multicolumn{2}{c}{ Quadratic inversion } & \multicolumn{2}{c}{ Plug-in s.e. estimation } \\
& & Lower end & Upper end & Lower end & Upper end \\
\hline 10 & $0.2$ & $0.057$ & $0.510$ & $-0.048$ & $0.448$ \\
10 & $0.5$ & $0.237$ & $0.763$ & $0.190$ & $0.810$ \\
20 & $0.1$ & $0.028$ & $0.301$ & $-0.031$ & $0.231$ \\
20 & $0.2$ & $0.081$ & $0.416$ & $0.025$ & $0.375$ \\
20 & $0.5$ & $0.299$ & $0.701$ & $0.281$ & $0.719$ \\
50 & $0.1$ & $0.043$ & $0.214$ & $0.017$ & $0.183$ \\
50 & $0.2$ & $0.112$ & $0.330$ & $0.089$ & $0.311$ \\
50 & $0.5$ & $0.366$ & $0.634$ & $0.361$ & $0.639$ \\
\hline
\end{tabular}

For smaller $n$ and $\bar{x}$ closer to 0 (or 1 ), the approximation required for the plug-in estimate of the standard error is insufficiently reliable. However, for larger $n$ it is adequate.

\subsubsection{Confidence interval for a Poisson $\lambda$ by quadratic inversion}

Here we proceed as in the Bernoulli case and using the CLT claim that a 95\% CI for $\lambda$ is given by:

$$
P\left(-1.96 \leq \sqrt{n} \frac{(\bar{X}-\lambda)}{\sqrt{\lambda}} \leq 1.96\right)=0.95 \quad \Rightarrow P\left(n \frac{(\bar{X}-\lambda)^{2}}{\lambda} \leq 1.96^{2}\right)=0.95 .
$$

Now the confidence interval for $\lambda$ is found by solving the (quadratic) equality for $\lambda$ by treating $n, \bar{x}$ and $h$ to be known:

$$
\begin{aligned}
& n \frac{(\bar{x}-\lambda)^{2}}{\lambda}=h^{2}, \quad \text { where } h=1.96 \\
\Rightarrow & \bar{x}^{2}-2 \lambda \bar{x}+\lambda^{2}=h^{2} \lambda / n \\
\Rightarrow & \lambda^{2}-\lambda\left(2 \bar{x}+h^{2} / n\right)+\bar{x}^{2}=0 .
\end{aligned}
$$

Hence, the endpoints of the $95 \%$ confidence interval for $\lambda$ are:

$$
\frac{\left(2 \bar{x}+\frac{h^{2}}{n}\right) \pm\left[\left(2 \bar{x}+\frac{h^{2}}{n}\right)^{2}-4 \bar{x}^{2}\right]^{1 / 2}}{2}=\bar{x}+\frac{h^{2}}{2 n} \pm \frac{h}{n^{1 / 2}}\left[\frac{h^{2}}{4 n}+\bar{x}\right]^{1 / 2} .
$$

D Example 64 For the computer failure data, $\bar{x}=3.75$ and $n=104$. For a $95 \%$ confidence interval (CI), $h=1.96$. Hence, we calculate the above CI using the $\mathrm{R}$ commands:

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-14.jpg?height=209&width=1414&top_left_y=1823&top_left_x=283)

\subsubsection{Take home points}

In this lecture we learned how to find confidence intervals using the CLT, when the sample size is large. We have seen that we can make more accurate inferences if we can assume a model, e.g. the Poisson model for the computer failure data. However, we have also encountered problems when applying the method for a small sample size. In such cases we should use alternative methods for calculating confidence intervals. For example, we learned a technique of finding confidence intervals which does not require us to approximately estimate the standard errors for Bernoulli and Poisson distributions. In the next lecture, we will learn how to find an exact confidence interval for the normal mean $\mu$ using the t-distribution. 

\subsection{Lecture 24: Exact confidence interval for the normal mean}

\subsubsection{Lecture mission}

Recall that we can obtain better quality inferences if we can justify a precise model for the data. This saying is analogous to the claim that a person can better predict and infer in a situation when there are established rules and regulations, i.e. the analogue of a statistical model. In this lecture, we will discuss a procedure for finding confidence intervals based on the statistical modelling assumption that the data are from a normal distribution. This assumption will enable us to find an exact confidence interval for the mean rather than an approximate one using the central limit theorem.

\subsubsection{Obtaining an exact confidence interval for the normal mean}

For normal models we do not have to rely on large sample approximations, because it turns out that the distribution of

$$
T=\frac{\sqrt{n}(\bar{X}-\mu)}{S},
$$

where $S^{2}$ is the sample variance with divisor $n-1$, is standard (easily calculated) and thus the statistic $T=T(\mathbf{X}, \mu)$ can be an exact pivot for any sample size $n>1$. The point about easy calculation is that for any given $1-\alpha$, e.g. $1-\alpha=0.95$, we can calculate the critical value $h$ such that $P(-h<T<h)=1-\alpha$. Note also that the pivot $T$ does not involve the other unknown parameter of the normal model, namely the variance $\sigma^{2}$. If indeed, we can find $h$ for any given $1-\alpha$, we can proceed as follows to find the exact CI for $\mu$ :

$$
\begin{aligned}
& P(-h \leq T \leq h)=1-\alpha \\
\text { i.e. } & P\left(-h \leq \sqrt{n} \frac{(\bar{X}-\mu)}{S} \leq h\right)=0.95 \\
\Rightarrow & P\left(\bar{X}-h \frac{S}{\sqrt{n}} \leq \mu \leq \bar{X}+h \frac{S}{\sqrt{n}}\right)=0.95
\end{aligned}
$$

The observed value of this interval, $\left(\bar{x} \pm h \frac{s}{\sqrt{n}}\right)$, is the $95 \%$ confidence interval for $\mu$. Remarkably, this also of the general form, Estimate $\pm$ Critical value $\times$ Standard error, where the Critical value is $h$ and the standard error of the sample mean is $\frac{s}{\sqrt{n}}$. Now, how do we find the critical value $h$ for a given $1-\alpha$ ? We need to introduce the $t$-distribution.

Let $X_{1}, \ldots, X_{n}$ be i.i.d $N\left(\mu, \sigma^{2}\right)$ random variables. Define $\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$ and

$$
S^{2}=\frac{1}{n-1}\left(\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right) .
$$

Then, it can be shown (and will be in MATH2011) that

$$
\sqrt{n} \frac{(\bar{X}-\mu)}{S} \sim t_{n-1},
$$

where $t_{n-1}$ denotes the standard $t$ distribution with $n-1$ degrees of freedom. The standard $t$ distribution is a family of distributions which depend on one parameter called the degrees-of-freedom (df) which is $n-1$ here. The concept of degrees of freedom is that it is usually the number of independent random samples, $n$ here, minus the number of linear parameters estimated, 1 here for $\mu$. Hence the df is $n-1$.

The probability density function of the $t_{k}$ distribution is similar to a standard normal, in that it is symmetric around zero and 'bell-shaped', but the t-distribution is more heavy-tailed, giving greater probability to observations further away from zero. The figure below illustrates the $t_{k}$ density function for $k=1,2,5,20$ together with the standard normal pdf (solid line).

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-16.jpg?height=400&width=880&top_left_y=1048&top_left_x=588)

The values of $h$ for a given $1-\alpha$ have been tabulated using the standard $t$-distribution and can be obtained using the $\mathrm{R}$ command qt (abbreviation for quantile of $t$ ). For example, if we want to find $h$ for $1-\alpha=0.95$ and $n=20$ then we issue the command: qt $(0.975, \mathrm{df}=19)=2.093$. Note that it should be $0.975$ so that we are splitting $0.05$ probability between the two tails equally and the df should be $n-1=19$. Indeed, using the above command repeatedly, we obtain the following critical values for the $95 \%$ interval for different values of the sample size $n$.

\begin{tabular}{c|ccccccccc}
$n$ & 2 & 5 & 10 & 15 & 20 & 30 & 50 & 100 & $\infty$ \\
\hline$h$ & $12.71$ & $2.78$ & $2.26$ & $2.14$ & $2.09$ & $2.05$ & $2.01$ & $1.98$ & $1.96$
\end{tabular}

Note that the critical value approaches $1.96$ (which is the critical value for the normal distribution) as $n \rightarrow \infty$, since the $t$-distribution itself approaches the normal distribution for large values of its df parameter.

If you can justify that the underlying distribution is normal then you
can use the $t$-distribution-based confidence interval.

$\bigodot$ Example 65 Fast food waiting time revisited We would like to find a confidence interval for the true mean waiting time. If $X$ denotes the waiting time in seconds, we have $n=20, \bar{x}=67.85$, $s=18.36$. Hence, recalling that the critical value $h=2.093$, from the command qt $0.975$, df $=19$ ), a $95 \%$ confidence interval for $\mu$ is

$$
\begin{aligned}
67.85-2.093 \times 18.36 / \sqrt{20} \leq \mu & \leq 67.85+2.093 \times 18.36 / \sqrt{20} \\
\Rightarrow 59.26 & \leq \mu \leq 76.44 .
\end{aligned}
$$

In $\mathrm{R}$ we issue the commands: ffood <- read. csv ("servicetime. csv", head=T)

a <- c (ffood\$AM, ffood\$PM)

mean $(a)+c(-1,1) *$ qt $(0.975, d f=19) * \operatorname{sqrt}(\operatorname{var}(a)) / \operatorname{sqrt}(20)$

does the job and it gives the result $(59.25,76.45)$.

If we want a $90 \%$ confidence interval then we issue the command:

$\operatorname{mean}(a)+c(-1,1) * q t(0.95, d f=19) * \operatorname{sqrt}(\operatorname{var}(a)) / \operatorname{sqrt}(20)$,

which give $(60.75,74.95)$.

If we want a $99 \%$ confidence interval then we issue the command:

$\operatorname{mean}(a)+c(-1,1) * q t(0.995, d f=19) * \operatorname{sqrt}(\operatorname{var}(a)) / \operatorname{sqrt}(20)$,

which gives $(56.10,79.60)$. We can see clearly that the interval is getting wider as the level of confidence is getting higher.

O Example 66 Weight gain revisited We would like to find a confidence interval for the true average weight gain (final weight - initial weight). Here $n=68, \bar{x}=0.8672$ and $s=0.9653$. Hence, a $95 \%$ confidence interval for $\mu$ is

$$
\begin{aligned}
0.8672-1.996 \times 0.9653 / \sqrt{68} \leq \mu & \leq 0.8672+1.996 \times 0.9653 / \sqrt{68} \\
\Rightarrow 0.6335 & \leq \mu \leq 1.1008
\end{aligned}
$$

[In $\mathrm{R}$, we obtain the critical value $1.996$ by $\mathrm{qt}(0.975, \mathrm{df}=67)$ or $-\mathrm{qt}(0.025, \mathrm{df}=67)]$

In $R$ the command is: mean $(\mathrm{x})+\mathrm{c}(-1,1) * \mathrm{qt}(0.975, \mathrm{df}=67) * \operatorname{sqrt}(\operatorname{var}(\mathrm{x}) / 68)$ if the vector $\mathrm{x}$ contains the 68 weight gain differences. You may obtain this by issuing the commands:

wgain <- read.table ("wtgain.txt", head=T)

$x<-$ wgain\$final -wgain\$initial

Note that the interval here does not include the value 0 , so it is very likely that the weight gain is significantly positive, which we will justfy using what is called testing of hypothesis.

\subsubsection{Take home points}

In this lecture we have learned how to find an exact confidence interval for a population mean based on the assumption that the population is normal. The confidence interval is based on the $t$-distribution which is a very important distribution in statistics. The $t$-distribution converges to the normal distribution when its only parameter, called the degrees of freedom, becomes very large. If the assumption of the normal distribution for the data can be justified, then the method of inference based on the $t$-distribution is best when the variance parameter, sometimes called the nuisance parameter, is unknown.

\subsection{Lecture 25: Hypothesis testing I}

\subsubsection{Lecture mission}

The manager of a new fast food chain claims that the average waiting time to be served in their restaurant is less than a minute. The marketing department of a mobile phone company claims that their phones never break down in the first three years of their lifetime. A professor of nutrition claims that students gain significant weight in the first year of their life in college away form home. How can we verify these claims? We will learn the procedures of hypothesis testing for such problems.

\subsubsection{Introduction}

In statistical inference, we use observations $x_{1}, \ldots, x_{n}$ of univariate random variables $X_{1}, \ldots, X_{n}$ in order to draw inferences about the probability distribution $f(x)$ of the underlying random variable $X$. So far, we have mainly been concerned with estimating features (usually unknown parameters) of $f(x)$. It is often of interest to compare alternative specifications for $f(x)$. If we have a set of competing probability models which might have generated the observed data, we may want to determine which of the models is most appropriate. A proposed (hypothesised) model for $X_{1}, \ldots, X_{n}$ is then referred to as a hypothesis, and pairs of models are compared using hypothesis tests.

For example, we may have two competing alternatives, $f^{(0)}(x)$ (model $\left.\mathrm{H}_{0}\right)$ and $f^{(1)}(x)$ (model $\mathrm{H}_{1}$ ) for $f(x)$, both of which completely specify the joint distribution of the sample $X_{1}, \ldots, X_{n}$. Completely specified statistical models are called simple hypotheses. Usually, $\mathrm{H}_{0}$ and $\mathrm{H}_{1}$ both take the same parametric form $f(x, \theta)$, but with different values $\theta^{(0)}$ and $\theta^{(1)}$ of $\theta$. Thus the joint distribution of the sample given by $f(\mathbf{X})$ is completely specified apart from the values of the unknown parameter $\theta$ and $\theta^{(0)} \neq \theta^{(1)}$ are specified alternative values.

More generally, competing hypotheses often do not completely specify the joint distribution of $X_{1}, \ldots, X_{n}$. For example, a hypothesis may state that $X_{1}, \ldots, X_{n}$ is a random sample from the probability distribution $f(x ; \theta)$ where $\theta<0$. This is not a completely specified hypothesis, since it is not possible to calculate probabilities such as $P\left(X_{1}<2\right)$ when the hypothesis is true, as we do not know the exact value of $\theta$. Such an hypothesis is called a composite hypothesis.

Examples of hypotheses:

$X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)$ with $\mu=0, \sigma^{2}=2$.

$X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)$ with $\mu=0, \sigma^{2} \in \mathcal{R}_{+}$.

$X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)$ with $\mu \neq 0, \sigma^{2} \in \mathcal{R}_{+}$.

$X_{1}, \ldots, X_{n} \sim \operatorname{Bernoulli}(p)$ with $p=\frac{1}{2}$.

$X_{1}, \ldots, X_{n} \sim \operatorname{Bernoulli}(p)$ with $p \neq \frac{1}{2}$.

$X_{1}, \ldots, X_{n} \sim \operatorname{Bernoulli}(p)$ with $p>\frac{1}{2}$.

$X_{1}, \ldots, X_{n} \sim \operatorname{Poisson}(\lambda)$ with $\lambda=1$.

$X_{1}, \ldots, X_{n} \sim \operatorname{Poisson}(\theta)$ with $\theta>1$

\subsubsection{Hypothesis testing procedure}

A hypothesis test provides a mechanism for comparing two competing statistical models, $\mathrm{H}_{0}$ and $\mathrm{H}_{1}$. A hypothesis test does not treat the two hypotheses (models) symmetrically. One hypothesis, $\mathrm{H}_{0}$, is given special status, and referred to as the null hypothesis. The null hypothesis is the reference model, and is assumed to be appropriate unless the observed data strongly indicate that $\mathrm{H}_{0}$ is inappropriate, and that $\mathrm{H}_{1}$ (the alternative hypothesis) should be preferred. Hence, the fact that a hypothesis test does not reject $\mathrm{H}_{0}$ should not be taken as evidence that $\mathrm{H}_{0}$ is true and $\mathrm{H}_{1}$ is not, or that $\mathrm{H}_{0}$ is better-supported by the data than $\mathrm{H}_{1}$, merely that the data does not provide significant evidence to reject $\mathrm{H}_{0}$ in favour of $\mathrm{H}_{1}$.

A hypothesis test is defined by its critical region or rejection region, which we shall denote by $C$. $C$ is a subset of $\mathcal{R}^{n}$ and is the set of possible observed values of $\mathbf{X}$ which, if observed, would lead to rejection of $\mathrm{H}_{0}$ in favour of $\mathrm{H}_{1}$, i.e.

$$
\begin{array}{ll}
\text { If } \mathbf{x} \in C & \mathrm{H}_{0} \text { is rejected in favour of } \mathrm{H}_{1} \\
\text { If } \mathbf{x} \notin C & \mathrm{H}_{0} \text { is not rejected }
\end{array}
$$

As $\mathbf{X}$ is a random variable, there remains the possibility that a hypothesis test will give an erroneous result. We define two types of error:

\begin{tabular}{|l|}
\hline Type I error: $\mathrm{H}_{0}$ is rejected when it is true \\
Type II error: $\mathrm{H}_{0}$ is not rejected when it is false \\
\hline
\end{tabular}

The following table helps to understand further:

\begin{tabular}{|l|l|l|}
\hline & $H_{0}$ true & $H_{0}$ false \\
\hline Reject $H_{0}$ & Type I error & Correct decision \\
\hline Do not reject $H_{0}$ & Correct decision & Type II error \\
\hline
\end{tabular}

When $\mathrm{H}_{0}$ and $\mathrm{H}_{1}$ are simple hypotheses, we can define

$$
\begin{aligned}
& \alpha=P(\text { Type I error })=P(\mathbf{X} \in C) \quad \text { if } \mathrm{H}_{0} \text { is true } \\
& \beta=P(\text { Type II error })=P(\mathbf{X} \notin C) \quad \text { if } \mathrm{H}_{1} \text { is true }
\end{aligned}
$$

$\odot$ Example 67 Uniform Suppose that we have one observation from the uniform distribution on the range $(0, \theta)$. In this case, $f(x)=1 / \theta$ if $0<x<\theta$ and $P(X \leq x)=\frac{x}{\theta}$ for $0<x<\theta$. We want to test $\mathrm{H}_{0}: \theta=1$ against the alternative $\mathrm{H}_{1}: \theta=2$. Suppose we decide arbitrarily that we will reject $H_{0}$ if $X>0.75$. Then

$$
\begin{aligned}
& \alpha=P \text { (Type I error) }=P(X>0.75) \text { if } \mathrm{H}_{0} \text { is true } \\
& \beta=P(\text { Type II error })=P(X<0.75) \text { if } \mathrm{H}_{1} \text { is true }
\end{aligned}
$$

which will imply:

$$
\begin{array}{r}
\alpha=P(X>0.75 \mid \theta=1)=1-0.75=\frac{1}{4}, \\
\beta=P(X<0.75 \mid \theta=2)=0.75 / 2=\frac{3}{8} .
\end{array}
$$

Here the notation $\mid$ means given that.

D Example 68 Poisson The daily demand for a product has a Poisson distribution with mean $\lambda$, the demands on different days being statistically independent. It is desired to test the hypotheses $H_{0}: \lambda=0.7, H_{1}: \lambda=0.3$. The null hypothesis is to be accepted if in 20 days the number of days with no demand is less than 15 . Calculate the Type I and Type II error probabilities.

Let $p$ denote the probability that the demand on a given day is zero. Then

$$
p=e^{-\lambda}= \begin{cases}e^{-0.7} & \text { under } H_{0} \\ e^{-0.3} & \text { under } H_{1}\end{cases}
$$

If $X$ denotes the number of days out of 20 with zero demand, it follows that

$$
\begin{aligned}
&X \sim B\left(20, e^{-0.7}\right) \text { under } H_{0}, \\
&X \sim B\left(20, e^{-0.3}\right) \text { under } H_{1} .
\end{aligned}
$$

Thus

$$
\begin{aligned}
\alpha &=P\left(\text { Reject } H_{0} \mid H_{0} \text { true }\right) \\
&=P\left(X \geq 15 \mid X \sim B\left(20, e^{-0.7}\right)\right) \\
&=1-P(X \leq 14 \mid X \sim B(20,0.4966)) \\
&=1-0.98028 \\
&=0.01923 \text { (1-pbinom (14, size=20, prob=0.4966) in } \mathrm{R}) .
\end{aligned}
$$

Furthermore

$$
\begin{aligned}
\beta &=P\left(\text { Accept } H_{0} \mid H_{1} \text { true }\right) \\
&=P\left(X \leq 14 \mid X \sim B\left(20, e^{-0.3}\right)\right) \\
&=P(X \leq 14 \mid X \sim B(20,0.7408)) \\
&=P(Y \geq 6 \mid Y \sim B(20,0.2592)) \\
&=1-P(Y \leq 5 \mid Y \sim B(20,0.2592)) \\
&=1-0.58022 \\
&=0.42023(1 \text {-pbinom }(5, \text { size }=20, \text { prob=0. 2592) in } \mathrm{R}) .
\end{aligned}
$$

Sometimes $\alpha$ is called the size (or significance level) of the test and $\omega \equiv 1-\beta$ is called the power of the test. Ideally, we would like to avoid error so we would like to make both $\alpha$ and $\beta$ as small as possible. In other words, a good test will have small size, but large power. However, it is not possible to make $\alpha$ and $\beta$ both arbitrarily small. For example if $C=\emptyset$ then $\alpha=0$, but $\beta=1$. On the other hand if $C=\mathbf{S}=\mathcal{R}^{n}$ then $\beta=0$, but $\alpha=1$.

The general hypothesis testing procedure is to fix $\alpha$ to be some small value (often 0.05), so that the probability of a Type I error is limited. In doing this, we are giving $\mathrm{H}_{0}$ precedence over $\mathrm{H}_{1}$, and acknowledging that Type I error is potentially more serious than Type II error. (Note that for discrete random variables, it may be difficult to find $C$ so that the test has exactly the required size). Given our specified $\alpha$, we try to choose a test, defined by its rejection region $C$, to make $\beta$ as small as possible, i.e. we try to find the most powerful test of a specified size. Where $\mathrm{H}_{0}$ and $\mathrm{H}_{1}$ are simple hypotheses this can be achieved easily.

Note that tests are usually based on a one-dimensional test statistic $T(\mathbf{X})$ whose sample space is some subset of $\mathcal{R}$. The rejection region is then a set of possible values for $T(\mathbf{X})$, so we also think of $C$ as a subset of $\mathcal{R}$. In order to be able to ensure the test has size $\alpha$, the distribution of the test statistic under $\mathrm{H}_{0}$ should be known.

\subsubsection{The test statistic}

We perform a hypothesis test by computing a test statistic, $T(\boldsymbol{X})$. A test statistic must (obviously) be a statistic (i.e. a function of $\boldsymbol{X}$ and other known quantities only). Furthermore, the random variable $T(\boldsymbol{X})$ must have a distribution which is known under the null hypothesis. The easiest way to construct a test statistic is to obtain a pivot for $\theta$. If $T(\boldsymbol{X}, \theta)$ is a pivot for $\theta$ then its sampling distribution is known and, therefore, under the null hypothesis $\left(\theta=\theta_{0}\right)$ the sampling distribution of $T\left(\boldsymbol{X}, \theta_{0}\right)$ is known. Hence $T\left(\boldsymbol{x}, \theta_{0}\right)$ is a test statistic, as it depends on observed data $\boldsymbol{x}$ and the hypothesised value $\theta_{0}$ only. We then assess the plausibility of $\mathrm{H}_{0}$ by evaluating whether $T\left(\boldsymbol{x}, \theta_{0}\right)$ seems like a reasonable observation from its (known) distribution. This is all rather abstract. How does it work in a concrete example?

\subsubsection{Testing a normal mean $\mu$}

Suppose that we observe data $x_{1}, \ldots, x_{n}$ which are modelled as observations of i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables $X_{1}, \ldots, X_{n}$, and we want to test the null hypothesis

$$
\mathrm{H}_{0}: \mu=\mu_{0}
$$

against the alternative hypothesis

$$
\mathrm{H}_{1}: \mu \neq \mu_{0} .
$$

We recall that

$$
\sqrt{n} \frac{(\bar{X}-\mu)}{S} \sim t_{n-1}
$$

and therefore, when $\mathrm{H}_{0}$ is true, often written as under $\mathrm{H}_{0}$,

$$
\sqrt{n} \frac{\left(\bar{X}-\mu_{0}\right)}{S} \sim t_{n-1}
$$

so $\sqrt{n}\left(\bar{X}-\mu_{0}\right) / s$ is a test statistic for this test. The sampling distribution of the test statistic when the null hypothesis is true is called the null distribution of the test statistic. In this example, the null distribution is the t-distribution with $n-1$ degrees of freedom.

This test is called a $t$-test. We reject the null hypothesis $\mathrm{H}_{0}$ in favour of the alternative $\mathrm{H}_{1}$ if the observed test statistic seems unlikely to have been generated by the null distribution.

\section{$\bigcirc$ Example 69 Weight gain data}

For the weight gain data, if $x$ denotes the differences in weight gain, we have $\bar{x}=0.8672$, $s=0.9653$ and $n=68$. Hence our test statistic for the null hypothesis $\mathrm{H}_{0}: \mu=\mu_{0}=0$ is

$$
\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}=7.41 \text {. }
$$

The observed value of $7.41$ does not seem reasonable from the graph below. The graph has plotted the density of the $t$-distribution with 67 degrees of freedom, and a vertical line is drawn at the observed value of $7.41$. So there may be evidence here to reject $\mathrm{H}_{0}: \mu=0$.

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-22.jpg?height=351&width=414&top_left_y=761&top_left_x=821)

$\odot$ Example 70 Fast food waiting time revisited Suppose the manager of the fast food outlet claims that the average waiting time is only 60 seconds. So, we want to test $H_{0}: \mu=60$. We have $n=20, \bar{x}=67.85, s=18.36$. Hence our test statistic for the null hypothesis $\mathrm{H}_{0}: \mu=\mu_{0}=60$ is

$$
\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}=\sqrt{20} \frac{(67.85-60)}{18.36}=1.91 .
$$

The observed value of $1.91$ may or may not be reasonable from the graph below. The graph has plotted the density of the t-distribution with 19 degrees of freedom and a vertical line is drawn at the observed value of $1.91$. This value is a bit out in the tail but we are not sure, unlike in the previous weight gain example. So how can we decide whether to reject the null hypothesis?

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-22.jpg?height=351&width=420&top_left_y=1789&top_left_x=818)

\subsubsection{Take home points}

In this lecture we have learned the concepts of hypothesis testing such as: simple and composite hypotheses, null and alternative hypotheses, type I error and type II error and their probabilities, test statistic and critical region. We have also introduced the t-test statistic for testing hypotheses regarding a normal mean. We have not yet learned when to reject a null hypothesis, which will be discussed in the next lecture. 

\subsection{Lecture 26: Hypothesis testing II}

\subsubsection{Lecture mission}

This lecture will discuss the rejection region of a hypothesis test with an example. We will learn the key concepts of the level of significance, rejection region and the p-value associated with a hypothesis test. We will also learn the equivalence of testing and interval estimation.

\subsubsection{The significance level}

In the weight gain example, it seems clear that there is no evidence to reject $\mathrm{H}_{0}$, but how extreme (far from the mean of the null distribution) should the test statistic be in order for $\mathrm{H}_{0}$ to be rejected? The significance level of the test, $\alpha$, is the probability that we will erroneously reject $\mathrm{H}_{0}$ (called Type I error as discussed before). Clearly we would like $\alpha$ to be small, but making it too small risks failing to reject $\mathrm{H}_{0}$ even when it provides a poor model for the observed data (Type II error). Conventionally, $\alpha$ is usually set to a value of $0.05$, or $5 \%$. Therefore we reject $\mathrm{H}_{0}$ when the test statistic lies in a rejection region which has probability $\alpha=0.05$ under the null distribution.

\subsubsection{Rejection region for the t-test}

For the t-test, the null distribution is $t_{n-1}$ where $n$ is the sample size, so the rejection region for the test corresponds to a region of total probability $\alpha=0.05$ comprising the 'most extreme' values in the direction of the alternative hypothesis. If the alternative hypothesis is two-sided, e.g. $H_{1}: \mu \neq \mu_{0}$, then this is obtained as below, where the two shaded regions both have area (probability) $\alpha / 2=0.025$.

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-23.jpg?height=331&width=991&top_left_y=1676&top_left_x=538)

The value of $h$ depends on the sample size $n$ and can be found by issuing the qt command. Here are few examples obtained from qt $(0.975, d f=c(1,4,9,14,19,29,49,99)$ ):

\begin{tabular}{cccccccccc}
$n$ & 2 & 5 & 10 & 15 & 20 & 30 & 50 & 100 & $\infty$ \\
\hline$h$ & $12.71$ & $2.78$ & $2.26$ & $2.14$ & $2.09$ & $2.05$ & $2.01$ & $1.98$ & $1.96$
\end{tabular}

Note that we need to put $n-1$ in the df argument of qt and the last value for $n=\infty$ is obtained from the normal distribution.

However, if the alternative hypothesis is one-sided, e.g. $H_{1}: \mu>\mu_{0}$, then the critical region will only be in the right tail. Consequently, we need to leave an area $\alpha$ on the right and as a result the critical values will be from a command such as:

qt $(0.95, d f=c(1,4,9,14,19,29,49,99))$ 

\begin{tabular}{cccccccccc}
$n$ & 2 & 5 & 10 & 15 & 20 & 30 & 50 & 100 & $\infty$ \\
\hline$h$ & $6.31$ & $2.13$ & $1.83$ & $1.76$ & $1.73$ & $1.70$ & $1.68$ & $1.66$ & $1.64$
\end{tabular}

\subsection{4 t-test summary}

Suppose that we observe data $x_{1}, \ldots, x_{n}$ which are modelled as observations of i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables $X_{1}, \ldots, X_{n}$ and we want to test the null hypothesis $\mathrm{H}_{0}: \mu=\mu_{0}$ against the alternative hypothesis $\mathrm{H}_{1}: \mu \neq \mu_{0}$ :

1. Compute the test statistic

$$
t=\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s} .
$$

2. For chosen significance level $\alpha$ (usually 0.05) calculate the rejection region for $t$, which is of the form $|t|>h$ where $-h$ is the $\alpha / 2$ percentile of the null distribution, $t_{n-1}$.

3. If your computed $t$ lies in the rejection region, i.e. $|t|>h$, you report that $\mathrm{H}_{0}$ is rejected in favour of $\mathrm{H}_{1}$ at the chosen level of significance. If $t$ does not lie in the rejection region, you report that $\mathrm{H}_{0}$ is not rejected. [Never refer to 'accepting' a hypothesis.]

$\odot$ Example 71 Fast food waiting time We would like to test $H_{0}: \mu=60$ against the alternative $H_{1}: \mu>60$, as this alternative will refute the claim of the store manager that customers only wait for a maximum of one minute. We calculated the observed value to be $1.91$. This is a one-sided test and for a $5 \%$ level of significance, the critical value $h$ will come from qt $(0.95, \mathrm{df}=19)=1.73$. Thus the observed value is higher than the critical value so we will reject the null hypothesis, disputing the manager's claim regarding a minute wait.

\section{$\curvearrowright$ Example 72 Weight gain}

For the weight gain example $\bar{x}=0.8671, s=0.9653, n=68$. Then, we would be interested in testing $\mathrm{H}_{0}: \mu=0$ against the alternative hypothesis $\mathrm{H}_{1}: \mu \neq 0$ in the model that the data are observations of i.i.d. $N\left(\mu, \sigma^{2}\right)$ random variables.

- We obtain the test statistic

$$
t=\sqrt{n} \frac{\left(\bar{x}-\mu_{0}\right)}{s}=\sqrt{68} \frac{(0.8671-0)}{0.9653}=7.41 .
$$

- Under $\mathrm{H}_{0}$ this is an observation from a $t_{67}$ distribution. For significance level $\alpha=0.05$ the rejection region is $|t|>1.996$.

- Our computed test statistic lies in the rejection region, i.e. $|t|>1.996$, so $\mathrm{H}_{0}$ is rejected in favour of $\mathrm{H}_{1}$ at the $5 \%$ level of significance.

In $\mathrm{R}$ we can perform the test as follows:

wgain <- read.table("wtgain.txt", head=T)

$x<-$ wgain\$final - wgain\$initial

t.test $(x)$

This gives the results: $\mathrm{t}=7.4074$, and $\mathrm{df}=67$. 

\subsection{5 p-values}

The result of a test is most commonly summarised by rejection or non-rejection of $\mathrm{H}_{0}$ at the stated level of significance. An alternative, which you may see in practice, is the computation of a p-value. This is the probability that the reference distribution would have generated the actual observed value of the statistic or something more extreme. A small p-value is evidence against the null hypothesis, as it indicates that the observed data were unlikely to have been generated by the reference distribution. In many examples a threshold of $0.05$ is used, below which the null hypothesis is rejected as being insufficiently well-supported by the observed data. Hence for the t-test with a two-sided alternative, the p-value is given by:

$$
p=P\left(|T|>\left|t_{\text {obs }}\right|\right)=2 P\left(T>\left|t_{\text {obs }}\right|\right),
$$

where $T$ has a $t_{n-1}$ distribution and $t_{\text {obs }}$ is the observed sample value.

However, if the alternative is one-sided and to the right then the p-value is given by:

$$
p=P\left(T>t_{\mathrm{obs}}\right),
$$

where $T$ has a $t_{n-1}$ distribution and $t_{\text {obs }}$ is the observed sample value.

A small p-value corresponds to an observation of $T$ that is improbable (since it is far out in the low probability tail area) under $\mathrm{H}_{0}$ and hence provides evidence against $\mathrm{H}_{0}$. The p-value should not be misinterpreted as the probability that $\mathrm{H}_{0}$ is true. $\mathrm{H}_{0}$ is not a random event (under our models) and so cannot be assigned a probability. The null hypothesis is rejected at significance level $\alpha$ if the p-value for the test is less than $\alpha$.

$$
\text { Reject } \mathrm{H}_{0} \text { if p-value }<\alpha \text {. }
$$

\subsection{6 p-value examples}

In the fast food example, a test of $\mathrm{H}_{0}: \mu=60$ resulted in a test statistic $t=1.91$. Then the p-value is given by:

$$
p=P(T>1.91)=0.036 \text {, when } T \sim t_{19} .
$$

This is the area of the shaded region in the figure overleaf. In $R$ it is: $1-p t(1.91, d f=19)$. The p-value $0.036$ indicates some evidence against the manager's claim at the $5 \%$ level of significance but not the $1 \%$ level of significance. In the graph, what would be the area under the curve to the right of of the red line?

![](https://cdn.mathpix.com/cropped/2022_11_09_459f18f8fbd5ea1f3005g-25.jpg?height=362&width=408&top_left_y=2252&top_left_x=824)

When the alternative hypothesis is two-sided the p-value has to be calculated from $P\left(|T|>t_{\text {obs }}\right)$, where $t_{\text {obs }}$ is the observed value and $T$ follows the $t$-distribution with $n-1$ df. For the weight gain example, because the alternative is two-sided, the p-value is given by:

$$
p=P(|T|>7.41)=2.78 \times 10^{-10} \approx 0.0, \text { when } T \sim t_{67} .
$$

This very small p-value for the second example indicates very strong evidence against the null hypothesis of no weight gain in the first year of university.

\subsubsection{Equivalence of testing and interval estimation}

Note that the $95 \%$ confidence interval for $\mu$ in the weight gain example has previously been calculated to be $(0.6335,1.1008)$ in Section 4.6.2. This interval does not include the hypothesised value 0 of $\mu$. Hence we can conclude that the hypothesis test at the $5 \%$ level of significance will reject the null hypothesis $H_{0}: \mu=0$. This is because $\left|T_{o b s}=\frac{\sqrt{n}\left(\bar{x}-\mu_{0}\right)}{s}\right|>h$ implies and is implied by $\mu_{0}$ being outside the interval $(\bar{x}-h s / \sqrt{n}, \bar{x}+h s / \sqrt{n})$. Notice that $h$ is the same in both. For this reason we often just calculate the confidence interval and take the reject/do not reject decision merely by inspection.

\subsubsection{Take home points}

This lecture has introduced the key concepts for hypothesis testing. We have defined the p-value of a test and learned that we reject the null hypothesis if the p-value of the test is less than a given level of significance. We have also learned that hypothesis testing and interval estimation are equivalent concepts.

\subsection{Lecture 27: Two sample t-tests}

\subsubsection{Lecture mission}

Suppose that we observe two samples of data, $x_{1}, \ldots, x_{n}$ and $y_{1}, \ldots, y_{m}$, and that we propose to model them as observations of

$$
X_{1}, \ldots, X_{n} \stackrel{i . i . d .}{\sim} N\left(\mu_{X}, \sigma_{X}^{2}\right)
$$

and

$$
Y_{1}, \ldots, Y_{m} \stackrel{\text { i.i.d. }}{\sim} N\left(\mu_{Y}, \sigma_{Y}^{2}\right)
$$

respectively, where it is also assumed that the $X$ and $Y$ variables are independent of each other. Suppose that we want to test the hypothesis that the distributions of $X$ and $Y$ are identical, that is

$$
\mathrm{H}_{0}: \mu_{X}=\mu_{Y}, \quad \sigma_{X}=\sigma_{Y}=\sigma
$$

against the alternative hypothesis

$$
\mathrm{H}_{1}: \mu_{X} \neq \mu_{Y} .
$$



\subsubsection{Two sample t-test statistic}

In the probability lectures we proved that

$$
\bar{X} \sim N\left(\mu_{X}, \sigma_{X}^{2} / n\right) \text { and } \bar{Y} \sim N\left(\mu_{Y}, \sigma_{Y}^{2} / m\right)
$$

and therefore

$$
\bar{X}-\bar{Y} \sim N\left(\mu_{X}-\mu_{Y}, \frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}\right) .
$$

Hence, under $\mathrm{H}_{0}$,

$$
\bar{X}-\bar{Y} \sim N\left(0, \sigma^{2}\left[\frac{1}{n}+\frac{1}{m}\right]\right) \Rightarrow \sqrt{\frac{n m}{n+m}} \frac{(\bar{X}-\bar{Y})}{\sigma} \sim N(0,1) .
$$

The involvement of the (unknown) $\sigma$ above means that this is not a pivotal test statistic. It will be proved in MATH2011 that if $\sigma$ is replaced by its unbiased estimator $S$, which here is the two-sample estimator of the common standard deviation, given by

$$
S^{2}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{i=1}^{m}\left(Y_{i}-\bar{Y}\right)^{2}}{n+m-2},
$$

then

$$
\sqrt{\frac{n m}{n+m}} \frac{(\bar{X}-\bar{Y})}{S} \sim t_{n+m-2} .
$$

Hence

$$
t=\sqrt{\frac{n m}{n+m}} \frac{(\bar{x}-\bar{y})}{s}
$$

is a test statistic for this test. The rejection region is $|t|>h$ where $-h$ is the $\alpha / 2$ (usually $0.025$ ) percentile of $t_{n+m-2}$.

Confidence interval for $\mu_{X}-\mu_{Y}$.

From the hypothesis testing, a $100(1-\alpha) \%$ confidence interval is given by

$$
\bar{x}-\bar{y} \pm h \sqrt{\frac{n+m}{n m}} s,
$$

where $-h$ is the $\alpha / 2$ (usually $0.025$ ) percentile of $t_{n+m-2}$.

\section{$\Upsilon$ Example 73 Fast food waiting time as a two sample t-test}

In this example, we would like to know if there are significant differences between the AM and PM waiting times. Here the 10 morning waiting times $(x)$ are: $38,100,64,43,63,59,107,52,86,77$ and the 10 afternoon waiting times $(y)$ are: 45, 62, 52, 72, 81, 88, 64, 75, 59, 70. Here $n=m=10$, $\bar{x}=68.9, \bar{y}=66.8, s_{x}^{2}=538.22$ and $s_{y}^{2}=171.29$. From this we calculate,

$$
s^{2}=\frac{(n-1) s_{x}^{2}+(m-1) s_{y}^{2}}{n+m-2}=354.8,
$$



$$
t_{\mathrm{obs}}=\sqrt{\frac{n m}{n+m}} \frac{(\bar{x}-\bar{y})}{s}=0.25 .
$$

This is not significant as the critical value $h=q t(0.975,18)=2.10$ is larger in absolute value than $0.25$. This can be achieved by calling the $R$ function t.test as follows:

y <- read.csv("servicetime.csv", head=T)

t.test $(\mathrm{y} \$ \mathrm{AM}, \mathrm{y} \$ \mathrm{PM})$

It automatically calculates the test statistic as $0.249$ and a p-value of $0.8067$. It also obtains the $95 \%$ CI given by $(-15.94,20.14)$.

\subsubsection{Paired t-test}

Sometimes the assumption that the $X$ and $Y$ variables are independent of each other is unlikely to be valid, due to the design of the study. The most common example of this is where $n=m$ and data are paired. For example, a measurement has been made on patients before treatment $(X)$ and then again on the same set of patients after treatment $(Y)$. Recall the weight gain example is exactly of this type. In such examples, we proceed by computing data on the differences

$$
z_{i}=x_{i}-y_{i}, \quad i=1, \ldots, n
$$

and modelling these differences as observations of i.i.d. $N\left(\mu_{z}, \sigma_{Z}^{2}\right)$ variables $Z_{1}, \ldots, Z_{n}$. Then, a test of the hypothesis $\mu_{X}=\mu_{Y}$ is achieved by testing $\mu_{Z}=0$, which is just a standard (one sample) t-test, as described previously.

$\odot$ Example 74 Paired t-test Water-quality researchers wish to measure the biomass to chlorophyll ratio for phytoplankton (in milligrams per litre of water). There are two possible tests, one less expensive than the other. To see whether the two tests give the same results, ten water samples were taken and each was measured both ways. The results are as follows:

\begin{tabular}{l|llllllllll} 
Test 1 $(x)$ & $45.9$ & $57.6$ & $54.9$ & $38.7$ & $35.7$ & $39.2$ & $45.9$ & $43.2$ & $45.4$ & $54.8$ \\
Test 2 (y) & $48.2$ & $64.2$ & $56.8$ & $47.2$ & $43.7$ & $45.7$ & $53.0$ & $52.0$ & $45.1$ & $57.5$
\end{tabular}

To test the null-hypothesis

$$
\mathrm{H}_{0}: \mu_{Z}=0 \text { against } \mathrm{H}_{1}: \mu_{Z} \neq 0
$$

we use the test statistic $t=\sqrt{n} \frac{\bar{z}}{s_{z}}$, where $s_{z}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(z_{i}-\bar{z}\right)^{2}$.

Confidence interval for $\mu_{Z}$.

From the hypothesis testing, a $100(1-\alpha) \%$ confidence interval is given by $\bar{z} \pm h \frac{s_{z}}{\sqrt{n}}$, where $h$ is the critical value of the $t$ distribution with $n-1$ degrees of freedom. In $\mathrm{R}$ we perform the test as follows:

$\mathrm{x}<-\mathrm{c}(45.9,57.6,54.9,38.7,35.7,39.2,45.9,43.2,45.4,54.8)$

$\mathrm{y}<-\mathrm{c}(48.2,64.2,56.8,47.2,43.7,45.7,53.0,52.0,45.1,57.5)$

t.test $(x, y$, paired $=T)$

This gives the test statistic $t_{\mathrm{obs}}=-5.0778$ with a $\mathrm{df}$ of 9 and a p-value $=0.0006649$. Thus we reject the null hypothesis. The associated $95 \%$ CI is $(-7.53,-2.89)$, printed by R.

Interpretation: The values of the second test are significantly higher than the ones of the first test, and so the second test cannot be considered as a replacement for the first.
